{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into creating a bag of words document model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Project level imports\n",
    "sys.path.insert(0, '../lib')\n",
    "from ncbi_remap.notebook import Nb\n",
    "from ncbi_remap.plotting import make_figs\n",
    "\n",
    "# Connect to data store\n",
    "store = pd.HDFStore('../output/sra.h5', mode='r')\n",
    "samples = store['aln/complete'].srx.unique().tolist()\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "try:\n",
    "    with open('../output/.mongodb_host', 'r') as fh:\n",
    "        host = fh.read().strip()\n",
    "except FileNotFoundError:\n",
    "    host = 'localhost'\n",
    "\n",
    "mongoClient = MongoClient(host=host, port=27017)\n",
    "db = mongoClient['sra']\n",
    "ncbi = db['ncbi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(vals):\n",
    "    string = []\n",
    "    srx = vals['srx']\n",
    "    sample_title = vals.get('sample_title', '')\n",
    "    attrs = vals.get('attrs', [])\n",
    "    \n",
    "    string.append(sample_title)\n",
    "        \n",
    "    for attr in attrs:\n",
    "        string.append(attr['value'])\n",
    "        \n",
    "    return srx, ' '.join([str(x) for x in string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [get_document(x) for x in ncbi.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            '_id': {'$in': samples}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'srx': '$srx',\n",
    "            'sample_title': '$sra.sample.title',\n",
    "            'attrs': '$sra.sample.attributes',\n",
    "        }\n",
    "    }\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_document(doc):\n",
    "    # tokens document into individual words\n",
    "    tokens = regexp_tokenize(doc.lower(), r\"[\\w\\(\\);\\-\\+\\[\\]\\/]+\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    alpha_num = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # remove lone numbers\n",
    "    no_num = [token for token in alpha_num if not token.isnumeric()]\n",
    "    \n",
    "    # remove single characters\n",
    "    not_single = [token for token in no_num if len(token) > 1]\n",
    "    \n",
    "    # remove stop words\n",
    "    eng_stops = stopwords.words('english') + ['drosophila', 'melanogaster']\n",
    "    no_stops = [token for token in not_single if not token in eng_stops]\n",
    "    \n",
    "    # lemmatize\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [wordnet_lemmatizer.lemmatize(token) for token in no_stops]\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize document\n",
    "tokenized_documents = list(map(tokenize_document, np.asarray(docs)[:, 1]))\n",
    "\n",
    "# create bag of words\n",
    "dictionary = Dictionary(tokenized_documents)\n",
    "corpus = [dictionary.doc2bow(document) for document in tokenized_documents]\n",
    "\n",
    "# Get corpus level counts\n",
    "total_cnts = defaultdict(int)\n",
    "for word_id, word_cnt in chain.from_iterable(corpus):\n",
    "    total_cnts[word_id] += word_cnt\n",
    "\n",
    "# Remove tokens that are only in one document\n",
    "def drop_unique(document):\n",
    "    res = []\n",
    "    for token_id, token_cnt in document:\n",
    "        if token_cnt == total_cnts[token_id]:\n",
    "            continue\n",
    "        res.append((token_id, token_cnt))\n",
    "    return res\n",
    "\n",
    "corpus_no_unique = list(map(drop_unique, corpus))\n",
    "\n",
    "# Create document model\n",
    "tfidf = TfidfModel(corpus_no_unique)\n",
    "\n",
    "# Calculate weights\n",
    "tfidf_weights = [tfidf[document] for document in corpus_no_unique] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_weights(document):\n",
    "    sorted_weights = sorted(document, key=lambda w: w[1], reverse=True)\n",
    "    \n",
    "    human = []\n",
    "    for wt in sorted_weights:\n",
    "        if wt[1] > 0.25:\n",
    "            word = dictionary.get(wt[0])\n",
    "            human.append((word, wt[1]))\n",
    "            \n",
    "    return human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the best terms and make them readable\n",
    "weights = list(map(human_weights, tfidf_weights))\n",
    "\n",
    "# Concatenate words in order of weight\n",
    "bows = []\n",
    "for srx, wts in zip(np.asarray(docs)[:, 0], weights):\n",
    "    if len(wts) == 0:\n",
    "        string = ''\n",
    "    else:\n",
    "        string = '|'.join(np.asarray(wts)[:, 0])\n",
    "    bows.append((srx, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.DataFrame(bows, columns=['srx', 'keywords'])\n",
    "keywords.set_index('srx', inplace=True)\n",
    "keywords.to_parquet('../output/notebook/2018-07-11_bow_keywords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ncbi_remap]",
   "language": "python",
   "name": "conda-env-ncbi_remap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
