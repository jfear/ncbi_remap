{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Migrating Old Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have re-factored the the prealignment workflow. I need to move the old data over so I want to test what all needs done. For this notebook I am going to focus on a single sample that I have manually copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2017-10-20 \n",
      "Git hash: 6b527ce9b88199c4f8ea2cfccb60c05c5b35093b\n"
     ]
    }
   ],
   "source": [
    "# %load ../start.py\n",
    "# Load useful extensions\n",
    "\n",
    "# Activate the autoreload extension for easy reloading of external packages\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Trun on the water mark\n",
    "%reload_ext watermark\n",
    "%watermark -u -d -g\n",
    "\n",
    "# Load ipycache extension\n",
    "%reload_ext ipycache\n",
    "from ipycache import CacheMagics\n",
    "CacheMagics.cachedir = '../cachedir'\n",
    "\n",
    "# Add project library to path\n",
    "import sys\n",
    "sys.path.insert(0, '../../lib/python')\n",
    "\n",
    "# The usual suspects\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "\n",
    "# Turn off scientific notation\n",
    "np.set_printoptions(precision=5, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../bin/load.py\n",
    "from pymongo import MongoClient\n",
    "with open('/home/fearjm/Projects/ncbi_remap/output/.mongodb_host', 'r') as fh:\n",
    "    host = fh.read().strip()\n",
    "client = MongoClient(host=host, port=27022)\n",
    "db = client['sra2']\n",
    "remap = db['remap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "srx = 'SRX287752'\n",
    "srr = 'SRR869941'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two TSVs that are now created by the pipeline instead of dumping to the database:\n",
    "\n",
    "* {SRR}.fastq.tsv\n",
    "* {SRR}.hisat2.bam.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FASTQ tsv\n",
    "\n",
    "Output of the table looks like this:\n",
    "\n",
    "md5_R1 | libsize_R1 | avgLen_R1 | md5_R2 | libsize_R2 | avgLen_R2\n",
    "------ | ---------- | --------- | ------ | ---------- | ---------\n",
    "8a82f3f5ef231d8653702d5fb14e3b99 | 12752083 | 101.0 | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(remap.aggregate([\n",
    "    {'$unwind': '$runs'},\n",
    "    {\n",
    "        '$match': {\n",
    "            'runs.md5': {'$exists': 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'srx': '$_id',\n",
    "            'srr': '$runs.srr',\n",
    "            'md5_R1': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.md5.R1',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "            'libsize_R1': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.libsize.R1',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "            'avgLen_R1': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.avgReadLen.R1',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "            'md5_R2': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.md5.R2',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "            'libsize_R2': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.libsize.R2',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "            'avgLen_R2': {\n",
    "                '$ifNull': [\n",
    "                    '$runs.avgReadLen.R2',\n",
    "                    np.nan\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['srx', 'srr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['md5_R1', 'libsize_R1',  'avgLen_R1',  'md5_R2', 'libsize_R2',  'avgLen_R2']\n",
    "fastq = df.loc[~df.isnull().all(axis=1), cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = []\n",
    "done = []\n",
    "def make_fastq(srx, srr, df):\n",
    "    fname = '../../output/prealignment/raw/{srx}/{srr}/{srr}.fastq.tsv'.format(srx=srx, srr=srr)\n",
    "    try:\n",
    "        df.to_frame().T.to_csv(fname, sep='\\t', index=False)\n",
    "        done.append((srx, srr))\n",
    "    except FileNotFoundError:\n",
    "        skipped.append((srx, srr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [delayed(make_fastq)(srx, srr, df) for (srx, srr), df in fastq.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 43.7s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    compute(*dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24452, 3163)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(done), len(skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hisat2 Summary tsv\n",
    "\n",
    "num_reads | num_reads_unpaired | num_unaligned | num_uniquely_aligned | num_multimappers | per_alignment\n",
    "--------- | ------------------ | ------------- | -------------------- | ---------------- | -------------\n",
    "12752083 | 12752083 | 2554251 | 8260350 | 1937482 | 79.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [(x['srx'], x['srr']) for x in remap.aggregate([\n",
    "    {'$unwind': '$runs'},\n",
    "    {\n",
    "        '$match': {\n",
    "            'runs.pre_aln_workflow.hisat2': {'$exists': 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'srx': '$_id',\n",
    "            'srr': '$runs.srr',\n",
    "        }\n",
    "    }\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ncbi_remap.parser import parse_hisat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = []\n",
    "done = []\n",
    "bad = []\n",
    "def parse(srx, srr):\n",
    "    try:\n",
    "        fn='../../output/prealignment/raw/{srx}/{srr}/{srr}.hisat2.bam.log'.format(srx=srx, srr=srr)\n",
    "        dd = parse_hisat2(srr, fn)\n",
    "        output='../../output/prealignment/raw/{srx}/{srr}/{srr}.hisat2.bam.tsv'.format(srx=srx, srr=srr)\n",
    "        dd.to_csv(output, sep='\\t', index=False)\n",
    "\n",
    "        if dd.ix[0, 'per_alignment'] < .50:\n",
    "            fn='../../output/prealignment/raw/{srx}/{srr}/ALIGNMENT_BAD'.format(srx=srx, srr=srr)\n",
    "            bad.append((srx, srr))\n",
    "            Path(fn).touch()\n",
    "            \n",
    "        done.append((srx, srr))\n",
    "    except FileNotFoundError:\n",
    "        skipped.append((srx, srr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [delayed(parse)(srx, srr) for srx, srr in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 44.6s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    compute(*dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24419, 14)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(done), len(skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbBad = [(x['srx'], x['srr']) for x in remap.aggregate([\n",
    "    {'$unwind': '$runs'},\n",
    "    {\n",
    "        '$match': {\n",
    "            'runs.pre_aln_flags': 'alignment_bad'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'srx': '$_id',\n",
    "            'srr': '$runs.srr'\n",
    "        }\n",
    "    }\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in bad if x in dbBad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [x for x in dbBad if x not in bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = np.array(problems)[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'SRX886177', 'flags': ['SE', 'alignment_bad', 'unstranded']},\n",
       " {'_id': 'SRX765645', 'flags': ['SE', 'alignment_bad']},\n",
       " {'_id': 'SRX765644', 'flags': ['SE', 'alignment_bad', 'opposite_strand']},\n",
       " {'_id': 'SRX765642', 'flags': ['SE', 'alignment_bad', 'same_strand']},\n",
       " {'_id': 'SRX765641', 'flags': ['SE', 'alignment_bad', 'same_strand']},\n",
       " {'_id': 'SRX765640', 'flags': ['SE', 'alignment_bad', 'opposite_strand']},\n",
       " {'_id': 'SRX751576', 'flags': ['SE', 'alignment_bad', 'opposite_strand']},\n",
       " {'_id': 'SRX679372', 'flags': ['SE', 'alignment_bad', 'opposite_strand']},\n",
       " {'_id': 'SRX679372', 'flags': ['SE', 'alignment_bad', 'opposite_strand']},\n",
       " {'_id': 'SRX679369', 'flags': ['SE', 'alignment_bad']},\n",
       " {'_id': 'SRX679368', 'flags': ['SE', 'alignment_bad']},\n",
       " {'_id': 'SRX468097', 'flags': ['SE', 'alignment_bad']},\n",
       " {'_id': 'SRX156291', 'flags': ['SE', 'alignment_bad']},\n",
       " {'_id': 'SRX029216', 'flags': ['SE', 'keep_R1', 'alignment_bad']}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(remap.aggregate([\n",
    "    {'$unwind': '$runs'},\n",
    "    {\n",
    "        '$match': {\n",
    "            'runs.srr': {'$in': bob}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            'flags': '$runs.pre_aln_flags'\n",
    "        }\n",
    "    }\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('../../sra.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = pd.DataFrame(problems, columns=['srx', 'srr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ncbi_remap.io import remove_chunk, add_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10643\n",
      "10643\n"
     ]
    }
   ],
   "source": [
    "print(store['prealn/queue'].shape[0])\n",
    "add_table(store, 'prealn/queue', data=problems)\n",
    "print(store['prealn/queue'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242\n",
      "2228\n"
     ]
    }
   ],
   "source": [
    "print(store['prealn/alignment_bad'].shape[0])\n",
    "remove_chunk(store, 'prealn/alignment_bad', problems.srr)\n",
    "print(store['prealn/alignment_bad'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in problems.iterrows():\n",
    "    srx, srr = row.srx, row.srr\n",
    "    #!rm ../../output/prealignment/raw/{srx}/{srr}/{srr}*bam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ncbi_remap]",
   "language": "python",
   "name": "conda-env-ncbi_remap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
