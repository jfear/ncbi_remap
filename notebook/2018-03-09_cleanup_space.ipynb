{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am running into space issues on BioWulf. I am trying to finish up the new samples and I am sitting at ~39TB of 40TB. Both Pre-alignment and the alignment workflows do some clean-up but there are probably some files that are still around. Here I am just going to go through systematically and try to clean things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check output/fbgn2chrom.tsv. If it does not exist, run bin/fbgn2chrom.py\n",
      "last updated: 2018-03-09 \n",
      "Git hash: 481186bc23484439db5d199bf22411933a1bca80\n"
     ]
    }
   ],
   "source": [
    "# %load ../config/defaults.py\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "from lcdblib.snakemake.helpers import fill_patterns\n",
    "from lcdblib.utils.utils import flatten\n",
    "\n",
    "# Project level imports\n",
    "sys.path.insert(0, '../lib')\n",
    "from ncbi_remap.notebook import Nb\n",
    "from ncbi_remap.plotting import make_figs\n",
    "from ncbi_remap.snakemake import get_patterns\n",
    "\n",
    "# Setup notebook\n",
    "nbconfig = Nb.setup_notebook()\n",
    "\n",
    "# Connect to data store\n",
    "store = pd.HDFStore('../sra.h5', mode='r')\n",
    "\n",
    "# Start dask cluster\n",
    "client = Client(n_workers=12, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Alignment Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = get_patterns('../prealn-wf/patterns.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abi_solid': 'output/samples/{srx}/{srr}/ABI_SOLID',\n",
       " 'alignment_bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',\n",
       " 'atropos': {'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',\n",
       "  'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz'},\n",
       " 'bai': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bai',\n",
       " 'bamtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bamtools.stats',\n",
       " 'download_bad': 'output/samples/{srx}/{srr}/DOWNLOAD_BAD',\n",
       " 'fastq': {'r1': 'output/samples/{srx}/{srr}/{srr}_1.fastq.gz',\n",
       "  'r2': 'output/samples/{srx}/{srr}/{srr}_2.fastq.gz',\n",
       "  'summary': 'output/samples/{srx}/{srr}/{srr}.fastq.tsv'},\n",
       " 'fastq_screen': 'output/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt',\n",
       " 'feature_counts': {'counts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts',\n",
       "  'jcounts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.jcounts',\n",
       "  'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.summary'},\n",
       " 'hisat2': {'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam',\n",
       "  'splice_sites': 'output/known_splice_sites_r6-11.txt',\n",
       "  'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv'},\n",
       " 'layout': 'output/samples/{srx}/{srr}/LAYOUT',\n",
       " 'picard': {'collectrnaseqmetrics': {'metrics': {'first': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',\n",
       "    'second': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',\n",
       "    'unstranded': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics'}},\n",
       "  'markduplicates': {'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.bam',\n",
       "   'metrics': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics'}},\n",
       " 'quality_scores_bad': 'output/samples/{srx}/{srr}/QUALITY',\n",
       " 'samtools_idxstats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.idxstats',\n",
       " 'samtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.stats',\n",
       " 'strand': 'output/samples/{srx}/{srr}/STRAND'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patterns['download_bad']\n",
    "del patterns['fastq']['summary']\n",
    "del patterns['hisat2']['splice_sites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dln = store['prealn/download_bad']\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    targets = flatten(fill_patterns(patterns, dict(srx=srx, srr=srr)))\n",
    "    for target in targets:\n",
    "        _t = Path('../prealn-wf', target)\n",
    "        if _t.exists():\n",
    "            print(_t)\n",
    "            _t.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra logs\n",
    "dln = store['prealn/download_bad']\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    odir = Path(f'../output/prealn-wf/samples/{srx}/{srr}')\n",
    "    for log in odir.glob('*.log'):\n",
    "        if log.name.endswith('fastq.gz.log'):\n",
    "            continue\n",
    "        \n",
    "        print(log)\n",
    "        log.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abi Solid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patterns['abi_solid']\n",
    "del patterns['layout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dln = store['prealn/abi_solid']\n",
    "assert len(dln) == 548    # just making sure I match the 548 from the prealn-store.py queue --print\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    targets = flatten(fill_patterns(patterns, dict(srx=srx, srr=srr)))\n",
    "    for target in targets:\n",
    "        _t = Path('../prealn-wf', target)\n",
    "        if _t.exists():\n",
    "            print(_t)\n",
    "            _t.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra logs that are not needed\n",
    "dln = store['prealn/abi_solid']\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    odir = Path(f'../output/prealn-wf/samples/{srx}/{srr}')\n",
    "    for log in odir.glob('*.log'):\n",
    "        if log.name.endswith('fastq.gz.log'):\n",
    "            continue\n",
    "        print(log)\n",
    "        log.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality scores bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patterns['quality_scores_bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dln = store['prealn/quality_scores_bad']\n",
    "assert len(dln) == 4    # just making sure I match the 4 from the prealn-store.py queue --print\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    targets = flatten(fill_patterns(patterns, dict(srx=srx, srr=srr)))\n",
    "    for target in targets:\n",
    "        _t = Path('../prealn-wf', target)\n",
    "        if _t.exists():\n",
    "            print(_t)\n",
    "            _t.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra logs that are not needed\n",
    "dln = store['prealn/quality_scores_bad']\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    odir = Path(f'../output/prealn-wf/samples/{srx}/{srr}')\n",
    "    for log in odir.glob('*.log'):\n",
    "        if log.name.endswith('fastq.gz.log'):\n",
    "            continue\n",
    "        print(log)\n",
    "        log.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patterns['alignment_bad']\n",
    "del patterns['hisat2']['summary']\n",
    "del patterns['fastq_screen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_aln_bad(srx, srr):\n",
    "    targets = flatten(fill_patterns(patterns, dict(srx=srx, srr=srr)))\n",
    "    outs = []\n",
    "    for target in targets:\n",
    "        _t = Path('../prealn-wf', target)\n",
    "        if _t.exists():\n",
    "            _t.unlink()\n",
    "            outs.append(_t.as_posix())\n",
    "    return outs\n",
    "\n",
    "dln = store['prealn/alignment_bad']\n",
    "assert len(dln) == 2337    # just making sure I match the 2337 from the prealn-store.py queue --print\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    lazy.append(check_aln_bad(srx, srr))\n",
    "\n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_aln_log(odir):\n",
    "    outs = []\n",
    "    for log in odir.glob('*.log'):\n",
    "        if log.name.endswith('fastq.gz.log') | log.name.endswith('hisat2.bam.log') | log.name.endswith('fastq_screen.txt.log'):\n",
    "            continue\n",
    "        outs.append(log)\n",
    "        log.unlink()\n",
    "    return outs\n",
    "\n",
    "# Delete extra logs that are not needed\n",
    "dln = store['prealn/alignment_bad']\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    odir = Path(f'../output/prealn-wf/samples/{srx}/{srr}')\n",
    "    lazy.append(check_aln_log(odir))\n",
    "    \n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_complete(srx, srr):\n",
    "    targets = fill_patterns(patterns, dict(srx=srx, srr=srr))\n",
    "    targets = [*targets['hisat2']['bam'], *targets['bai'], *targets['atropos']['r1'], *targets['atropos']['r2']]\n",
    "    outs = []\n",
    "    for target in targets:\n",
    "        _t = Path('../prealn-wf', target)\n",
    "        if _t.exists():\n",
    "            outs.append(_t.as_posix())\n",
    "    return outs\n",
    "\n",
    "# This is very dangerous, could accidently delete evertyhing.\n",
    "# So I am leaving it as just a print statement\n",
    "dln = store['prealn/complete']\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    lazy.append(check_complete(srx, srr))\n",
    "\n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = get_patterns('../aln-wf/patterns.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alignment_bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',\n",
       " 'atropos': {'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',\n",
       "  'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz'},\n",
       " 'chromSizes_fb': '../output/dmel_r6-11.flybase.chromsizes',\n",
       " 'fastq': {'r1': '../output/prealn-wf/samples/{srx}/{srr}/{srr}_1.fastq.gz',\n",
       "  'r2': '../output/prealn-wf/samples/{srx}/{srr}/{srr}_2.fastq.gz',\n",
       "  'summary': '../output/prealn-wf/samples/{srx}/{srr}/{srr}.fastq.tsv'},\n",
       " 'hisat2': {'bai': 'output/samples/{srx}/{srr}/{srr}.fq.bam.bai',\n",
       "  'bam': 'output/samples/{srx}/{srr}/{srr}.fq.bam',\n",
       "  'splice_sites': '../output/prealn-wf/known_splice_sites_r6-11.txt',\n",
       "  'summary': 'output/samples/{srx}/{srr}/{srr}.fq.bam.tsv'},\n",
       " 'intergenic': {'bed': '../output/dmel_r6-11.intergenic.bed',\n",
       "  'gtf': '../output/dmel_r6-11.intergenic.gtf'},\n",
       " 'layout': '../output/prealn-wf/samples/{srx}/{srr}/LAYOUT',\n",
       " 'srxMerge': {'bai': 'output/samples/{srx}/{srx}.bam.bai',\n",
       "  'bam': 'output/samples/{srx}/{srx}.bam',\n",
       "  'bamCoverage': 'output/samples/{srx}/{srx}.{strand}.bedgraph',\n",
       "  'bamCoverageFlyBase': 'output/samples/{srx}/{srx}.flybase.{strand}.bedgraph',\n",
       "  'bamtools_stats': 'output/samples/{srx}/{srx}.bam.bamtools.stats',\n",
       "  'bigWig': 'output/samples/{srx}/{srx}.{strand}.bw',\n",
       "  'bigWigFlyBase': 'output/samples/{srx}/{srx}.flybase.{strand}.bw',\n",
       "  'feature_counts': {'counts': 'output/samples/{srx}/{srx}.bam.counts',\n",
       "   'jcounts': 'output/samples/{srx}/{srx}.bam.counts.jcounts',\n",
       "   'summary': 'output/samples/{srx}/{srx}.bam.counts.summary'},\n",
       "  'feature_counts_intergenic': {'counts': 'output/samples/{srx}/{srx}.bam.intergenic.counts',\n",
       "   'jcounts': 'output/samples/{srx}/{srx}.bam.intergenic.counts.jcounts',\n",
       "   'summary': 'output/samples/{srx}/{srx}.bam.intergenic.counts.summary'},\n",
       "  'samtools_idxstats': 'output/samples/{srx}/{srx}.bam.samtools.idxstats',\n",
       "  'samtools_stats': 'output/samples/{srx}/{srx}.bam.samtools.stats'},\n",
       " 'strand': '../output/prealn-wf/samples/{srx}/{srr}/STRAND'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patterns['strand']\n",
    "del patterns['layout']\n",
    "del patterns['chromSizes_fb']\n",
    "del patterns['intergenic']\n",
    "del patterns['hisat2']['splice_sites']\n",
    "del patterns['alignment_bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_aln_bad(srx, srr, strand):\n",
    "    targets = flatten(fill_patterns(patterns, dict(srx=srx, srr=srr, strand=strand)))\n",
    "    outs = []\n",
    "    for target in targets:\n",
    "        _t = Path('../aln-wf', target)\n",
    "        if _t.exists():\n",
    "            #_t.unlink()\n",
    "            outs.append(_t.as_posix())\n",
    "    return outs\n",
    "\n",
    "dln = store['aln/alignment_bad']\n",
    "assert len(dln) == 102    # just making sure I match the 102 from the aln-store.py queue --print\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    for strand in ['first', 'second']:\n",
    "        lazy.append(check_aln_bad(srx, srr, strand))\n",
    "\n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_aln_log(odir):\n",
    "    outs = []\n",
    "    for log in odir.glob('*.log'):\n",
    "        if log.name.endswith('hisat2.bam.log'):\n",
    "            continue\n",
    "        outs.append(log)\n",
    "        #log.unlink()\n",
    "    return outs\n",
    "\n",
    "# Delete extra logs that are not needed\n",
    "dln = store['aln/alignment_bad']\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    odir = Path(f'../output/aln-wf/samples/{srx}/{srr}')\n",
    "    lazy.append(check_aln_log(odir))\n",
    "    \n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def check_complete(srx, srr, strand):\n",
    "    targets = fill_patterns(patterns, dict(srx=srx, srr=srr, strand=strand))\n",
    "    targets = [\n",
    "        *targets['fastq']['r1'],\n",
    "        *targets['fastq']['r2'],\n",
    "        *targets['hisat2']['bam'], \n",
    "        *targets['hisat2']['bai'], \n",
    "        *targets['atropos']['r1'], \n",
    "        *targets['atropos']['r2'],\n",
    "        *targets['srxMerge']['bamCoverage'],\n",
    "        *targets['srxMerge']['bamCoverageFlyBase'],\n",
    "    ]\n",
    "    outs = []\n",
    "    for target in targets:\n",
    "        if target.startswith('../prealn'):\n",
    "            _t = Path(target)\n",
    "        else:\n",
    "            _t = Path('../aln-wf', target)\n",
    "        if _t.exists():\n",
    "            outs.append(_t.as_posix())\n",
    "            #_t.unlink()\n",
    "    return outs\n",
    "\n",
    "# This is very dangerous, could accidently delete evertyhing.\n",
    "dln = store['aln/complete']\n",
    "lazy = []\n",
    "for i, (srx, srr) in dln.iterrows():\n",
    "    for strand in ['first', 'second']:\n",
    "        lazy.append(check_complete(srx, srr, strand))\n",
    "\n",
    "futures = client.compute(lazy)\n",
    "res = list(chain.from_iterable(client.gather(futures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ncbi_remap]",
   "language": "python",
   "name": "conda-env-ncbi_remap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
