"""Aggregated BigWig of extremely well stranded files."""

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID', ''))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'



# Make sure to run the make_srx_list workflow first, otherwise this file won't be there to import.

#subworkflow make_srxs:
#    workdir: '.'
#    snakefile: '../make_srx_list.snake'
#
#
#rule run_make_srxs:
#  input: make_srxs('../output/uber-stranded-wf/uber_srxs.txt')
#

with open('../output/uber-stranded-wf/uber_srxs.txt') as fh:
    SRXS = fh.read().strip().split('\n')


PATTERNS = {
    'bam': '../output/aln-wf/samples/{srx}/{srx}.bam',
    'raw_bigwig': {
        'plus': '../output/uber-stranded-wf/samples/{srx}.plus.bw',
        'minus': '../output/uber-stranded-wf/samples/{srx}.minus.bw',
    }
}

rule targets:
    input:
        expand(PATTERNS['raw_bigwig']['plus'], srx=SRXS[:10]),
#        '../output/uber-stranded-wf/uber_stranded.plus_sum.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_sum.bw',
#        '../output/uber-stranded-wf/uber_stranded.plus_max.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_max.bw',
#        '../output/uber-stranded-wf/uber_stranded.plus_median.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_median.bw',
#        '../output/uber-stranded-wf/uber_stranded.plus_mean.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_mean.bw',
#        '../output/uber-stranded-wf/uber_stranded.plus_75.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_75.bw',
#        '../output/uber-stranded-wf/uber_stranded.plus_95.bw',
#        '../output/uber-stranded-wf/uber_stranded.minus_95.bw',
#        '../output/uber-stranded-wf/uber_stranded_merged.bam'
#         '../output/uber-stranded-wf/uber_stranded.plus_scaled.bw',
#         '../output/uber-stranded-wf/uber_stranded.minus_scaled.bw',
#         '../output/uber-stranded-wf/uber_stranded.plus_scaled_threshold1.bw',
#         '../output/uber-stranded-wf/uber_stranded.minus_scaled_threshold1.bw',
#         '../output/uber-stranded-wf/uber_stranded.plus_scaled_threshold10.bw',
#         '../output/uber-stranded-wf/uber_stranded.minus_scaled_threshold10.bw',


rule raw_bigwig:
    """Create a non-normalized bedgraph"""
    input: PATTERNS['bam']
    output:
        plus = PATTERNS['raw_bigwig']['plus'],
        minus = PATTERNS['raw_bigwig']['minus']
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: """
        bamCoverage -b {input[0]} -o {output.plus} -of bigwig -bs 1 --numberOfProcessors {threads} --filterRNAstrand forward && \
        bamCoverage -b {input[0]} -o {output.minus} -of bigwig -bs 1 --numberOfProcessors {threads}  --filterRNAstrand reverse
    """


def _bigWigMerge(wildcards):
    if wildcards['strand'] == 'plus':
        strand = 'first'
    else:
        strand = 'second'

    files = []
    for srx in SRXS:
        files.append(f'../output/aln-wf/samples/{srx}/{srx}.flybase.{strand}.bw')

    return files


rule bigWigMerge:
    input:
        fnames=_bigWigMerge,
        chromsizes='../output/dmel_r6-11.flybase.chromsizes'
    output: '../output/uber-stranded-wf/uber_stranded.{strand,plus|minus}_{agg_func,sum|mean|median|max|75|95}.bw'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: """
        bigWigMerge \
            --input {input.fnames} \
            --output {output[0]} \
            --chromSizes {input.chromsizes} \
            --threads {threads} \
            --agg {wildcards.agg_func}
    """


rule bigWigMerge_scale:
    input:
        fnames=_bigWigMerge,
        chromsizes='../output/dmel_r6-11.flybase.chromsizes'
    output: '../output/uber-stranded-wf/uber_stranded.{strand,plus|minus}_scaled.bw'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: """
        bigWigMerge \
            --input {input.fnames} \
            --output {output[0]} \
            --chromSizes {input.chromsizes} \
            --threads {threads} \
            --scale \
            --region X:0-23,542,271
    """


rule bigWigMerge_scale_theshold:
    input:
        fnames=_bigWigMerge,
        chromsizes='../output/dmel_r6-11.flybase.chromsizes'
    output: '../output/uber-stranded-wf/uber_stranded.{strand,plus|minus}_scaled_threshold1.bw'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: """
        bigWigMerge \
            --input {input.fnames} \
            --output {output[0]} \
            --chromSizes {input.chromsizes} \
            --threads {threads} \
            --scale \
            --threshold 1 \
            --region X:0-23,542,271
    """


rule bigWigMerge_scale_theshold10:
    input:
        fnames=_bigWigMerge,
        chromsizes='../output/dmel_r6-11.flybase.chromsizes'
    output: '../output/uber-stranded-wf/uber_stranded.{strand,plus|minus}_scaled_threshold10.bw'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: """
        bigWigMerge \
            --input {input.fnames} \
            --output {output[0]} \
            --chromSizes {input.chromsizes} \
            --threads {threads} \
            --scale \
            --threshold 10 \
            --region X:0-23,542,271
    """


rule mergeBam:
    """Merge a large number of bams together."""
    input: expand(PATTERNS['bam'], srx=SRXS)
    output: '../output/uber-stranded-wf/uber_stranded_merged.bam'
    threads: 12
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: 'scripts/merge_bam.py'


rule stringTie_merged:
    """Create a StringTie GTF from a merged set of BAMs.

    This should get around any coverage issues, but differences in
    strandedness may cause problems.
    """
    input:
        bam = rules.mergeBam.output[0],
        gtf = config['references']['dmel']['gtf'],
    output: '../output/uber-stranded-wf/uber_stranded_merged.stringtie.gtf'
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 1
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: 'scripts/stringtie.py'
