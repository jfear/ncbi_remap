"""RNA-Seq Alignment workflow.

The goal of the RNA-Seq workflow is to use parameters determined in the
pre-alignment workflow and processing files. This workflow uses a queue in
`../output/sra.h5`. To update the queue you need to run `./aln-store.py queue update
-j 8`. The major file types output by the alignment workflow include:

* Strand specific BigWig tracks
* Gene level coverage counts and junction counts
* Intergenic coverage counts and junction counts

Rules
-----
targets
    create a list of desired output files
intergenic
    create an intergenic bed and gtf based on the flybase gtf
fastq_dump
    download fastq file from SRA, determine if the file is pair-end or
    single-end and count the number of reads and average read length
atropos
    trim illumina adapters and low quality bases (<20) from fastq file, remove
    reads that have fewer than 25 bp
hisat2_splice_site
    create set of splice sites using the flybase gtf
hisat2
    align reads to the flybase reference
hisat2_summary
    parse alignment log and determine if the alignment failed (<50% aligned)
expMerge
    merge bams (SRRs) to the library (SRX) level
feature_counts
    count the number of reads that overlap genic regions and junction counts
featurecounts_intergenic
    count the number of reads that overlap intergenic regions and junction
    counts
run_stats
    calculate basic stats with `samtools stats`, `samtools idxstats`, and
    `bamtools stats`
bamCoverage
    create strand specific coverage tracks (bedgraph) using deeptools
convertToFlybase
    convert bedgraph chromosome names (UCSC format) to flybase chromosome names
convertBedGraphToBigWig
    convert begraph to bigwig
"""

import os
import sys
from pathlib import Path
import re

from more_itertools import flatten
import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

from ncbi_remap.queue import Queue
from ncbi_remap.snakemake import wrapper_for


# Setup tempdir to work with lscratch
if os.getenv("SLURM_JOBID"):
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
else:
    TMPDIR = os.getenv('TMPDIR', "/tmp")
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'

localrules: atropos_summary_agg, hisat2_summary_agg, srx_complete

###############################################################################
# Set up file naming patterns and targets
###############################################################################
queue = Queue(
    targets="../output/prealn-wf/done",
    subset="../output/library_strategy-wf/rnaseq.pkl",
    completed="../output/rnaseq-wf/done",
    problems=[
        "../output/fastq-wf/download_bad",
        "../output/fastq-wf/abi_solid",
        "../output/prealn-wf/atropos_bad",
        "../output/prealn-wf/alignment_bad",
        "../output/rnaseq-wf/atropos_bad",
        "../output/rnaseq-wf/alignment_bad",
        "../output/rnaseq-wf/bigwig_bad",
    ],
    srx2srr="../output/srx2srr.csv",
    size=1_000
)
# print(queue)

rule run_all:
    input: expand("../output/rnaseq-wf/done/{srx}", srx=queue.srxs)


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('rnaseq-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('rnaseq-wf: Something went wrong, you need to re-run')


###############################################################################
# FASTQ
###############################################################################
rule sra_prefetch:
    """Downloads container file from the SRA"""
    output: temp("../output/fastq-wf/sra_cache/{srr}.sra")
    log: "../output/fastq-wf/sra_download_logs/{srr}.log"
    conda: "./sratools.yaml"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 2,
        time_hr=lambda wildcards, attempt: attempt * 8
    shell: "prefetch --output-file {output[0]} --max-size 40G {wildcards.srr} > {log} 2>&1"


rule fastq_dump:
    input: rules.sra_prefetch.output[0]
    output:
        r1=temp("../output/fastq-wf/fastqs/{srr}_1.fastq"),
        r2=temp("../output/fastq-wf/fastqs/{srr}_2.fastq"),
    conda: "./sratools.yaml"
    threads: 8
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 4,
        time_hr=lambda wildcards, attempt: attempt * 12
    script: "../fastq-wf/scripts/fastq_extract.py"


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=rules.fastq_dump.output.r1,
        R2=rules.fastq_dump.output.r2,
        layout="../output/fastq-wf/fastq_info/{srr}/LAYOUT"
    output:
        R1=temp("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq"),
        R2=temp("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq")
    params:
        extra_pe='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25',
        extra_se='-a file:../data/adapters.fa -q 20 --minimum-length 25',
    log: "../output/rnaseq-wf/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz.log"
    threads: 8
    group: "atropos"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


rule atropos_summary:
    input:
        _=rules.atropos.output.R1,
        log="../output/rnaseq-wf/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz.log"
    output: "../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.trim.clean.tsv"
    threads: 8
    group: "atropos"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 12
    script: "../scripts/atropos_check.py"


rule atropos_summary_agg:
    input: lambda wildcards: expand("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.trim.clean.tsv", srx=wildcards.srx, srr=queue.get_srrs(wildcards.srx))
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.trim.clean.tsv"
    run:
        (
            pd.concat([pd.read_table(file_name) for file_name in input], axis=0)
            .to_csv(output[0], sep="\t", index=False)
        )


###############################################################################
# Alignment
###############################################################################
rule hisat2:
    """Basic alignment."""
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=config['references']['dmel']['known_splice_sites'],
        r1=rules.atropos.output.R1,
        r2=rules.atropos.output.R2,
        layout="../output/fastq-wf/fastq_info/{srr}/LAYOUT",
        strand="../output/prealn-wf/samples/{srx}/{srr}/STRAND"
    output:
        bam=temp("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.fq.bam"),
        bai=temp("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.fq.bam.bai")
    threads: 8
    group: "hisat2"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 16,
        time_hr=lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra='--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=config['references']['dmel']['known_splice_sites']),
        samtools_view_extra="--threads 4 -q 20",
        samtools_sort_extra='--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: "../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.fq.bam.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    input:
        _=rules.hisat2.output.bam,
        log=rules.hisat2.log[0]
    output: "../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv"
    threads: 8
    group: "hisat2"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 16,
        time_hr=lambda wildcards, attempt: attempt * 4
    script: "../scripts/hisat2_check.py"


rule hisat2_summary_agg:
    input: lambda wildcards: expand("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv", srx=wildcards.srx, srr=queue.get_srrs(wildcards.srx))
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.hisat2.bam.tsv"
    run:
        dfs = []
        for file_name in input:
            srr = re.findall(".*([DSE]RR\d+).*", file_name)[0]
            dfs.append(pd.read_table(file_name).assign(srx=wildcards.srx, srr=srr))
        pd.concat(dfs, axis=0).set_index(["srx", "srr"]).to_csv(output[0], sep="\t")


###############################################################################
# Merge Bams
###############################################################################
def _input_expMerge(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': queue.get_srrs(wildcards.srx)
    }
    return expand("../output/rnaseq-wf/samples/{srx}/{srr}/{srr}.fq.bam", **fill)


rule expMerge:
    input: _input_expMerge
    output:
        bam="../output/rnaseq-wf/samples/{srx}/{srx}.bam",
        bai="../output/rnaseq-wf/samples/{srx}/{srx}.bam.bai"
    conda: 'conda.yaml'
    group: "merge"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    threads: 4
    shell:
        'samtools merge '
        '-f '
        '-@ {threads} '
        '{output.bam} '
        '{input} && '
        'samtools index {output.bam}'


rule run_stats:
    input:
        bam=rules.expMerge.output.bam,
        bai=rules.expMerge.output.bai,
    output:
        samtools_stats="../output/rnaseq-wf/samples/{srx}/{srx}.bam.samtools.stats",
        samtools_idxstats="../output/rnaseq-wf/samples/{srx}/{srx}.bam.samtools.idxstats",
        bamtools_stats="../output/rnaseq-wf/samples/{srx}/{srx}.bam.bamtools.stats",
    conda: 'conda.yaml'
    group: "merge"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    threads: 4
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# Feature Counts
###############################################################################
def _layout(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': queue.get_srrs(wildcards.srx)[0]
    }
    return expand("../output/fastq-wf/fastq_info/{srr}/LAYOUT", **fill)[0]


def _strand(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': queue.get_srrs(wildcards.srx)[0]
    }
    return expand("../output/prealn-wf/samples/{srx}/{srr}/STRAND", **fill)[0]


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=rules.expMerge.output.bam,
        layout=_layout,
        strand=_strand,
    output:
        counts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.counts",
        jcounts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.counts.jcounts",
        summary="../output/rnaseq-wf/samples/{srx}/{srx}.bam.counts.summary",
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 8
    group: "featureCount"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: (attempt ** 2) * 4
    log: "../output/rnaseq-wf/samples/{srx}/{srx}.bam.counts.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule feature_counts_intergenic:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=config['references']['dmel']['intergenic'],
        bam=rules.expMerge.output.bam,
        layout=_layout,
    output:
        counts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.intergenic.counts",
        jcounts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.intergenic.counts.jcounts",
        summary="../output/rnaseq-wf/samples/{srx}/{srx}.bam.intergenic.counts.summary",
    params:
        extra_pe = '-p -P -C -J -t gene ',
        extra_se = '-J -t gene '
    threads: 8
    group: "featureCount"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: (attempt ** 2) * 4
    log: "../output/rnaseq-wf/samples/{srx}/{srx}.bam.intergenic.counts.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule feature_counts_segments:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=config['references']['dmel']['nonstranded_segments'],
        bam=rules.expMerge.output.bam,
        layout=_layout,
    output:
        counts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_segments.counts",
        jcounts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_segments.counts.jcounts",
        summary="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_segments.counts.summary",
    params:
        extra_pe = '-p -P -C -J -t segment -g ID ',
        extra_se = '-J -t segment -g ID '
    threads: 8
    group: "featureCount"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: (attempt ** 2) * 4
    log: "../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_segments.counts.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule feature_counts_fusions:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=config['references']['dmel']['nonstranded_fusions'],
        bam=rules.expMerge.output.bam,
        layout=_layout,
    output:
        counts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_fusions.counts",
        jcounts="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_fusions.counts.jcounts",
        summary="../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_fusions.counts.summary",
    params:
        extra_pe = '-p -P -C -J -t fusion -g ID ',
        extra_se = '-J -t fusion -g ID '
    threads: 8
    group: "featureCount"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: (attempt ** 2) * 4
    log: "../output/rnaseq-wf/samples/{srx}/{srx}.bam.exon_fusions.counts.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Make BedGraphs
###############################################################################
bamCoverage_options = (
    '--outFileFormat bedgraph '
    '--binSize 1 '
    '--effectiveGenomeSize 129000000 '
    '--normalizeUsing RPGC '
    '--ignoreForNormalization chrX '
)

rule bamCoverage_first:
    input:
        bam=rules.expMerge.output.bam,
        bai=rules.expMerge.output.bai,
    output: temp('../output/rnaseq-wf/samples/{srx}/{srx}.first.bedgraph')
    params:
        extra=bamCoverage_options + '--filterRNAstrand forward'
    threads: 8
    group: "bamCoverage"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/deeptools/bamCoverage')


rule bamCoverage_second:
    input:
        bam=rules.expMerge.output.bam,
        bai=rules.expMerge.output.bai,
    output: temp('../output/rnaseq-wf/samples/{srx}/{srx}.second.bedgraph')
    params:
        extra=bamCoverage_options + '--filterRNAstrand reverse'
    threads: 8
    group: "bamCoverage"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/deeptools/bamCoverage')


rule convertToFlybase_first:
    input: rules.bamCoverage_first.output[0]
    output: temp("../output/rnaseq-wf/samples/{srx}/{srx}.flybase.first.bedgraph")
    conda: 'conda.yaml'
    group: "FlyBaseConvert"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output[0]}'


rule convertToFlybase_second:
    input: rules.bamCoverage_second.output[0]
    output: temp("../output/rnaseq-wf/samples/{srx}/{srx}.flybase.second.bedgraph")
    conda: 'conda.yaml'
    group: "FlyBaseConvert"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output[0]}'


###############################################################################
# Make BigWigs
###############################################################################
rule bigwig_first:
    input:
        bedgraph=rules.bamCoverage_first.output[0],
        chromSizes=config['references']['dmel']['chromSizes']
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.first.bw"
    conda: 'ucsc.yaml'
    group: "BigWig"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output[0]} '
        '&& rm $tmpBg '


rule bigwig_second:
    input:
        bedgraph=rules.bamCoverage_second.output[0],
        chromSizes=config['references']['dmel']['chromSizes']
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.second.bw"
    conda: 'ucsc.yaml'
    group: "BigWig"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output[0]} '
        '&& rm $tmpBg '


rule flybase_bigwig_first:
    input:
        bedgraph=rules.convertToFlybase_first.output[0],
        chromSizes=config['references']['dmel']['fb_chromSizes']
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.flybase.first.bw"
    conda: 'ucsc.yaml'
    group: "FlyBaseBigWig"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output[0]} '
        '&& rm $tmpBg '


rule flybase_bigwig_second:
    input:
        bedgraph=rules.convertToFlybase_second.output[0],
        chromSizes=config['references']['dmel']['fb_chromSizes']
    output: "../output/rnaseq-wf/samples/{srx}/{srx}.flybase.second.bw"
    conda: 'ucsc.yaml'
    group: "FlyBaseBigWig"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output[0]} '
        '&& rm $tmpBg '


###############################################################################
# What to keep
###############################################################################
rule srx_complete:
    input:
        rules.atropos_summary_agg.output[0],
        rules.hisat2_summary_agg.output[0],
        rules.expMerge.output.bam,
        rules.feature_counts.output.counts,
        rules.feature_counts_intergenic.output.counts,
        rules.feature_counts_segments.output.counts,
        rules.feature_counts_fusions.output.counts,
        rules.run_stats.output.samtools_stats,
        rules.run_stats.output.samtools_idxstats,
        rules.run_stats.output.bamtools_stats,
        rules.bigwig_first.output[0],
        rules.bigwig_second.output[0],
        rules.flybase_bigwig_first.output[0],
        rules.flybase_bigwig_second.output[0],
    output: "../output/rnaseq-wf/done/{srx}"
    params:
        srrs=lambda wildcards: queue.get_srrs(wildcards.srx)
    run:
        Path(output[0]).touch()
        for srr in params.srrs:
            if Path(f"../output/fastq-wf/fastqs/{srr}_1.fastq").exists():
                Path(f"../output/fastq-wf/fastqs/{srr}_1.fastq").unlink()

            if Path(f"../output/fastq-wf/fastqs/{srr}_2.fastq").exists():
                Path(f"../output/fastq-wf/fastqs/{srr}_2.fastq").unlink()

            if Path(f"../output/fastq-wf/sra_cache/{srr}.sra").exists():
                Path(f"../output/fastq-wf/sra_cache/{srr}.sra").unlink()
