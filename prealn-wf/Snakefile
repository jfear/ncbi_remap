#/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib/python')
from ncbi_remap.snakemake import wrapper_for, put_flag


# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yml'


################################################################################
# Build Sample Table
################################################################################
mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
sample_table = store['prealn/queue'].head(3)
store.close()

################################################################################
# Set up file naming patterns
################################################################################
patterns = {
    'fastq': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.fastq.gz',
        'summary': 'output/samples/{srx}/{srr}/{srr}.fastq.tsv',
    },
    'layout': 'output/samples/{srx}/{srr}/LAYOUT',
    'fastq_screen': 'output/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt',
    'fastqc': {
        'html': 'output/samples/{srx}/{srr}/{srr}_1.fastqc.html',
        'zip': 'output/samples/{srx}/{srr}/{srr}_1.fastqc.zip',
    },
    'atropos': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': 'output/known_splice_sites_r6-11.txt',
        'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv',
        'bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',
    },
    'bai': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bai',
    'feature_counts': {
        'counts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts',
        'jcounts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.jcounts',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.summary',
    },
    'picard': {
        'collectrnaseqmetrics': {
            'metrics': {
                'unstranded': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics',
                'first': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
                'second': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
            },
        },
        'markduplicates': {
            'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.bam',
            'metrics': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics',
        },
    },
    'strand': 'output/samples/{srx}/{srr}/STRAND',
    'samtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.stats',
    'samtools_idxstats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.idxstats',
    'bamtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bamtools.stats',
}


################################################################################
# Set up Build Targets
################################################################################
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['fastq_screen'],
        targets['hisat2']['summary'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats'],
        targets['picard']['markduplicates']['metrics']
    ]


rule targets:
    input: keepers(targets)


rule update_queue:
    """Updates queue.

    After running targets you need to update the queue. This job itereates over
    outputs and moves ids around in the queing system.
    """
    run:
        sys.path.insert(0, '../lib/python')
        from ncbi_remap.io import add_table, remove_chunk, add_id, remove_id, check_layout, check_strand, check_alignment

        def check_outputs(**kwargs):
            tg = helpers.fill_patterns(patterns, kwargs)
            for fname in utils.flatten(keepers(tg)):
                if not os.path.exists(fname):
                    return
            return kwargs

        store = pd.HDFStore(mystore)
        done = []
        for i, row in sample_table.iterrows():
            curr = row.to_dict()

            if check_alignment(store, patterns['hisat2']['bad'], **curr):
                continue

            val = check_outputs(**curr)
            if val:
                done.append(val)
                check_layout(store, patterns['layout'], **curr)
                check_strand(store, patterns['strand'], **curr)

        df = pd.DataFrame(done)

        # Add to complete, add to alignment queue, remove from prealn queue
        add_table(store, 'prealn/complete', data=df)
        add_table(store, 'aln/queue', data=df)
        remove_chunk(store, 'prealn/queue', df.srr.tolist())
        store.close()


rule print_queue:
    """Prints the current counts for the queue."""
    run:
        store = pd.HDFStore(mystore, mode='r')

        def cnts(key):
            if store.__contains__(key):
                return store[key].shape[0]
            else:
                return 0

        keys = {
            "ids": "ids in the system",
            "prealn/queue": "queued",
            "prealn/complete": "completed",
            "prealn/download_bad": "download bad",
            "prealn/quality_scores_bad": "quality scores bad",
            "prealn/alignment_bad": "alignment bad",
            "prealn/abi_solid": "abi solid",
            "layout/SE": "Single End",
            "layout/PE": "Pair End",
            "layout/keep_R1": "Really Single End R1",
            "layout/keep_R2": "Really Single End R2",
            "strand/first": "first strand",
            "strand/second": "second strand",
            "strand/unstranded": "unstranded",
        }

        report = '\nCurrent Queue Summary\n'
        for k, v in keys.items():
            report += '{:,}\t\t\t{}\n'.format(cnts(k), v)

        print(report)
        store.close()


onsuccess:
    print('All Finished')


onerror:
    print('Something went wrong, you need to re-run')


################################################################################
# FASTQ dump and check for SE or PE
################################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1=patterns['fastq']['r1'],
        fq2=patterns['fastq']['r2'],
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    wrapper:
        wrapper_for('wrappers/fastq_dump')


################################################################################
# FASTQ QC
################################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq=patterns['fastq']['r1'],
        dm6=config['references']['dmel']['bowtie2'],
        hg19=config['references']['human']['bowtie2'],
        wolbachia=config['references']['wolbachia']['bowtie2'],
        ecoli=config['references']['ecoli']['bowtie2'],
        yeast=config['references']['yeast']['bowtie2'],
        rRNA=config['references']['rRNA']['bowtie2'],
        phix=config['references']['phix']['bowtie2'],
        ercc=config['references']['ercc']['bowtie2'],
        adapters=config['references']['adapters']['bowtie2']
    output:
        txt=patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


################################################################################
# FASTQ Pre-process
################################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=patterns['fastq']['r1'],
        R2=patterns['fastq']['r2'],
        layout=patterns['layout']
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2']),
    params:
        extra_pe='-U 0 --minimum-length 25',
        extra_se='--minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper:
        wrapper_for('wrappers/atropos')


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda: "../config/extra_env.yaml"
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        R1=patterns['atropos']['r1'],
        R2=patterns['atropos']['r2'],
        layout=patterns['layout']
    output:
        bam=temp(patterns['hisat2']['bam'])
    threads: 8
    params:
        hisat2_extra='--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn=patterns['hisat2']['bam']
    output:
        tsv=patterns['hisat2']['summary']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.srr
        df = parse_hisat2(srr, input.fn + '.log')
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['hisat2']['bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam='{prefix}.bam'
    output:
        bai=temp('{prefix}.bam.bai')
    conda: "../config/extra_env.yaml"
    shell:
        "samtools index {input.bam}"


################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra='STRAND=NONE',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra='STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra='STRAND=SECOND_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first=patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second=patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag=patterns['strand']
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(srr, input.unstranded)
        dfF = parse_picardCollect_summary(srr, input.first)
        dfS = parse_picardCollect_summary(srr, input.second)

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


################################################################################
# Feature Counts
################################################################################
rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam'],
        layout=patterns['layout'],
        strand=patterns['strand'],
    output:
        counts=patterns['feature_counts']['counts'],
        jcounts=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 4
    log:
        patterns['feature_counts']['counts'] + '.log'
    wrapper:
        wrapper_for('wrappers/featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule markduplicates:
    input:
        bam=rules.hisat2.output.bam
    output:
        bam=temp(patterns['picard']['markduplicates']['bam']),
        metrics=patterns['picard']['markduplicates']['metrics']
    params:
        java_args='-Xmx30g'
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/picard/markduplicates')


# vim: set ft=snakemake.python
