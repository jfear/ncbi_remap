"""Pre-alignment workflow.

The goal of the pre-alignment workflow is to determine technical metadata for
use as parameters in the alignment workflow. This workflow uses a queue in
`../output/sra.h5`. To update the queue you need to run `./prealn-store.py queue
update -j 8`. The major file types output by the pre-alignment workflow
include:

* Strand specific BigWig tracks
* Gene level coverage counts and junction counts
* Intergenic coverage counts and junction counts

Rules
-----
targets
    create a list of desired output files
download
    create a list of fastq files, can be used instead of `targets` to just download fastqs
fastq_dump
    download fastq file from SRA, determine if the file is pair-end or
    single-end and count the number of reads and average read length, and
    creates file called `LAYOUT`
fastq_screen
    align small subset of reads against multiple references to look for contamination
atropos
    remove reads that have fewer than 25 bp
hisat2_splice_site
    create set of splice sites using the flybase gtf
hisat2
    align reads to the flybase reference
hisat2_summary
    parse alignment log and determine if the alignment failed (<50% aligned)
    somtimes creates file `alignment_bad`
bai
    create indexed version of bam files
collectrnaseqmetrics_unstrand
    run `picard collectRNASeqMetrics` using unstranded settings
collectrnaseqmetrics_first
    run `picard collectRNASeqMetrics` using first strand settings
collectrnaseqmetrics_second
    run `picard collectRNASeqMetrics` using second strand settings
collectrnaseqmetrics_agg
    aggregate `picard collectRNASeqMetrics` results and determine how a sample
    is stranded and create a file called `STRAND`
feature_counts
    count the number of reads that overlap genic regions and junction counts
run_stats
    calculate basic stats with `samtools stats`, `samtools idxstats`, and
    `bamtools stats`
markduplicates
    run `picard markduplicates` to determine library complexity
"""

import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns


# Setup tempdir to work with lscratch
if os.getenv("SLURM_JOBID", False):
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
else:
    TMPDIR = os.getenv('TMPDIR', "/tmp")
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


###############################################################################
# Build Sample Table
###############################################################################
mystore = '../output/sra.h5'
store = pd.HDFStore(mystore, mode='r')
queue = store['prealn/queue']
srxs = queue.srx.unique()[:500]
sample_table = queue[queue.srx.isin(srxs)]
store.close()

# NOTE: pulling just modENCODE samples for running tau and TSPS
# sample_table = pd.read_csv('../output/modencode_samples.tsv', sep='\t')
# srxs = sample_table.srx.unique()
# sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
###############################################################################
# Set up file naming patterns and targets
###############################################################################
patterns = get_patterns('patterns.yaml')
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['fastq_screen'],
        targets['layout'],
        targets['strand'],
        targets['hisat2']['summary'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats'],
        targets['picard']['markduplicates']['metrics']
    ]


rule targets:
    input: keepers(targets)


rule download:
    """Only download the FASTQs."""
    input: targets['fastq']['r1']


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('prealn-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('prealn-wf: Something went wrong, you need to re-run')


##############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads.

    If there are less than 1000 reads or reads are less than 10bp long a
    indicator file `DOWNLOAD_BAD` is created.
    """
    output:
        fq1 = patterns['fastq']['r1'],
        fq2 = patterns['fastq']['r2'],
        flag = patterns['layout'],
        summary = patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 1,
      time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ QC
###############################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq = patterns['fastq']['r1'],
        dm6 = config['references']['dmel']['bowtie2'],
        hg19 = config['references']['human']['bowtie2'],
        wolbachia = config['references']['wolbachia']['bowtie2'],
        ecoli = config['references']['ecoli']['bowtie2'],
        yeast = config['references']['yeast']['bowtie2'],
        rRNA = config['references']['rRNA']['bowtie2'],
        phix = config['references']['phix']['bowtie2'],
        ercc = config['references']['ercc']['bowtie2'],
        adapters = config['references']['adapters']['bowtie2']
    output:
        txt = patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 3,
      time_hr = lambda wildcards, attempt: attempt * 1
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1 = patterns['fastq']['r1'],
        R2 = patterns['fastq']['r2'],
        layout = patterns['layout']
    output:
        R1 = temp(patterns['atropos']['r1']),
        R2 = temp(patterns['atropos']['r2']),
    params:
        extra_pe = '-U 0 --minimum-length 25',
        extra_se = '--minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


rule atropos_summary:
    input: rules.atropos.log
    output: patterns["atropos"]["summary"] 
    script: "../scripts/atropos_check.py"

    
###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf = config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    conda: "conda.yaml"
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag = patterns['layout'],
        index = config['references']['dmel']['hisat2'],
        splice_sites = patterns['hisat2']['splice_sites'],
        R1 = patterns['atropos']['r1'],
        R2 = patterns['atropos']['r2'],
        layout = patterns['layout']
    output:
        bam = temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 16,
        time_hr = lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra = '--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    input: rules.hisat2.log
    output: patterns['hisat2']['summary']
    script: "../scripts/hisat2_check.py"


rule bai:
    input:
        bam = '{prefix}.bam'
    output:
        bai = temp('{prefix}.bam.bai')
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    conda: "conda.yaml"
    shell:
        "samtools index {input.bam}"


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra = 'STRAND=NONE',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra = 'STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra = 'STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first = patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second = patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag = patterns['strand']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srx = wildcards.srx
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(input.unstranded.format(srx=srx,
                                                                  srr=srr))
        dfF = parse_picardCollect_summary(input.first.format(srx=srx, srr=srr))
        dfS = parse_picardCollect_summary(input.second.format(srx=srx,
                                                              srr=srr))

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


###############################################################################
# Feature Counts
###############################################################################
rule feature_counts:
    input:
        annotation = config['references']['dmel']['gtf'],
        bam = patterns['hisat2']['bam'],
        layout = patterns['layout'],
        strand = patterns['strand'],
    output:
        counts = patterns['feature_counts']['counts'],
        jcounts = patterns['feature_counts']['jcounts'],
        summary = patterns['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 4
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 4,
        time_hr = lambda wildcards, attempt: attempt * 1
    log:
        patterns['feature_counts']['counts'] + '.log'
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam = patterns['hisat2']['bam'],
        bai = patterns['bai'],
    output:
        samtools_stats = patterns['samtools_stats'],
        samtools_idxstats = patterns['samtools_idxstats'],
        bamtools_stats = patterns['bamtools_stats']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 2,
        time_hr = lambda wildcards, attempt: attempt * 2
    conda: "conda.yaml"
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule markduplicates:
    input:
        bam = rules.hisat2.output.bam
    output:
        bam = temp(patterns['picard']['markduplicates']['bam']),
        metrics = patterns['picard']['markduplicates']['metrics']
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/markduplicates')


# vim: set ft=snakemake.python
