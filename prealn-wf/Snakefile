"""Pre-alignment workflow.

The goal of the pre-alignment workflow is to determine technical metadata for
use as parameters in the alignment workflow. This workflow uses a queue in
`../output/sra.h5`. To update the queue you need to run `./prealn-store.py queue
update -j 8`. The major file types output by the pre-alignment workflow
include:

* Strand specific BigWig tracks
* Gene level coverage counts and junction counts
* Intergenic coverage counts and junction counts

Rules
-----
targets
    create a list of desired output files
download
    create a list of fastq files, can be used instead of `targets` to just download fastqs
fastq_dump
    download fastq file from SRA, determine if the file is pair-end or
    single-end and count the number of reads and average read length, and
    creates file called `LAYOUT`
fastq_screen
    align small subset of reads against multiple references to look for contamination
atropos
    remove reads that have fewer than 25 bp
hisat2_splice_site
    create set of splice sites using the flybase gtf
hisat2
    align reads to the flybase reference
hisat2_summary
    parse alignment log and determine if the alignment failed (<50% aligned)
    somtimes creates file `alignment_bad`
bai
    create indexed version of bam files
collectrnaseqmetrics_unstrand
    run `picard collectRNASeqMetrics` using unstranded settings
collectrnaseqmetrics_first
    run `picard collectRNASeqMetrics` using first strand settings
collectrnaseqmetrics_second
    run `picard collectRNASeqMetrics` using second strand settings
collectrnaseqmetrics_agg
    aggregate `picard collectRNASeqMetrics` results and determine how a sample
    is stranded and create a file called `STRAND`
feature_counts
    count the number of reads that overlap genic regions and junction counts
run_stats
    calculate basic stats with `samtools stats`, `samtools idxstats`, and
    `bamtools stats`
markduplicates
    run `picard markduplicates` to determine library complexity
"""

import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

from ncbi_remap.queue import Queue
from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns


# Setup tempdir to work with lscratch
if os.getenv("SLURM_JOBID", False):
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
else:
    TMPDIR = os.getenv('TMPDIR', "/tmp")
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# FASTQ Workflow
subworkflow fastq_wf:
    workdir: "../fastq-wf"
    snakefile: "../fastq-wf/Snakefile"

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'

localrules: atropos_summary, hisat2_summary

###############################################################################
# Set up file naming patterns and targets
###############################################################################
queue = Queue(
    # targets="../output/library_strategy-wf/rnaseq.pkl", 
    targets="../output/example_srxs.pkl", 
    completed="../output/prealn-wf/done", 
    problems=[
        "../output/fastq-wf/download_bad",
        "../output/fastq-wf/abi_solid",
        "../output/prealn-wf/atropos_bad",
        "../output/prealn-wf/alignment_bad",
    ],
    srx2srr="../output/srx2srr.csv", 
    size=1000
)

rule run_all:
    input: expand("../output/prealn-wf/done/{srr}", srr=queue.srrs)


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('prealn-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('prealn-wf: Something went wrong, you need to re-run')


###############################################################################
# FASTQ QC
###############################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq=fastq_wf("../output/fastq-wf/fastqs/{srr}_1.fastq"),
        dm6=config['references']['dmel']['bowtie2'],
        hg19=config['references']['human']['bowtie2'],
        wolbachia=config['references']['wolbachia']['bowtie2'],
        ecoli=config['references']['ecoli']['bowtie2'],
        yeast=config['references']['yeast']['bowtie2'],
        rRNA=config['references']['rRNA']['bowtie2'],
        phix=config['references']['phix']['bowtie2'],
        ercc=config['references']['ercc']['bowtie2'],
        adapters=config['references']['adapters']['bowtie2']
    output: txt="../output/prealn-wf/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt"
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt.log"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 3,
      time_hr=lambda wildcards, attempt: attempt * 1
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=fastq_wf("../output/fastq-wf/fastqs/{srr}_1.fastq"),
        R2=fastq_wf("../output/fastq-wf/fastqs/{srr}_2.fastq"),
        layout=fastq_wf("../output/fastq-wf/fastq_info/{srr}/LAYOUT")
    output:
        R1=temp("../output/prealn-wf/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq"),
        R2=temp("../output/prealn-wf/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq"),
    params:
        extra_pe='-U 0 --minimum-length 25',
        extra_se='--minimum-length 25',
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz.log"
    threads: 8
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 8,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


rule atropos_summary:
    input: rules.atropos.log
    output: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.trim.clean.tsv"
    script: "../scripts/atropos_check.py"

    
###############################################################################
# Alignment
###############################################################################
rule hisat2:
    """Basic alignment."""
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=config['references']['dmel']['known_splice_sites'],
        R1=rules.atropos.output.R1,
        R2=rules.atropos.output.R2,
        layout=fastq_wf("../output/fastq-wf/fastq_info/{srr}/LAYOUT"),
    output:
        bam=temp("../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam")
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.log"
    params:
        hisat2_extra='--max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=config['references']['dmel']['known_splice_sites']),
        samtools_sort_extra='--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    conda: "conda.yaml"
    threads: 8
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 16,
        time_hr=lambda wildcards, attempt: attempt * 4
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    input: rules.hisat2.log
    output: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv"
    script: "../scripts/hisat2_check.py"


rule bai:
    input: rules.hisat2.output.bam
    output: temp("../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.bai")
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    conda: "conda.yaml"
    shell: "samtools index {input[0]}"


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics"
    params:
        extra='STRAND=NONE',
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics.log"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 12,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics"
    params:
        extra='STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics.log"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 12,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics"
    params:
        extra='STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics.log"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 20,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded=rules.collectrnaseqmetrics_unstrand.output.metrics,
        first=rules.collectrnaseqmetrics_first.output.metrics,
        second=rules.collectrnaseqmetrics_second.output.metrics,
    output: "../output/prealn-wf/samples/{srx}/{srr}/STRAND"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 1,
        time_hr=lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srx = wildcards.srx
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(input.unstranded.format(srx=srx,
                                                                  srr=srr))
        dfF = parse_picardCollect_summary(input.first.format(srx=srx, srr=srr))
        dfS = parse_picardCollect_summary(input.second.format(srx=srx,
                                                              srr=srr))

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output[0], flag)


###############################################################################
# Feature Counts
###############################################################################
rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=rules.hisat2.output.bam,
        layout=fastq_wf("../output/fastq-wf/fastq_info/{srr}/LAYOUT"),
        strand=rules.collectrnaseqmetrics_agg.output[0],
    output:
        counts="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts",
        jcounts="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.jcounts",
        summary="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.summary",
    params:
        extra_pe='-p -P -C -J ',
        extra_se='-J '
    threads: 4
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 4,
        time_hr=lambda wildcards, attempt: attempt * 1
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.log"
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam=rules.hisat2.output.bam,
        bai=rules.bai.output[0],
    output:
        samtools_stats="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.stats",
        samtools_idxstats="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.idxstats",
        bamtools_stats="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.bamtools.stats",
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 2,
        time_hr=lambda wildcards, attempt: attempt * 2
    conda: "conda.yaml"
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule markduplicates:
    input:
        bam=rules.hisat2.output.bam
    output:
        bam=temp("../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.bam"),
        metrics="../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics",
    log: "../output/prealn-wf/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics.log"
    resources:
        mem_gb=lambda wildcards, attempt: attempt * 20,
        time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/markduplicates')


###############################################################################
# What to keep
###############################################################################
def _srr_complete(wildcards):
    srr = wildcards.srr
    srx = queue.get_srx(srr)
    return [
        rules.fastq_screen.output.txt.format(srx=srx, srr=srr),
        rules.atropos_summary.output[0].format(srx=srx, srr=srr),
        rules.hisat2_summary.output[0].format(srx=srx, srr=srr),
        rules.collectrnaseqmetrics_agg.output[0].format(srx=srx, srr=srr),
        rules.feature_counts.output.counts.format(srx=srx, srr=srr),
        rules.feature_counts.output.jcounts.format(srx=srx, srr=srr),
        rules.feature_counts.output.summary.format(srx=srx, srr=srr),
        rules.run_stats.output.samtools_stats.format(srx=srx, srr=srr),
        rules.run_stats.output.samtools_idxstats.format(srx=srx, srr=srr),
        rules.run_stats.output.bamtools_stats.format(srx=srx, srr=srr),
        rules.markduplicates.output.metrics.format(srx=srx, srr=srr),
    ]

rule srr_complete:
    input: _srr_complete
    output: "../output/prealn-wf/done/{srr}"
    shell: "touch {output[0]}"
