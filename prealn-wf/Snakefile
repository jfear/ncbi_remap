#/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
import re
from textwrap import dedent
import shutil as sh
import gzip

import numpy as np
import pandas as pd
from pymongo import MongoClient
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib/python')
from ncbi_remap.fastq import check_fastq, md5sum, fastq_stats
from ncbi_remap.snakemake import wrapper_for, put_flag, get_flag


# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yml'


################################################################################
# Build Sample Table
################################################################################
store = pd.HDFStore('../sra.h5')
sample_table = store['prealn/queue'].head(2)

################################################################################
# Set up file naming patterns
################################################################################
patterns = {
    'fastq': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.fastq.gz',
        'summary': 'output/samples/{srx}/{srr}/{srr}.fastq.tsv',
    },
    'layout': 'output/samples/{srx}/{srr}/LAYOUT',
    'fastq_screen': 'output/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt',
    'fastqc': {
        'html': 'output/samples/{srx}/{srr}/{srr}_1.fastqc.html',
        'zip': 'output/samples/{srx}/{srr}/{srr}_1.fastqc.zip',
    },
    'atropos': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': 'output/known_splice_sites_r6-11.txt',
        'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv',
    },
    'bai': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bai',
    'feature_counts': {
        'counts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts',
        'jcounts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.jcounts',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.summary',
    },
    'picard': {
        'collectrnaseqmetrics': {
            'metrics': {
                'unstranded': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics',
                'first': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
                'second': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
            },
        },
        'markduplicates': {
            'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.bam',
            'metrics': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics',
        },
    },
    'strand': 'output/samples/{srx}/{srr}/STRAND',
    'samtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.stats',
    'samtools_idxstats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.idxstats',
    'bamtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bamtools.stats',
}


# Build target files
targets = helpers.fill_patterns(patterns, sample_table)

rule download:
    input:
        targets['fastq']['r1'],
        targets['fastq']['r2'],
        targets['fastq']['summary'],
        targets['layout'],

rule targets:
    input:
        targets['fastq_screen'],
        targets['hisat2']['summary'],
        targets['strand'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats']


onsuccess:
    print('All Finished')
    store.close()


onerror:
    print('Something went wrong, you need to re-run')
    store.close()


################################################################################
# FASTQ dump and check for SE or PE
################################################################################
"""Downloads fastq and checks if there is one or two sets of reads."""
rule fastq_dump:
    output:
        fq1=patterns['fastq']['r1'],
        fq2=patterns['fastq']['r2'],
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    run:
        # Get current sample id
        sample = wildcards.srr

        # Dump FASTQ
        shell("fastq-dump -O $TMPDIR -M 0 --split-files {sample}")

        # Get summaries for R1
        t1 = os.path.join(TMPDIR, sample + '_1.fastq')
        R1md5 = md5sum(t1)
        R1Libsize, R1avgLen = fastq_stats(t1)

        with open(t1, 'rb') as f_in:
            with gzip.open(output.fq1, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)

        # Get Summary for R2
        t2 = os.path.join(TMPDIR, sample + '_2.fastq')
        if check_fastq(fq2):
            # Get md5sum
            R2md5 = md5sum(t2)
            R2Libsize, R2avgLen = fastq_stats(t2)

            with open(t2, 'rb') as f_in:
                with gzip.open(output.fq2, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        else:
            R2md5 = R2Libsize = R2avgLen = None
            Path(output.fq2).touch()

        # Save summary
        df = pd.DataFrame([[R1md5, R1libsize, R1avgLen, R2md5, R2libsize, R2avgLen]],
                          columns=['md5_R1', 'libsize_R1', 'avgLen_R1', 'md5_R2', 'libsize_R2', 'avgLen_R2'])

        df.to_csv(output.summary, sep='\t')

        # Figure out flags
        if (R1Libsize > 1000) & (R2Libsize is None) & (R1avgLen > 10) & (R2avgLen is None):
            put_flag(output.flag, 'SE')

        elif (R1Libsize > 1000) & (R2Libsize > 1000) & (R1avgLen > 10) & (R2avgLen > 10):
            if R1Libsize == R2Libsize:
                # Both reads look good.
                put_flag(output.flag, 'PE')
            else:
                # There are an uneven number of reads between R1 and R2.
                # Instead of messing with this, just consider SE and use R1.
                put_flag(output.flag, 'keep_R1')

        elif (R1Libsize > 1000) & (R1avgLen > 10):
            # Only R1 looks ok, consider single-end
            put_flag(output.flag, 'keep_R1')

        elif (R2Libsize > 1000) & (R2avgLen > 10):
            # Only R2 looks ok, consider single-end, symlink R2 to R1 for proper workflow.
            put_flag(output.flag, 'keep_R2')

        else:
            fname = os.path.join(os.path.dirname(output.fastq), 'DOWNLOAD_BAD')
            Path(fname).touch()


################################################################################
# FASTQ QC
################################################################################
rule fastq_screen:
    input:
        fastq=patterns['fastq']['r1'],
        dm6=config['references']['dmel']['bowtie2'],
        hg19=config['references']['human']['bowtie2'],
        wolbachia=config['references']['wolbachia']['bowtie2'],
        ecoli=config['references']['ecoli']['bowtie2'],
        yeast=config['references']['yeast']['bowtie2'],
        rRNA=config['references']['rRNA']['bowtie2'],
        phix=config['references']['phix']['bowtie2'],
        ercc=config['references']['ercc']['bowtie2'],
        adapters=config['references']['adapters']['bowtie2']
    output:
        txt=patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    wrapper:
        wrapper_for('fastq_screen')


################################################################################
# FASTQ Pre-process
################################################################################
def _atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flag = get_flag(patterns['layout'].format(**wildcards))
    if flag == 'PE':
        return {'R1': expand(patterns['fastq']['r1'], **wildcards)[0], 'R2': expand(patterns['fastq']['r2'], **wildcards)[0]}
    elif flag == 'keep_R2':
        return {'R1': expand(patterns['fastq']['r2'], **wildcards)[0]}
    else:
        return {'R1': expand(patterns['fastq']['r1'], **wildcards)[0]}


def _params_extra_atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flag = get_flag(patterns['layout'].format(**wildcards))
    if flag == 'PE':
        return '-U 0 --minimum-length 25'
    else:
        return '--minimum-length 25'


def _params_r2_atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flag = get_flag(patterns['layout'].format(**wildcards))
    if flag == 'PE':
        return expand(patterns['atropos']['r2'], **wildcards)[0] + '.tmp.gz'
    else:
        return None


rule atropos:
    input: unpack(_atropos)
    output:
        R1=temp(patterns['atropos']['r1'])
    params:
        extra=_params_extra_atropos,
        R2=_params_r2_atropos
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper:
        wrapper_for('atropos')


rule atropos_phony:
    input: rules.atropos.output
    output: temp(patterns['atropos']['r2'])
    shell:
        "mv {output[0]}.tmp.gz {output[0]}"


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda: "../config/extra_env.yaml"
    shell:
        "hisat2_extract_splice_sites.py {input.gtf} > {output}"


def _hisat2(wildcards):
    flag = get_flag(patterns['layout'].format(**wildcards))
    if flag == 'PE':
        return expand(patterns['atropos']['r1'], **wildcards)[0], expand(patterns['atropos']['r2'], **wildcards)[0]
    elif flag == 'keep_R2':
        return expand(patterns['atropos']['r2'], **wildcards)[0]
    else:
        return expand(patterns['atropos']['r1'], **wildcards)[0]


rule hisat2:
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=_hisat2,
    output:
        bam=temp(patterns['hisat2']['bam'])
    threads: 8
    params:
        hisat2_extra='--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper:
        wrapper_for('hisat2/align')


rule hisat2_summary:
    input:
        fn=patterns['hisat2']['bam']
    output: patterns['hisat2']['summary']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.sample
        df = parse_hisat2(srr, input.fn + '.log')
        df.to_csv(output, sep='\t')

        if df.ix[0, 'per_alignment'] < .50:
            fname = os.path.join(os.path.dirname(output), 'ALIGNMENT_BAD')
            Path(fname).touch()


rule bai:
    input:
        bam='{prefix}.bam'
    output:
        bai=temp('{prefix}.bam.bai')
    conda: "../config/extra_env.yaml"
    shell:
        "samtools index {input.bam}"


################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra='STRAND=NONE',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    wrapper:
        wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra='STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    wrapper:
        wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam=rules.hisat2.output.bam,
        refflat=config['references']['dmel']['refflat']
    output:
        metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra='STRAND=SECOND_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    wrapper:
        wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first=patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second=patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag=patterns['strand']
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srr = wildcards.sample
        dfU = parse_picardCollect_summary(srr, input.unstranded)
        dfF = parse_picardCollect_summary(srr, input.first)
        dfS = parse_picardCollect_summary(srr, input.second)

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine if the sample is PE or SE"""
    flag = get_flag(patterns['layout'].format(**wildcards))
    if flag == 'PE':
        base = '-p -P -C -J '
    else:
        base = '-J '

    strand = get_flag(patterns['strand'].format(**wildcards))
    if strand == 'first_strand':
        return base + '-s 1'
    elif strand == 'second_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam'],
    output:
        counts=patterns['feature_counts']['counts'],
        jcounts=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    params:
        extra=_params_featurecounts
    threads: 4
    log:
        patterns['feature_counts']['counts'] + '.log'
    wrapper:
        wrapper_for('featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule markduplicates:
    input:
        bam=rules.hisat2.output.bam
    output:
        bam=temp(patterns['picard']['markduplicates']['bam']),
        metrics=patterns['picard']['markduplicates']['metrics']
    params:
        java_args='-Xmx30g'
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    wrapper:
        wrapper_for('picard/markduplicates')


# vim: set ft=snakemake.python
