import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib')
from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns


# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


###############################################################################
# Build Sample Table
###############################################################################
with open('../aln-wf/output/priority.txt', 'r') as fh:
    rnaseq_srxs = fh.read().strip().split()

mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
srxs = [x for x in store['prealn/queue'].srx.unique() if x in rnaseq_srxs]
mask = store['prealn/queue'].srx.isin(srxs[:3000])
sample_table = store['prealn/queue'][mask]
store.close()

#mystore = '../sra.h5'
#store = pd.HDFStore(mystore, mode='r')
#queue = store['prealn/queue']
#srxs = queue.srx.unique()[:3000]
#sample_table = queue[queue.srx.isin(srxs)]
#store.close()

# NOTE: pulling just modENCODE samples for running tau and TSPS
# sample_table = pd.read_csv('../output/modencode_samples.tsv', sep='\t')
# srxs = sample_table.srx.unique()
# sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
###############################################################################
# Set up file naming patterns and targets
###############################################################################
patterns = get_patterns('patterns.yaml')
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['fastq_screen'],
        targets['layout'],
        targets['strand'],
        targets['hisat2']['summary'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats'],
        targets['picard']['markduplicates']['metrics']
    ]


rule targets:
    input: keepers(targets)


rule download:
    """Only download the FASTQs."""
    input: targets['fastq']['r1']


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('prealn-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('prealn-wf: Something went wrong, you need to re-run')


##############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads.

    If there are less than 1000 reads or reads are less than 10bp long a
    indicator file `DOWNLOAD_BAD` is created.
    """
    output:
        fq1 = patterns['fastq']['r1'],
        fq2 = patterns['fastq']['r2'],
        flag = patterns['layout'],
        summary = patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 1,
      time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ QC
###############################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq = patterns['fastq']['r1'],
        dm6 = config['references']['dmel']['bowtie2'],
        hg19 = config['references']['human']['bowtie2'],
        wolbachia = config['references']['wolbachia']['bowtie2'],
        ecoli = config['references']['ecoli']['bowtie2'],
        yeast = config['references']['yeast']['bowtie2'],
        rRNA = config['references']['rRNA']['bowtie2'],
        phix = config['references']['phix']['bowtie2'],
        ercc = config['references']['ercc']['bowtie2'],
        adapters = config['references']['adapters']['bowtie2']
    output:
        txt = patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 3,
      time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1 = patterns['fastq']['r1'],
        R2 = patterns['fastq']['r2'],
        layout = patterns['layout']
    output:
        R1 = temp(patterns['atropos']['r1']),
        R2 = temp(patterns['atropos']['r2']),
    params:
        extra_pe = '-U 0 --minimum-length 25',
        extra_se = '--minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf = config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag = patterns['layout'],
        index = config['references']['dmel']['hisat2'],
        splice_sites = patterns['hisat2']['splice_sites'],
        R1 = patterns['atropos']['r1'],
        R2 = patterns['atropos']['r2'],
        layout = patterns['layout']
    output:
        bam = temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 16,
        time_hr = lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra = '--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and add indicator file.

    If the alignment is <50% of reads then add indicator file `ALIGNMENT_BAD`.
   """
    input:
        fn = patterns['hisat2']['bam']
    output:
        tsv = patterns['hisat2']['summary']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        fname = input.fn + '.log'
        df = parse_hisat2(fname.format(srx=srx, srr=srr))
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['download_bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam = '{prefix}.bam'
    output:
        bai = temp('{prefix}.bam.bai')
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell:
        "samtools index {input.bam}"


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra = 'STRAND=NONE',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra = 'STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra = 'STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first = patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second = patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag = patterns['strand']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srx = wildcards.srx
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(input.unstranded.format(srx=srx,
                                                                  srr=srr))
        dfF = parse_picardCollect_summary(input.first.format(srx=srx, srr=srr))
        dfS = parse_picardCollect_summary(input.second.format(srx=srx,
                                                              srr=srr))

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


###############################################################################
# Feature Counts
###############################################################################
rule feature_counts:
    input:
        annotation = config['references']['dmel']['gtf'],
        bam = patterns['hisat2']['bam'],
        layout = patterns['layout'],
        strand = patterns['strand'],
    output:
        counts = patterns['feature_counts']['counts'],
        jcounts = patterns['feature_counts']['jcounts'],
        summary = patterns['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 4
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 4,
        time_hr = lambda wildcards, attempt: attempt * 1
    log:
        patterns['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam = patterns['hisat2']['bam'],
        bai = patterns['bai'],
    output:
        samtools_stats = patterns['samtools_stats'],
        samtools_idxstats = patterns['samtools_idxstats'],
        bamtools_stats = patterns['bamtools_stats']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule markduplicates:
    input:
        bam = rules.hisat2.output.bam
    output:
        bam = temp(patterns['picard']['markduplicates']['bam']),
        metrics = patterns['picard']['markduplicates']['metrics']
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/markduplicates')


# vim: set ft=snakemake.python
