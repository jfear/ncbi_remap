import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib')
from ncbi_remap.snakemake import wrapper_for, put_flag


# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


###############################################################################
# Build Sample Table
###############################################################################
mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
srxs = store['prealn/queue'].srx.unique()[:1000]
mask = store['prealn/queue'].srx.isin(srxs)
sample_table = store['prealn/queue'][mask].copy()
store.close()

# NOTE: pulling just modENCODE samples for running tau and TSPS
sample_table = pd.read_csv('../output/modencode_samples.tsv', sep='\t')
srxs = sample_table.srx.unique()
sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
###############################################################################
# Set up file naming patterns
###############################################################################
patterns = {
    'fastq': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.fastq.gz',
        'summary': 'output/samples/{srx}/{srr}/{srr}.fastq.tsv',
    },
    'layout': 'output/samples/{srx}/{srr}/LAYOUT',
    'fastq_screen': 'output/samples/{srx}/{srr}/{srr}_1.fastq_screen.txt',
    'atropos': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': 'output/known_splice_sites_r6-11.txt',
        'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.tsv',
        'bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',
    },
    'bai': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bai',
    'feature_counts': {
        'counts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts',
        'jcounts': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.jcounts',
        'summary': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.feature_counts.counts.summary',
    },
    'picard': {
        'collectrnaseqmetrics': {
            'metrics': {
                'unstranded': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.NONE.picard.collectrnaseqmetrics',
                'first': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
                'second': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
            },
        },
        'markduplicates': {
            'bam': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.bam',
            'metrics': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.picard.markduplicates.metrics',
        },
    },
    'strand': 'output/samples/{srx}/{srr}/STRAND',
    'samtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.stats',
    'samtools_idxstats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.samtools.idxstats',
    'bamtools_stats': 'output/samples/{srx}/{srr}/{srr}.hisat2.bam.bamtools.stats',
}


###############################################################################
# Set up Build Targets
###############################################################################
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['fastq_screen'],
        targets['layout'],
        targets['strand'],
        targets['hisat2']['summary'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats'],
        targets['picard']['markduplicates']['metrics']
    ]


rule targets:
    input: keepers(targets)


rule download:
    """Only download the FASTQs."""
    input: targets['fastq']['r1']


onsuccess:
    print('All Finished')


onerror:
    print('Something went wrong, you need to re-run')


###############################################################################
# Summary Rules
###############################################################################
rule update_queue:
    """Updates queue.

    After running targets you need to update the queue. This job itereates over
    outputs and moves ids around in the queing system.
    """
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.io import add_table, remove_chunk, add_id, remove_id
        from ncbi_remap.snakemake import (check_download, check_alignment,
                                          check_layout, check_strand)

        def check_outputs(**kwargs):
            tg = helpers.fill_patterns(patterns, kwargs)
            for fname in utils.flatten(keepers(tg)):
                if not os.path.exists(fname):
                    return
            return kwargs

        store = pd.HDFStore(mystore)

        # Initialize values if needed
        if not store.get_node('layout'):
            store['layout'] = pd.Series(
                index=store['ids'].set_index(['srx', 'srr']).index,
                name='layout'
            ).fillna('Missing')

        if not store.get_node('strand'):
            store['strand'] = pd.Series(
                index=store['ids'].set_index(['srx', 'srr']).index,
                name='strand'
            ).fillna('Missing')

        if not store.get_node('prealn/flags'):
            store['prealn/flags'] = pd.DataFrame(
                data=[], index=store['ids'].set_index(['srx', 'srr']).index,
                columns=['flag_abi_solid', 'flag_alignment_bad',
                         'flag_complete', 'flag_download_bad',
                         'flag_quality_scores_bad', 'flag_qc_passed',
                         'flag_merge', ]
            ).fillna(False)

        done = []
        for i, row in sample_table.iterrows():
            curr = row.to_dict()

            if check_download(store, patterns['fastq']['r1'] + '.log', **curr):
                continue

            if check_alignment(store, patterns['hisat2']['bad'], **curr):
                continue

            val = check_outputs(**curr)
            if val is not None:
                done.append(val)
                check_layout(store, patterns['layout'], **curr)
                check_strand(store, patterns['strand'], **curr)

        df = pd.DataFrame(done)

        # Add to complete, add to alignment queue, remove from prealn queue
        add_table(store, 'prealn/complete', data=df)
        add_table(store, 'aln/queue', data=df)
        remove_chunk(store, 'prealn/queue', df.srr.tolist())

        flags = store['prealn/flags']
        flags.loc[df.set_index(['srx', 'srr']).index, 'flag_complete'] = True
        store['prealn/flags'] = flags

        store.close()


rule print_queue:
    """Prints the current counts for the queue."""
    run:
        store = pd.HDFStore(mystore, mode='r')

        def cnts(key):
            if store.__contains__(key):
                return store[key].shape[0]
            else:
                return 0

        layout = store['layout'].value_counts()
        strand = store['strand'].value_counts()

        pairs = [
            ("ids in the system", cnts('ids')),
            ("queued", cnts("prealn/queue")),
            ("completed", store['prealn/flags']['flag_completed'].sum()),
            ("download bad", store['prealn/flags']['flag_download_bad'].sum()),
            ("quality scores bad", store['prealn/flags']['flag_quality_scores_bad'].sum()),
            ("alignment bad", store['prealn/flags']['flag_alignment_bad'].sum()),
            ("abi solid", store['prealn/flags']['flag_abi_solid'].sum()),
            ("Single End", layout.SE),
            ("Pair End", layout.PE),
            ("Really Single End R1", layout.keep_R1),
            ("Really Single End R2", layout.keep_R2),
            ("first strand", strand.first),
            ("second strand", strand.second),
            ("unstranded", strand.unstranded),
        ]

        report = '\nCurrent Queue Summary\n'
        for k, v in pairs:
            report += '{:,}\t\t\t{}\n'.format(v, k)

        print(report)
        store.close()


rule aggregate:
    """Aggregate outputs from different tools."""
    threads: 8
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.logging import logger
        from ncbi_remap.io import add_table
        from ncbi_remap.snakemake import agg
        from ncbi_remap.parser import (
            parse_fastq_summary, parse_fastq_screen, parse_hisat2,
            parse_featureCounts_summary, parse_picardCollect_summary,
            parse_picard_markduplicate_metrics, parse_samtools_stats,
            parse_bamtools_stats, parse_featureCounts_counts,
            parse_featureCounts_jcounts, parse_samtools_idxstats
        )

        store = pd.HDFStore(mystore)
        flags = store['prealn/flags']
        completed = flags[flags['flag_complete'].reset_index()[['srx', 'srr']]]

        # fastq summary
        logger.info('Parsing Fastq Summary')
        agg(store, 'prealn/workflow/fastq', parse_fastq_summary,
            patterns['fastq']['summary'], completed)

        # fastq_screen
        logger.info('Parsing Fastq Screen')
        agg(store, 'prealn/workflow/fastq_screen', parse_fastq_screen,
            patterns['fastq_screen'], completed)

        # Hisat2
        logger.info('Parsing Hisat2')
        agg(store, 'prealn/workflow/hisat2', parse_hisat2,
            patterns['hisat2']['bam'] + '.log', completed)

        # stats
        logger.info('Parsing Other Stats')
        logger.info('Parsing Other Stats - Samtools Stats')
        agg(store, 'prealn/workflow/samtools_stats', parse_samtools_stats,
            patterns['samtools_stats'], completed)

        logger.info('Parsing Other Stats - Bamtools Stats')
        agg(store, 'prealn/workflow/bamtools_stats', parse_bamtools_stats,
            patterns['bamtools_stats'], completed)

        # MarkDuplicates
        logger.info('Parsing Mark Duplicates')
        agg(store, 'prealn/workflow/markduplicates',
            parse_picard_markduplicate_metrics,
            patterns['picard']['markduplicates']['metrics'], completed)

        # CollectRNASeqMetrics
        logger.info('Parsing Collect RNA-Seq Metrics')
        logger.info('Parsing Collect RNA-Seq Metrics - First')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/first',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
            completed)

        logger.info('Parsing Collect RNA-Seq Metrics - Second')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/second',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
            completed)

        logger.info('Parsing Collect RNA-Seq Metrics - Unstranded')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/unstranded',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
            completed)

        # Feature Counts
        logger.info('Parsing Feature Counts')
        logger.info('Parsing Feature Counts - Summary')
        agg(store, 'prealn/workflow/feature_counts/summary',
            parse_featureCounts_summary,
            patterns['feature_counts']['summary'], completed)

        logger.info('Parsing Large Datasets')
        logger.info('Parsing Other Stats - Samtools Index Stats')
        agg(store, 'prealn/workflow/samtools_idxstats',
            parse_samtools_idxstats, patterns['samtools_idxstats'],
            completed, large=True)

        logger.info('Parsing Feature Counts - Counts')
        agg(store, 'prealn/workflow/feature_counts/counts',
            parse_featureCounts_counts, patterns['feature_counts']['counts'],
            completed, large=True)

        logger.info('Parsing Feature Counts - Junction Counts')
        agg(store, 'prealn/workflow/feature_counts/jcounts',
            parse_featureCounts_jcounts,
            patterns['feature_counts']['jcounts'], completed, large=True)

        store.close()


###############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1 = patterns['fastq']['r1'],
        fq2 = patterns['fastq']['r2'],
        flag = patterns['layout'],
        summary = patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 1,
      time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ QC
###############################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq = patterns['fastq']['r1'],
        dm6 =config['references']['dmel']['bowtie2'],
        hg19 = config['references']['human']['bowtie2'],
        wolbachia = config['references']['wolbachia']['bowtie2'],
        ecoli = config['references']['ecoli']['bowtie2'],
        yeast = config['references']['yeast']['bowtie2'],
        rRNA = config['references']['rRNA']['bowtie2'],
        phix = config['references']['phix']['bowtie2'],
        ercc = config['references']['ercc']['bowtie2'],
        adapters = config['references']['adapters']['bowtie2']
    output:
        txt = patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 3,
      time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1 = patterns['fastq']['r1'],
        R2 = patterns['fastq']['r2'],
        layout = patterns['layout']
    output:
        R1 = temp(patterns['atropos']['r1']),
        R2 = temp(patterns['atropos']['r2']),
    params:
        extra_pe = '-U 0 --minimum-length 25',
        extra_se = '--minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf = config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag = patterns['layout'],
        index = config['references']['dmel']['hisat2'],
        splice_sites = patterns['hisat2']['splice_sites'],
        R1 = patterns['atropos']['r1'],
        R2 = patterns['atropos']['r2'],
        layout = patterns['layout']
    output:
        bam = temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 16,
        time_hr = lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra = '--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn = patterns['hisat2']['bam']
    output:
        tsv = patterns['hisat2']['summary']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        df = parse_hisat2(srx, srr, input.fn + '.log')
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['hisat2']['bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam = '{prefix}.bam'
    output:
        bai = temp('{prefix}.bam.bai')
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell:
        "samtools index {input.bam}"


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra = 'STRAND=NONE',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra = 'STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra = 'STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first = patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second = patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag = patterns['strand']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srx = wildcards.srx
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(srx, srr, input.unstranded)
        dfF = parse_picardCollect_summary(srx, srr, input.first)
        dfS = parse_picardCollect_summary(srx, srr, input.second)

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


###############################################################################
# Feature Counts
###############################################################################
rule feature_counts:
    input:
        annotation = config['references']['dmel']['gtf'],
        bam = patterns['hisat2']['bam'],
        layout = patterns['layout'],
        strand = patterns['strand'],
    output:
        counts = patterns['feature_counts']['counts'],
        jcounts = patterns['feature_counts']['jcounts'],
        summary = patterns['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 4
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 4,
        time_hr = lambda wildcards, attempt: attempt * 1
    log:
        patterns['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam = patterns['hisat2']['bam'],
        bai = patterns['bai'],
    output:
        samtools_stats = patterns['samtools_stats'],
        samtools_idxstats = patterns['samtools_idxstats'],
        bamtools_stats = patterns['bamtools_stats']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule markduplicates:
    input:
        bam = rules.hisat2.output.bam
    output:
        bam = temp(patterns['picard']['markduplicates']['bam']),
        metrics = patterns['picard']['markduplicates']['metrics']
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/markduplicates')


# vim: set ft=snakemake.python
