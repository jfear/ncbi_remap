import os
import sys

import pandas as pd
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib')
from ncbi_remap.io import HDFStore
from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns


# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


###############################################################################
# Build Sample Table
###############################################################################
mystore = '../sra.h5'
store = HDFStore(mystore, mode='r')
srxs = store.get_srxs('prealn/queue')[:1000]
mask = store['prealn/queue'].loc[(srxs, slice(None)), :]
sample_table = store['prealn/queue'][mask].reset_index()
store.close()

# NOTE: pulling just modENCODE samples for running tau and TSPS
sample_table = pd.read_csv('../output/modencode_samples.tsv', sep='\t')
srxs = sample_table.srx.unique()
sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
###############################################################################
# Set up file naming patterns and targets
###############################################################################
patterns = get_patterns('patterns.yaml')
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['fastq_screen'],
        targets['layout'],
        targets['strand'],
        targets['hisat2']['summary'],
        targets['feature_counts']['summary'],
        targets['samtools_stats'],
        targets['samtools_idxstats'],
        targets['bamtools_stats'],
        targets['picard']['markduplicates']['metrics']
    ]


rule targets:
    input: keepers(targets)


rule download:
    """Only download the FASTQs."""
    input: targets['fastq']['r1']


onsuccess:
    print('All Finished')


onerror:
    print('Something went wrong, you need to re-run')


###############################################################################
# Summary Rules
###############################################################################
rule update_queue:
    """Updates queue.

    After running targets you need to update the queue. This job itereates over
    outputs and moves ids around in the queing system.
    """
    run:
        from ncbi_remap.prealn_wf import (check_download, check_indicator_file,
                                          check_flag_file)

        def check_outputs(srx, srr, **kwargs):
            fill = {
                'srx': srx,
                'srr': srr
            }
            fill.update(kwargs)
            tg = helpers.fill_patterns(patterns, fill)
            for fname in utils.flatten(keepers(tg)):
                if not os.path.exists(fname):
                    return
            return srx, srr

        store = HDFStore(mystore)

        # Update
        done = []
        for (srx, srr), _ in store['prealn/queue'].iterrows():
            curr = {'srx': srx, 'srr': srr}

            if check_download(store, patterns['fastq']['r1'] + '.log', **curr):
                continue

            if check_indicator_file(store, 'prealn/quality_scores_bad',
                                    patterns['fastq']['quality'], **curr):
                continue

            if check_indicator_file(store, 'prealn/abi_solid',
                                    patterns['fastq']['abi'], **curr):
                continue

            if check_indicator_file(store, 'prealn/alignment_bad',
                                    patterns['hisat2']['bad'], **curr):
                continue

            val = check_outputs(**curr)
            if val is not None:
                done.append(val)
                check_flag_file(store, 'layout', patterns['layout'], **curr)
                check_flag_file(store, 'strand', patterns['strand'], **curr)

        if len(done) > 0:
            df = pd.DataFrame(done).set_index(['srx', 'srr'])

            # Add to complete, add to alignment queue, remove from prealn queue
            store.push('prealn/complete', df)
            store.push('aln/queue', df)
            store.pop('prealn/queue', df)

        store.close()


rule print_queue:
    """Prints the current counts for the queue."""
    run:
        store = HDFStore(mystore, mode='r')

        def cnts(key):
            if store.__contains__(key):
                return store[key].value_counts()[True]
            else:
                return 0

        layout = store['layout'].value_counts()
        strand = store['strand'].value_counts()

        pairs = [
            ("ids in the system", store['ids'].shape[0]),
            ("queued", store["prealn/queue"].shape[0]),
            ("completed", store['prealn/complete'].shape[0]),
            ("download bad", cnts('prealn/download_bad')),
            ("quality scores bad", cnts('prealn/quality_scores_bad')),
            ("alignment bad", cnts('prealn/alignment_bad')),
            ("abi solid", cnts('prealn/abi_solid')),
            ("Single End", layout.SE),
            ("Pair End", layout.PE),
            ("Really Single End R1", layout.keep_R1),
            ("Really Single End R2", layout.keep_R2),
            ("first strand", strand.same_strand),
            ("second strand", strand.opposite_strand),
            ("unstranded", strand.unstranded),
        ]

        report = '\nCurrent Queue Summary\n'
        for k, v in pairs:
            report += '{:,}\t\t\t{}\n'.format(v, k)

        print(report)
        store.close()


rule aggregate:
    """Aggregate outputs from different tools."""
    threads: 8
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.logging import logger
        from ncbi_remap.io import add_table
        from ncbi_remap.snakemake import agg
        from ncbi_remap.parser import (
            parse_fastq_summary, parse_fastq_screen, parse_hisat2,
            parse_featureCounts_summary, parse_picardCollect_summary,
            parse_picard_markduplicate_metrics, parse_samtools_stats,
            parse_bamtools_stats, parse_featureCounts_counts,
            parse_featureCounts_jcounts, parse_samtools_idxstats
        )

        store = HDFStore(mystore)
        flags = store['prealn/flags']
        completed = flags[flags['flag_complete'].reset_index()[['srx', 'srr']]]

        # fastq summary
        logger.info('Parsing Fastq Summary')
        agg(store, 'prealn/workflow/fastq', parse_fastq_summary,
            patterns['fastq']['summary'], completed)

        # fastq_screen
        logger.info('Parsing Fastq Screen')
        agg(store, 'prealn/workflow/fastq_screen', parse_fastq_screen,
            patterns['fastq_screen'], completed)

        # Hisat2
        logger.info('Parsing Hisat2')
        agg(store, 'prealn/workflow/hisat2', parse_hisat2,
            patterns['hisat2']['bam'] + '.log', completed)

        # stats
        logger.info('Parsing Other Stats')
        logger.info('Parsing Other Stats - Samtools Stats')
        agg(store, 'prealn/workflow/samtools_stats', parse_samtools_stats,
            patterns['samtools_stats'], completed)

        logger.info('Parsing Other Stats - Bamtools Stats')
        agg(store, 'prealn/workflow/bamtools_stats', parse_bamtools_stats,
            patterns['bamtools_stats'], completed)

        # MarkDuplicates
        logger.info('Parsing Mark Duplicates')
        agg(store, 'prealn/workflow/markduplicates',
            parse_picard_markduplicate_metrics,
            patterns['picard']['markduplicates']['metrics'], completed)

        # CollectRNASeqMetrics
        logger.info('Parsing Collect RNA-Seq Metrics')
        logger.info('Parsing Collect RNA-Seq Metrics - First')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/first',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
            completed)

        logger.info('Parsing Collect RNA-Seq Metrics - Second')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/second',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
            completed)

        logger.info('Parsing Collect RNA-Seq Metrics - Unstranded')
        agg(store, 'prealn/workflow/collectrnaseqmetrics/unstranded',
            parse_picardCollect_summary,
            patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
            completed)

        # Feature Counts
        logger.info('Parsing Feature Counts')
        logger.info('Parsing Feature Counts - Summary')
        agg(store, 'prealn/workflow/feature_counts/summary',
            parse_featureCounts_summary,
            patterns['feature_counts']['summary'], completed)

        logger.info('Parsing Large Datasets')
        logger.info('Parsing Other Stats - Samtools Index Stats')
        agg(store, 'prealn/workflow/samtools_idxstats',
            parse_samtools_idxstats, patterns['samtools_idxstats'],
            completed, large=True)

        logger.info('Parsing Feature Counts - Counts')
        agg(store, 'prealn/workflow/feature_counts/counts',
            parse_featureCounts_counts, patterns['feature_counts']['counts'],
            completed, large=True)

        logger.info('Parsing Feature Counts - Junction Counts')
        agg(store, 'prealn/workflow/feature_counts/jcounts',
            parse_featureCounts_jcounts,
            patterns['feature_counts']['jcounts'], completed, large=True)

        store.close()


###############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1 = patterns['fastq']['r1'],
        fq2 = patterns['fastq']['r2'],
        flag = patterns['layout'],
        summary = patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 1,
      time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ QC
###############################################################################
rule fastq_screen:
    """Check for contamination."""
    input:
        fastq = patterns['fastq']['r1'],
        dm6 =config['references']['dmel']['bowtie2'],
        hg19 = config['references']['human']['bowtie2'],
        wolbachia = config['references']['wolbachia']['bowtie2'],
        ecoli = config['references']['ecoli']['bowtie2'],
        yeast = config['references']['yeast']['bowtie2'],
        rRNA = config['references']['rRNA']['bowtie2'],
        phix = config['references']['phix']['bowtie2'],
        ercc = config['references']['ercc']['bowtie2'],
        adapters = config['references']['adapters']['bowtie2']
    output:
        txt = patterns['fastq_screen']
    log: patterns['fastq_screen'] + '.log'
    resources:
      mem_gb = lambda wildcards, attempt: attempt * 3,
      time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/fastq_screen')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1 = patterns['fastq']['r1'],
        R2 = patterns['fastq']['r2'],
        layout = patterns['layout']
    output:
        R1 = temp(patterns['atropos']['r1']),
        R2 = temp(patterns['atropos']['r2']),
    params:
        extra_pe = '-U 0 --minimum-length 25',
        extra_se = '--minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 8,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf = config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag = patterns['layout'],
        index = config['references']['dmel']['hisat2'],
        splice_sites = patterns['hisat2']['splice_sites'],
        R1 = patterns['atropos']['r1'],
        R2 = patterns['atropos']['r2'],
        layout = patterns['layout']
    output:
        bam = temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 16,
        time_hr = lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra = '--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn = patterns['hisat2']['bam']
    output:
        tsv = patterns['hisat2']['summary']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        df = parse_hisat2(srx, srr, input.fn + '.log')
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['hisat2']['bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam = '{prefix}.bam'
    output:
        bai = temp('{prefix}.bam.bai')
    conda:
        "conda.yaml"
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    shell:
        "samtools index {input.bam}"


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra = 'STRAND=NONE',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra = 'STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 12,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam = rules.hisat2.output.bam,
        refflat = config['references']['dmel']['refflat']
    output:
        metrics = patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra = 'STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log:
        patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_agg:
    input:
        unstranded = patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first = patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second = patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        flag = patterns['strand']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srx = wildcards.srx
        srr = wildcards.srr
        dfU = parse_picardCollect_summary(srx, srr, input.unstranded)
        dfF = parse_picardCollect_summary(srx, srr, input.first)
        dfS = parse_picardCollect_summary(srx, srr, input.second)

        if dfF.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'same_strand'
        elif dfS.PCT_CORRECT_STRAND_READS.values[0] >= .75:
            flag = 'opposite_strand'
        else:
            flag = 'unstranded'

        put_flag(output.flag, flag)


###############################################################################
# Feature Counts
###############################################################################
rule feature_counts:
    input:
        annotation = config['references']['dmel']['gtf'],
        bam = patterns['hisat2']['bam'],
        layout = patterns['layout'],
        strand = patterns['strand'],
    output:
        counts = patterns['feature_counts']['counts'],
        jcounts = patterns['feature_counts']['jcounts'],
        summary = patterns['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 4
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 4,
        time_hr = lambda wildcards, attempt: attempt * 1
    log:
        patterns['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam = patterns['hisat2']['bam'],
        bai = patterns['bai'],
    output:
        samtools_stats = patterns['samtools_stats'],
        samtools_idxstats = patterns['samtools_idxstats'],
        bamtools_stats = patterns['bamtools_stats']
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 1,
        time_hr = lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# PICARD RNA Seq Metrics
###############################################################################
rule markduplicates:
    input:
        bam = rules.hisat2.output.bam
    output:
        bam = temp(patterns['picard']['markduplicates']['bam']),
        metrics = patterns['picard']['markduplicates']['metrics']
    log:
        patterns['picard']['markduplicates']['metrics'] + '.log'
    resources:
        mem_gb = lambda wildcards, attempt: attempt * 20,
        time_hr = lambda wildcards, attempt: attempt * 12
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/picard/markduplicates')


# vim: set ft=snakemake.python
