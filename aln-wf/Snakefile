"""Alignment workflow.

The goal of the alignment workflow is to use parameters determined in the
pre-alignment workflow and processing files. This workflow uses a queue in
`../output/sra.h5`. To update the queue you need to run `./aln-store.py queue update
-j 8`. The major file types output by the alignment workflow include:

* Strand specific BigWig tracks
* Gene level coverage counts and junction counts
* Intergenic coverage counts and junction counts

Rules
-----
targets
    create a list of desired output files
intergenic
    create an intergenic bed and gtf based on the flybase gtf
fastq_dump
    download fastq file from SRA, determine if the file is pair-end or
    single-end and count the number of reads and average read length
atropos
    trim illumina adapters and low quality bases (<20) from fastq file, remove
    reads that have fewer than 25 bp
hisat2_splice_site
    create set of splice sites using the flybase gtf
hisat2
    align reads to the flybase reference
hisat2_summary
    parse alignment log and determine if the alignment failed (<50% aligned)
expMerge
    merge bams (SRRs) to the library (SRX) level
feature_counts
    count the number of reads that overlap genic regions and junction counts
featurecounts_intergenic
    count the number of reads that overlap intergenic regions and junction
    counts
run_stats
    calculate basic stats with `samtools stats`, `samtools idxstats`, and
    `bamtools stats`
bamCoverage
    create strand specific coverage tracks (bedgraph) using deeptools
convertToFlybase
    convert bedgraph chromosome names (UCSC format) to flybase chromosome names
convertBedGraphToBigWig
    convert begraph to bigwig
"""

import os
import sys
from pathlib import Path

from more_itertools import flatten
import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

from ncbi_remap.queue import get_samples
from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns

# Setup tempdir to work with lscratch
if os.getenv("SLURM_JOBID", False):
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
else:
    TMPDIR = os.getenv('TMPDIR', "/tmp")
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'

localrules: atropos_summary, hisat2_summary, srx_complete

###############################################################################
# Set up file naming patterns and targets
###############################################################################
patterns = get_patterns('patterns.yaml')

# srxs = get_samples(patterns["rnaseq_srxs"], "../output/aln-wf/done", "../output/done.pkl", size=100)
srxs = get_samples("../output/done.pkl", "../output/aln-wf/done", size=5000)
sample_table = pd.read_csv(patterns["srx2srr"]).query(f"srx == {srxs}")

targets = helpers.fill_patterns(patterns, sample_table)

rule all:
    input: targets["done"]


rule srx_complete:
    input: utils.flatten(patterns["srxMerge"])
    output: patterns["done"]
    shell: "touch {output[0]}"


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('aln-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('aln-wf: Something went wrong, you need to re-run')


###############################################################################
# Build References
###############################################################################
rule intergenic:
    input: config['references']['dmel']['db']
    output:
        bed=patterns['intergenic']['bed'],
        gtf=patterns['intergenic']['gtf']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: "scripts/intergenic_region.py"


rule segment_exons_ignore_strand:
    input: config['references']['dmel']['db']
    output: patterns["segments"]["nonstranded"]
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: "scripts/segment_exons_ignore_strand.py"


rule segment_exons_with_strand:
    input: config['references']['dmel']['db']
    output: patterns["segments"]["stranded"]
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: "scripts/segment_exons_with_strand.py"


rule fuse_exons_ignore_strand:
    input: config['references']['dmel']['db']
    output: patterns["fusions"]["nonstranded"]
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: "scripts/fuse_exons_ignore_strand.py"


rule fuse_exons_with_strand:
    input: config['references']['dmel']['db']
    output: patterns["fusions"]["stranded"]
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    script: "scripts/fuse_exons_with_strand.py"


###############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
# TODO: Refactor to use prefetch and fasterq-dump
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1=temp(patterns['fastq']['r1']),
        fq2=temp(patterns['fastq']['r2']),
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=patterns['fastq']['r1'],
        R2=patterns['fastq']['r2'],
        layout=patterns['layout']
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2']),
    params:
        extra_pe='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25',
        extra_se='-a file:../data/adapters.fa -q 20 --minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 12
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


rule atropos_summary:
    input: rules.atropos.log
    output: patterns["atropos"]["summary"]
    script: "../scripts/atropos_check.py"


###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda: "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        R1=patterns['atropos']['r1'],
        R2=patterns['atropos']['r2'],
        layout=patterns['layout'],
        strand=patterns['strand']
    output:
        bam=temp(patterns['hisat2']['bam']),
        bai=temp(patterns['hisat2']['bai'])
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra='--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites']),
        samtools_view_extra="--threads 4 -q 20",
        samtools_sort_extra='--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    input: rules.hisat2.log
    output: patterns['hisat2']['summary']
    script: "../scripts/hisat2_check.py"


###############################################################################
# Merge Bams
###############################################################################
def _input_expMerge(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique().tolist()
    }
    return expand(patterns['hisat2']['bam'], **fill)


rule expMerge:
    input: _input_expMerge
    output:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    conda: 'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    threads: 4
    shell:
        'samtools merge '
        '-f '
        '-@ {threads} '
        '{output.bam} '
        '{input} && '
        'samtools index {output.bam}'


###############################################################################
# Feature Counts
###############################################################################
def _layout(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['layout'], **fill)[0]


def _strand(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['strand'], **fill)[0]


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
        strand=_strand,
    output:
        counts=patterns['srxMerge']['feature_counts']['counts'],
        jcounts=patterns['srxMerge']['feature_counts']['jcounts'],
        summary=patterns['srxMerge']['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 4,
      time_hr=lambda wildcards, attempt: attempt * 2
    log:
        patterns['srxMerge']['feature_counts']['counts'] + '.log'
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule featurecounts_intergenic:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=patterns['intergenic']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
    output:
        counts=patterns['srxMerge']['feature_counts_intergenic']['counts'],
        jcounts=patterns['srxMerge']['feature_counts_intergenic']['jcounts'],
        summary=patterns['srxMerge']['feature_counts_intergenic']['summary']
    params:
        extra_pe = '-p -P -C -J -t gene ',
        extra_se = '-J -t gene '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 4,
      time_hr=lambda wildcards, attempt: attempt * 2
    log:
        patterns['srxMerge']['feature_counts_intergenic']['counts'] + '.log'
    conda: "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')

# TODO: Add feature counts for Segments and Fusions

###############################################################################
# Salmon Quantification
###############################################################################
# TODO: Add salmon quantification step using hisat2 bams

###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai'],
    output:
        samtools_stats=patterns['srxMerge']['samtools_stats'],
        samtools_idxstats=patterns['srxMerge']['samtools_idxstats'],
        bamtools_stats=patterns['srxMerge']['bamtools_stats']
    conda: 'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 2
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# Make BedGraphs
###############################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = ('--outFileFormat bedgraph '
            '--binSize 1 '
            '--normalizeTo1x 129000000 '
            '--ignoreForNormalization chrX '
            )

    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    output:
        temp('../output/aln-wf/samples/{srx}/{srx}.{strand,first|second}.bedgraph')
    params:
        extra=_param_bamCoverage
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda: "conda.yaml"
    wrapper:
        wrapper_for('wrappers/deeptools/bamCoverage')


rule convertToFlybase:
    input: rules.bamCoverage.output[0]
    output: bedgraph=temp("../output/aln-wf/samples/{srx}/{srx}.flybase.{strand,first|second}.bedgraph")
    conda: 'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output.bedgraph}'


###############################################################################
# Make BigWigs
###############################################################################
def _convertBedGraphToBigWig(wildcards):
    if 'flybase' in wildcards.prefix:
        return patterns['chromSizes_fb']
    else:
        return config['references']['dmel']['chromSizes']


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=_convertBedGraphToBigWig
    output:
        bigwig='{prefix}.bw'
    conda: 'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 12,
      time_hr=lambda wildcards, attempt: attempt * 4
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '

# vim: set ft=python.snakemake
