#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib')
from ncbi_remap.snakemake import wrapper_for, put_flag

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


################################################################################
# Build Sample Table
###############################################################################
mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
srxs = store['aln/queue'].srx.unique()[:1000]
mask = store['aln/queue'].srx.isin(srxs)
sample_table = store['aln/queue'][mask]
store.close()

#NOTE: pulling just flybase samples for building BedGraphs
sample_table = pd.read_csv('../output/flybase_samples.tsv', sep='\t')
srxs = sample_table.srx.unique()
sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})

################################################################################
# Set up file naming patterns and targets
################################################################################
patterns = {
    'fastq': {
        'r1': '../prealn-wf/output/samples/{srx}/{srr}/{srr}_1.fastq.gz',
        'r2': '../prealn-wf/output/samples/{srx}/{srr}/{srr}_2.fastq.gz',
        'summary': '../prealn-wf/output/samples/{srx}/{srr}/{srr}.fastq.tsv',
    },

    'layout': '../prealn-wf/output/samples/{srx}/{srr}/LAYOUT',
    'strand': '../prealn-wf/output/samples/{srx}/{srr}/STRAND',
    'atropos': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz',
    },

    'hisat2': {
        'splice_sites': '../prealn-wf/output/known_splice_sites_r6-11.txt',
        'bam': 'output/samples/{srx}/{srr}/{srr}.fq.bam',
        'bai': 'output/samples/{srx}/{srr}/{srr}.fq.bam.bai',
        'summary': 'output/samples/{srx}/{srr}/{srr}.fq.bam.tsv',
        'bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',
    },

    # SRX Level Files
    'srxMerge': {
        'bam': 'output/samples/{srx}/{srx}.bam',
        'bai': 'output/samples/{srx}/{srx}.bam.bai',
        'feature_counts': {
            'counts': 'output/samples/{srx}/{srx}.bam.counts',
            'jcounts': 'output/samples/{srx}/{srx}.bam.counts.jcounts',
            'summary': 'output/samples/{srx}/{srx}.bam.counts.summary',
        },
        'feature_counts_intergenic': {
            'counts': 'output/samples/{srx}/{srx}.bam.intergenic.counts',
            'jcounts': 'output/samples/{srx}/{srx}.bam.intergenic.counts.jcounts',
            'summary': 'output/samples/{srx}/{srx}.bam.intergenic.counts.summary',
        },
        'samtools_stats': 'output/samples/{srx}/{srx}.bam.samtools.stats',
        'samtools_idxstats': 'output/samples/{srx}/{srx}.bam.samtools.idxstats',
        'bamtools_stats': 'output/samples/{srx}/{srx}.bam.bamtools.stats',
        'bamCoverage': 'output/samples/{srx}/{srx}.{strand}.bedgraph',
        'bamCoverageFlyBase': 'output/samples/{srx}/{srx}.flybase.{strand}.bedgraph',
        'bigWig': 'output/samples/{srx}/{srx}.{strand}.bw',
        'bigWigFlyBase': 'output/samples/{srx}/{srx}.flybase.{strand}.bw',
    },

    # MISC Files
    'intergenic': {
        'bed': '../output/dmel_r6-11.intergenic.bed',
        'gtf': '../output/dmel_r6-11.intergenic.gtf',
    },

    'chromSizes_fb': '../output/dmel_r6-11.flybase.chromsizes',
}

################################################################################
# Set up Build Targets
################################################################################
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['hisat2']['summary'],
        targets['srxMerge']['feature_counts']['summary'],
        targets['srxMerge']['feature_counts_intergenic']['summary'],
        targets['srxMerge']['samtools_stats'],
        targets['srxMerge']['samtools_idxstats'],
        targets['srxMerge']['bamtools_stats'],
        targets['srxMerge']['bigWig'],
        targets['srxMerge']['bigWigFlyBase'],
    ]


rule targets:
    input: keepers(targets)


onsuccess:
    print('All Finished')


onerror:
    print('Something went wrong, you need to re-run')


################################################################################
# Summary Rules
################################################################################
rule update_queue:
    """Updates queue.

    After running targets you need to update the queue. This job itereates over
    outputs and moves ids around in the queing system.
    """
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.io import add_table, remove_chunk, add_id, remove_id
        from ncbi_remap.snakemake import check_alignment

        def check_outputs(**kwargs):
            tg = helpers.fill_patterns(patterns, kwargs)
            for fname in utils.flatten(keepers(tg)):
                if not os.path.exists(fname):
                    return
            return kwargs

        store = pd.HDFStore(mystore)

        # Initialize values if needed
        if not store.get_node('aln/flags'):
            store['aln/flags'] = pd.DataFrame(data=[], index=store['ids'].set_index(['srx', 'srr']).index,
                                              columns=['flag_alignment_bad', 'flag_complete']).fillna(False)

        done = []
        for i, row in sample_table.iterrows():
            curr = row.to_dict()

            if check_alignment(store, patterns['hisat2']['bad'], **curr):
                continue

            val = check_outputs(**curr)
            if val is not None:
                done.append(val)

        df = pd.DataFrame(done)

        # Add to complete, add to alignment queue, remove from prealn queue
        add_table(store, 'aln/complete', data=df)
        remove_chunk(store, 'aln/queue', df.srr.tolist())

        flags = store['aln/flags']
        flags.loc[df.set_index(['srx', 'srr']).index, 'flag_complete'] = True
        store['aln/flags'] = flags

        store.close()


rule print_queue:
    """Prints the current counts for the queue."""
    run:
        store = pd.HDFStore(mystore, mode='r')

        def cnts(key):
            if store.__contains__(key):
                return store[key].shape[0]
            else:
                return 0

        pairs = [
            ("ids in the system", cnts('ids')),
            ("queued", cnts("aln/queue")),
            ("completed", store['aln/flags']['flag_completed'].sum()),
            ("alignment bad", store['aln/flags']['flag_alignment_bad'].sum()),
        ]

        report = '\nCurrent Queue Summary\n'
        for k, v in pairs:
            report += '{:,}\t\t\t{}\n'.format(v, k)

        print(report)
        store.close()


rule aggregate:
    """Aggregate outputs from different tools."""
    threads: 8
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.logging import logger
        from ncbi_remap.io import add_table
        from ncbi_remap.snakemake import agg
        from ncbi_remap.parser import (parse_hisat2, parse_featureCounts_summary,
            parse_picard_markduplicate_metrics, parse_samtools_stats,
            parse_bamtools_stats, parse_featureCounts_counts,
            parse_featureCounts_jcounts, parse_samtools_idxstats,
        )

        store = pd.HDFStore(mystore)
        flags = store['aln/flags']
        completed = flags[flags['flag_complete']].reset_index()[['srx', 'srr']]

        # Hisat2
        logger.info('Parsing Hisat2')
        agg(store, 'aln/workflow/hisat2', parse_hisat2, patterns['hisat2']['bam'] + '.log', completed)

        # stats
        logger.info('Parsing Other Stats')
        logger.info('Parsing Other Stats - Samtools Stats')
        agg(store, 'aln/workflow/samtools_stats', parse_samtools_stats, patterns['srxMerge']['samtools_stats'], completed)

        logger.info('Parsing Other Stats - Bamtools Stats')
        agg(store, 'aln/workflow/bamtools_stats', parse_bamtools_stats, patterns['srxMerge']['bamtools_stats'], completed)

        # Feature Counts
        logger.info('Parsing Feature Counts')
        logger.info('Parsing Feature Counts - Summary')
        agg(store, 'aln/workflow/feature_counts/summary', parse_featureCounts_summary, patterns['srxMerge']['feature_counts']['summary'], completed)

        logger.info('Parsing Feature Counts - Counts')
        agg(store, 'aln/workflow/feature_counts/counts', parse_featureCounts_counts, patterns['srxMerge']['feature_counts']['counts'], completed, large=True)

        logger.info('Parsing Feature Counts - Junction Counts')
        agg(store, 'aln/workflow/feature_counts/jcounts', parse_featureCounts_jcounts, patterns['srxMerge']['feature_counts']['jcounts'], completed, large=True)

        logger.info('Parsing Feature Counts Intergenic')
        logger.info('Parsing Feature Counts Intergenic - Summary')
        agg(store, 'aln/workflow/feature_counts/summary', parse_featureCounts_summary, patterns['srxMerge']['feature_counts_intergenic']['summary'], completed)

        logger.info('Parsing Feature Counts Intergenic - Counts')
        agg(store, 'aln/workflow/feature_counts/counts', parse_featureCounts_counts, patterns['srxMerge']['feature_counts_intergenic']['counts'], completed, large=True)

        logger.info('Parsing Feature Counts Intergenic - Junction Counts')
        agg(store, 'aln/workflow/feature_counts/jcounts', parse_featureCounts_jcounts, patterns['srxMerge']['feature_counts_intergenic']['jcounts'], completed, large=True)

        logger.info('Parsing Large Datasets')
        logger.info('Parsing Other Stats - Samtools Index Stats')
        agg(store, 'aln/workflow/samtools_idxstats', parse_samtools_idxstats, patterns['srxMerge']['samtools_idxstats'], completed, large=True)

        store.close()


################################################################################
# Build Intergenic Reference
################################################################################
rule intergenic:
    input:
        db=config['references']['dmel']['db']
    output:
        bed=patterns['intergenic']['bed'],
        gtf=patterns['intergenic']['gtf']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    run:
        import gffutils
        from gffutils.pybedtools_integration import to_bedtool, featurefuncs
        db = gffutils.FeatureDB(input.db)
        gene = to_bedtool(db.features_of_type('gene')).saveas()
        slopped = gene.slop(b=100, genome='dm6')
        merged = slopped.sort().merge()
        complement = merged.complement(genome='dm6').saveas()

        global cnt
        cnt = 1
        def interName(feature):
            global cnt
            feature = featurefuncs.extend_fields(feature, 4)
            feature.name = 'intergenic{}'.format(cnt)
            cnt += 1
            return feature

        def interGFF(feature):
            gff = featurefuncs.bed2gff(feature)
            gff[1] = 'bedtools'
            gff[2] = 'gene'
            gff.attrs['gene_id'] = gff.name
            return gff

        bed = complement.each(interName).saveas(output.bed)
        bed.each(interGFF).saveas(output.gtf)


################################################################################
# FASTQ dump and check for SE or PE
################################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1=temp(patterns['fastq']['r1']),
        fq2=temp(patterns['fastq']['r2']),
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


################################################################################
# FASTQ Pre-process
################################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=patterns['fastq']['r1'],
        R2=patterns['fastq']['r2'],
        layout=patterns['layout']
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2']),
    params:
        extra_pe='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25',
        extra_se='-a file:../data/adapters.fa -q 20 --minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        R1=patterns['atropos']['r1'],
        R2=patterns['atropos']['r2'],
        layout=patterns['layout'],
        strand=patterns['strand']
    output:
        bam=temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16
    params:
        hisat2_extra='--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites']),
        samtools_view_extra="--threads 6 -q 20",
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn=patterns['hisat2']['bam']
    output:
        tsv=patterns['hisat2']['summary']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        df = parse_hisat2(srx, srr, input.fn + '.log')
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['hisat2']['bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam='{prefix}.bam'
    output:
        bai=temp('{prefix}.bam.bai')
    conda:
        "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell:
        "samtools index {input.bam}"


################################################################################
# Merge Bams
################################################################################
def _input_expMerge(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique().tolist()
    }
    return expand(patterns['hisat2']['bam'], **fill)


rule expMerge:
    input: _input_expMerge
    output:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 4
    threads: 4
    shell:
        'samtools merge '
        '-f '
        '-@ {threads} '
        '{output.bam} '
        '{input} && '
        'samtools index {output.bam}'


################################################################################
# Feature Counts
################################################################################
def _layout(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['layout'], **fill)[0]


def _strand(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['strand'], **fill)[0]


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
        strand=_strand,
    output:
        counts=patterns['srxMerge']['feature_counts']['counts'],
        jcounts=patterns['srxMerge']['feature_counts']['jcounts'],
        summary=patterns['srxMerge']['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    log:
        patterns['srxMerge']['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule featurecounts_intergenic:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=targets['intergenic']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
    output:
        counts=patterns['srxMerge']['feature_counts_intergenic']['counts'],
        jcounts=patterns['srxMerge']['feature_counts_intergenic']['jcounts'],
        summary=patterns['srxMerge']['feature_counts_intergenic']['summary']
    params:
        extra_pe = '-p -P -C -J -t gene ',
        extra_se = '-J -t gene '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    log:
        patterns['srxMerge']['feature_counts_intergenic']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai'],
    output:
        samtools_stats=patterns['srxMerge']['samtools_stats'],
        samtools_idxstats=patterns['srxMerge']['samtools_idxstats'],
        bamtools_stats=patterns['srxMerge']['bamtools_stats']
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


################################################################################
# Make BedGraphs
################################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = ('--outFileFormat bedgraph '
            '--binSize 1 '
            '--normalizeTo1x 129000000 '
            '--ignoreForNormalization chrX '
            )

    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    output:
        temp('output/samples/{srx}/{srx}.{strand,first|second}.bedgraph')
    params:
        extra=_param_bamCoverage
    threads: 4
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/deeptools/bamCoverage')


rule convertToFlybase:
    input: patterns['srxMerge']['bamCoverage']
    output: bedgraph=temp(patterns['srxMerge']['bamCoverageFlyBase'])
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output.bedgraph}'


################################################################################
# Make BigWigs
################################################################################
def _convertBedGraphToBigWig(wildcards):
    if 'flybase' in wildcards.prefix:
        return targets['chromSizes_fb']
    else:
        return config['references']['dmel']['chromSizes']


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=_convertBedGraphToBigWig
    output:
        bigwig='{prefix}.bw'
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'export LC_COLLATE=C; '
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& sort -k1,1 -k2,2n {input.bedgraph} > $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '
