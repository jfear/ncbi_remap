#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib')
from ncbi_remap.snakemake import wrapper_for, put_flag

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


################################################################################
# Build Sample Table
###############################################################################
mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
srxs = store['aln/queue'].srx.unique()[:1000]
mask = store['aln/queue'].srx.isin(srxs)
sample_table = store['aln/queue'][mask]
store.close()

#NOTE: pulling just flybase samples for building BedGraphs
sample_table = pd.read_csv('../output/flybase_samples.tsv', sep='\t')
srxs = sample_table.srx.unique()
sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})

################################################################################
# Set up file naming patterns and targets
################################################################################
patterns = {
    'fastq': {
        'r1': '../prealn-wf/output/samples/{srx}/{srr}/{srr}_1.fastq.gz',
        'r2': '../prealn-wf/output/samples/{srx}/{srr}/{srr}_2.fastq.gz',
        'summary': '../prealn-wf/output/samples/{srx}/{srr}/{srr}.fastq.tsv',
    },

    'layout': '../prealn-wf/output/samples/{srx}/{srr}/LAYOUT',
    'strand': '../prealn-wf/output/samples/{srx}/{srr}/STRAND',
    'atropos': {
        'r1': 'output/samples/{srx}/{srr}/{srr}_1.trim.clean.fastq.gz',
        'r2': 'output/samples/{srx}/{srr}/{srr}_2.trim.clean.fastq.gz',
    },

    'hisat2': {
        'splice_sites': '../prealn-wf/output/known_splice_sites_r6-11.txt',
        'bam': 'output/samples/{srx}/{srr}/{srr}.fq.bam',
        'bai': 'output/samples/{srx}/{srr}/{srr}.fq.bam.bai',
        'summary': 'output/samples/{srx}/{srr}/{srr}.fq.bam.tsv',
        'bad': 'output/samples/{srx}/{srr}/ALIGNMENT_BAD',
    },

    # SRX Level Files
    'srxMerge': {
        'bam': 'output/samples/{srx}/{srx}.bam',
        'bai': 'output/samples/{srx}/{srx}.bam.bai',
        'feature_counts': {
            'counts': 'output/samples/{srx}/{srx}.bam.counts',
            'jcounts': 'output/samples/{srx}/{srx}.bam.counts.jcounts',
            'summary': 'output/samples/{srx}/{srx}.bam.counts.summary',
        },
        'samtools_stats': 'output/samples/{srx}/{srx}.bam.samtools.stats',
        'samtools_idxstats': 'output/samples/{srx}/{srx}.bam.samtools.idxstats',
        'bamtools_stats': 'output/samples/{srx}/{srx}.bam.bamtools.stats',
        'bamCoverage': 'output/samples/{srx}/{srx}.{strand}.bw',
        'bamCoverageFlyBase': 'output/samples/{srx}/{srx}.flybase.{strand}.bw',
    },

    # MISC Files
    'chromSizes_fb': '../output/dmel_r6-11.flybase.chromsizes',
}

################################################################################
# Set up Build Targets
################################################################################
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['hisat2']['summary'],
        targets['srxMerge']['feature_counts']['summary'],
        targets['srxMerge']['samtools_stats'],
        targets['srxMerge']['samtools_idxstats'],
        targets['srxMerge']['bamtools_stats'],
        targets['srxMerge']['bamCoverage'],
        targets['srxMerge']['bamCoverageFlyBase'],
    ]


rule targets:
    input: keepers(targets)


rule update_queue:
    """Updates queue.

    After running targets you need to update the queue. This job itereates over
    outputs and moves ids around in the queing system.
    """
    run:
        sys.path.insert(0, '../lib')
        from ncbi_remap.io import add_table, remove_chunk, add_id, remove_id, check_layout, check_strand, check_alignment

        def check_outputs(**kwargs):
            tg = helpers.fill_patterns(patterns, kwargs)
            for fname in utils.flatten(keepers(tg)):
                if not os.path.exists(fname):
                    return
            return kwargs

        store = pd.HDFStore(mystore)
        done = []
        for i, row in sample_table.iterrows():
            curr = row.to_dict()

            if check_alignment(store, patterns['hisat2']['bad'], **curr):
                continue

            val = check_outputs(**curr)
            if val:
                done.append(val)
                check_layout(store, patterns['layout'], **curr)
                check_strand(store, patterns['strand'], **curr)

        df = pd.DataFrame(done)

        # Add to complete, add to alignment queue, remove from prealn queue
        add_table(store, 'aln/complete', data=df)
        remove_chunk(store, 'aln/queue', df.srr.tolist())
        store.close()


rule print_queue:
    """Prints the current counts for the queue."""
    run:
        store = pd.HDFStore(mystore, mode='r')

        def cnts(key):
            if store.__contains__(key):
                return store[key].shape[0]
            else:
                return 0

        pairs = [
            {"ids": "ids in the system"},
            {"aln/queue": "queued"},
            {"aln/complete": "completed"},
            {"aln/alignment_bad": "alignment bad"},
            {"layout/SE": "Single End"},
            {"layout/PE": "Pair End"},
            {"layout/keep_R1": "Really Single End R1"},
            {"layout/keep_R2": "Really Single End R2"},
            {"strand/first": "first strand"},
            {"strand/second": "second strand"},
            {"strand/unstranded": "unstranded"},
        ]

        report = '\nCurrent Queue Summary\n'
        for pair in pairs:
            k, v = list(pair.items())[0]
            report += '{:,}\t\t\t{}\n'.format(cnts(k), v)

        print(report)
        store.close()


onsuccess:
    print('All Finished')


onerror:
    print('Something went wrong, you need to re-run')


################################################################################
# FASTQ dump and check for SE or PE
################################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1=patterns['fastq']['r1'],
        fq2=patterns['fastq']['r2'],
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


################################################################################
# FASTQ Pre-process
################################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=patterns['fastq']['r1'],
        R2=patterns['fastq']['r2'],
        layout=patterns['layout']
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2']),
    params:
        extra_pe='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25',
        extra_se='-a file:../data/adapters.fa -q 20 --minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        R1=patterns['atropos']['r1'],
        R2=patterns['atropos']['r2'],
        layout=patterns['layout'],
        strand=patterns['strand']
    output:
        bam=temp(patterns['hisat2']['bam'])
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 16
    params:
        hisat2_extra='--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites']),
        samtools_view_extra="--threads 6 -q 20",
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn=patterns['hisat2']['bam']
    output:
        tsv=patterns['hisat2']['summary']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        df = parse_hisat2(srx, srr, input.fn + '.log')
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.ix[0, 'per_alignment'] < .50:
            fname = patterns['hisat2']['bad'].format(**wildcards)
            Path(fname).touch()


rule bai:
    input:
        bam='{prefix}.bam'
    output:
        bai=temp('{prefix}.bam.bai')
    conda:
        "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell:
        "samtools index {input.bam}"


################################################################################
# Merge Bams
################################################################################
def _input_expMerge(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique().tolist()
    }
    return expand(patterns['hisat2']['bam'], **fill)


rule expMerge:
    input: _input_expMerge
    output: bam=temp(patterns['srxMerge']['bam'])
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    threads: 8
    shell:
        'samtools merge '
        '-f '
        '-@ {threads} '
        '{output.bam} '
        '{input}'


################################################################################
# Feature Counts
################################################################################
def _layout(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['layout'], **fill)

def _strand(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['strand'], **fill)


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
        strand=_strand,
    output:
        counts=patterns['srxMerge']['feature_counts']['counts'],
        jcounts=patterns['srxMerge']['feature_counts']['jcounts'],
        summary=patterns['srxMerge']['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    log:
        patterns['srxMerge']['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai'],
    output:
        samtools_stats=patterns['srxMerge']['samtools_stats'],
        samtools_idxstats=patterns['srxMerge']['samtools_idxstats'],
        bamtools_stats=patterns['srxMerge']['bamtools_stats']
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


################################################################################
# Make BedGraphs
################################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = ('--outFileFormat bedgraph '
            '--binSize 1 '
            '--normalizeTo1x 129000000 '
            '--ignoreForNormalization chrX '
            )

    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    output: temp('output/samples/{srx}/{srx}.{strand,first|second}.bw')
    params:
        extra=_param_bamCoverage
    threads: 8
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 3
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../lcdb-wf/wrappers/wrappers/deeptools/bamCoverage')


rule convertToFlybase:
    input: rules.bamCoverage.output
    output: bedgraph=temp(patterns['srxMerge']['bamCoverageFlyBase'])
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 3
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output.bedgraph}'


################################################################################
# Make BigWigs
################################################################################
def _convertBedGraphToBigWig(wildcards):
    if 'flybase' in wildcards.prefix:
        return targets['chromSizes_fb']
    else:
        return config['references']['dmel']['chromSizes']


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=_convertBedGraphToBigWig
    output:
        bigwig='{prefix}.bw'
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1
    shell:
        'export LC_COLLATE=C; '
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& sort -k1,1 -k2,2n {input.bedgraph} > $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '
