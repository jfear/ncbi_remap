#!/usr/bin/env python
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib')
from ncbi_remap.snakemake import wrapper_for, put_flag, get_patterns

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '.'

# import config
configfile: '../config/reference_config.yaml'


###############################################################################
# Build Sample Table
###############################################################################
mystore = '../sra.h5'
store = pd.HDFStore(mystore, mode='r')
srxs = store['aln/queue'].srx.unique()
mask = store['aln/queue'].srx.isin(srxs)
sample_table = store['aln/queue'][mask]
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})
store.close()

# NOTE: pulling just flybase samples for building BedGraphs
# sample_table = pd.read_csv('../output/flybase_samples.tsv', sep='\t')
# srxs = sample_table.srx.unique()
# sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
# sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})

# NOTE: pulling just modENCODE samples for running tau and TSPS
# sample_table = pd.read_csv('../output/modencode_samples.tsv', sep='\t')
# srxs = sample_table.srx.unique()
# sample_table = sample_table[sample_table.srx.isin(srxs)].copy()
# sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})

# NOTE: Need to remove tmep files from some of the complete
# 9,425 srx
# store = pd.HDFStore(mystore, mode='r')
# srxs = store['aln/complete'].srx.unique()[:5000]
# mask = store['aln/complete'].srx.isin(srxs)
# sample_table = store['aln/complete'][mask]
# sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})
# store.close()
###############################################################################
# Set up file naming patterns and targets
###############################################################################
patterns = get_patterns('patterns.yaml')
targets = helpers.fill_patterns(patterns, sample_table)


def keepers(targets):
    return [
        targets['hisat2']['summary'],
        targets['srxMerge']['feature_counts']['summary'],
        targets['srxMerge']['feature_counts_intergenic']['summary'],
        targets['srxMerge']['samtools_stats'],
        targets['srxMerge']['samtools_idxstats'],
        targets['srxMerge']['bamtools_stats'],
        targets['srxMerge']['bigWig'],
        targets['srxMerge']['bigWigFlyBase'],
    ]


rule targets:
    input: keepers(targets)


def slack(text):
    try:
        from slackclient import SlackClient
        token = os.environ['SLACK_SNAKEMAKE_BOT_TOKEN']
        sc = SlackClient(token)
        sc.api_call('chat.postMessage', channel='U6N9L3ZSQ', text=text)
    except (ImportError, KeyError):
        pass


onsuccess:
    print('All Finished')
    slack('aln-wf: All Finished')


onerror:
    print('Something went wrong, you need to re-run')
    slack('aln-wf: Something went wrong, you need to re-run')


###############################################################################
# Build Intergenic Reference
###############################################################################
rule intergenic:
    input:
        db=config['references']['dmel']['db']
    output:
        bed=patterns['intergenic']['bed'],
        gtf=patterns['intergenic']['gtf']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    run:
        import gffutils
        from gffutils.pybedtools_integration import to_bedtool, featurefuncs
        db = gffutils.FeatureDB(input.db)
        gene = to_bedtool(db.features_of_type('gene')).saveas()
        slopped = gene.slop(b=100, genome='dm6')
        merged = slopped.sort().merge()
        complement = merged.complement(genome='dm6').saveas()

        global cnt
        cnt = 1

        def interName(feature):
            global cnt
            feature = featurefuncs.extend_fields(feature, 4)
            feature.name = 'intergenic{}'.format(cnt)
            cnt += 1
            return feature

        def interGFF(feature):
            gff = featurefuncs.bed2gff(feature)
            gff[1] = 'bedtools'
            gff[2] = 'gene'
            gff.attrs['gene_id'] = gff.name
            return gff

        bed = complement.each(interName).saveas(output.bed)
        bed.each(interGFF).saveas(output.gtf)


###############################################################################
# FASTQ dump and check for SE or PE
###############################################################################
rule fastq_dump:
    """Downloads fastq and checks if there is one or two sets of reads."""
    output:
        fq1=temp(patterns['fastq']['r1']),
        fq2=temp(patterns['fastq']['r2']),
        flag=patterns['layout'],
        summary=patterns['fastq']['summary'],
    log: patterns['fastq']['r1'] + '.log'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/fastq_dump')


###############################################################################
# FASTQ Pre-process
###############################################################################
rule atropos:
    """Filter reads that are less than 25bp."""
    input:
        R1=patterns['fastq']['r1'],
        R2=patterns['fastq']['r2'],
        layout=patterns['layout']
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2']),
    params:
        extra_pe='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25',
        extra_se='-a file:../data/adapters.fa -q 20 --minimum-length 25',
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 4
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/atropos')


###############################################################################
# Alignment
###############################################################################
rule hisat2_splice_site:
    """Generate splicesite information from known annotations."""
    input:
        gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda:
        "conda.yaml"
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2:
    """Basic alignment."""
    input:
        flag=patterns['layout'],
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        R1=patterns['atropos']['r1'],
        R2=patterns['atropos']['r2'],
        layout=patterns['layout'],
        strand=patterns['strand']
    output:
        bam=temp(patterns['hisat2']['bam']),
        bai=temp(patterns['hisat2']['bai'])
    threads: 4
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    params:
        hisat2_extra='--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites']),
        samtools_view_extra="--threads 4 -q 20",
        samtools_sort_extra='--threads 4 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/hisat2/align')


rule hisat2_summary:
    """Parse log and flag as bad alignment if <50% aligned."""
    input:
        fn=patterns['hisat2']['bam']
    output:
        tsv=patterns['hisat2']['summary']
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    run:
        from ncbi_remap.parser import parse_hisat2
        srx = wildcards.srx
        srr = wildcards.srr
        fname = input.fn + '.log'
        df = parse_hisat2(fname.format(srx=srx, srr=srr))
        df.to_csv(output.tsv, sep='\t', index=False)

        if df.iloc[0, :]['per_alignment'] < .50:
            fname = patterns['alignment_bad'].format(**wildcards)
            Path(fname).touch()


###############################################################################
# Merge Bams
###############################################################################
def _input_expMerge(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique().tolist()
    }
    return expand(patterns['hisat2']['bam'], **fill)


rule expMerge:
    input: _input_expMerge
    output:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 4
    threads: 4
    shell:
        'samtools merge '
        '-f '
        '-@ {threads} '
        '{output.bam} '
        '{input} && '
        'samtools index {output.bam}'


###############################################################################
# Feature Counts
###############################################################################
def _layout(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['layout'], **fill)[0]


def _strand(wildcards):
    fill = {
        'srx': wildcards.srx,
        'srr': sample_table[(sample_table.srx == wildcards.srx)].srr.unique()[0]
    }
    return expand(patterns['strand'], **fill)[0]


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
        strand=_strand,
    output:
        counts=patterns['srxMerge']['feature_counts']['counts'],
        jcounts=patterns['srxMerge']['feature_counts']['jcounts'],
        summary=patterns['srxMerge']['feature_counts']['summary']
    params:
        extra_pe = '-p -P -C -J ',
        extra_se = '-J '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    log:
        patterns['srxMerge']['feature_counts']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


rule featurecounts_intergenic:
    """
    Count reads in intergenic regions with featureCounts from the subread package
    """
    input:
        annotation=targets['intergenic']['gtf'],
        bam=patterns['srxMerge']['bam'],
        layout=_layout,
    output:
        counts=patterns['srxMerge']['feature_counts_intergenic']['counts'],
        jcounts=patterns['srxMerge']['feature_counts_intergenic']['jcounts'],
        summary=patterns['srxMerge']['feature_counts_intergenic']['summary']
    params:
        extra_pe = '-p -P -C -J -t gene ',
        extra_se = '-J -t gene '
    threads: 2
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    log:
        patterns['srxMerge']['feature_counts_intergenic']['counts'] + '.log'
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('../wrappers/featurecounts')


###############################################################################
# Stats
###############################################################################
rule run_stats:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai'],
    output:
        samtools_stats=patterns['srxMerge']['samtools_stats'],
        samtools_idxstats=patterns['srxMerge']['samtools_idxstats'],
        bamtools_stats=patterns['srxMerge']['bamtools_stats']
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


###############################################################################
# Make BedGraphs
###############################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = ('--outFileFormat bedgraph '
            '--binSize 1 '
            '--normalizeTo1x 129000000 '
            '--ignoreForNormalization chrX '
            )

    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam=patterns['srxMerge']['bam'],
        bai=patterns['srxMerge']['bai']
    output:
        temp('output/samples/{srx}/{srx}.{strand,first|second}.bedgraph')
    params:
        extra=_param_bamCoverage
    threads: 4
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 8,
      time_hr=lambda wildcards, attempt: attempt * 4
    conda:
        "conda.yaml"
    wrapper:
        wrapper_for('wrappers/deeptools/bamCoverage')


rule convertToFlybase:
    input: patterns['srxMerge']['bamCoverage']
    output: bedgraph=temp(patterns['srxMerge']['bamCoverageFlyBase'])
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 1,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'chrom_convert '
        '--from UCSC '
        '--to FlyBase '
        '--fileType BED '
        '-i {input[0]} '
        '-o {output.bedgraph}'


###############################################################################
# Make BigWigs
###############################################################################
def _convertBedGraphToBigWig(wildcards):
    if 'flybase' in wildcards.prefix:
        return targets['chromSizes_fb']
    else:
        return config['references']['dmel']['chromSizes']


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=_convertBedGraphToBigWig
    output:
        bigwig='{prefix}.bw'
    conda:
        'conda.yaml'
    resources:
      mem_gb=lambda wildcards, attempt: attempt * 2,
      time_hr=lambda wildcards, attempt: attempt * 1
    shell:
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& bedSort {input.bedgraph} $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '

# vim: set ft=python.snakemake
