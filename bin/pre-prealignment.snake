#/usr/bin/env python
"""The pre-prealignment pipeline downloads and checks FASTQ files.

This workflow downloads all of the FASTQ files from SRA and calculates:

    * md5sum
    * Number of reads (libsize)
    * Determines if SRR is single-end or pair-end

All files are saved in:

    output/pre-prealignment/SRX####/SRR####/SRR####_1.fastq.gz   # SE and PE
    output/pre-prealignment/SRX####/SRR####/SRR####_2.fastq.gz   # PE

"""
# vim: set ft=python.snakemake
import os
import signal
import re
from pathlib import Path
from textwrap import dedent
import shutil as sh
import hashlib

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils

from pymongo import MongoClient

sys.path.insert(0, '../lib/python')

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

workdir: '../.pre-prealignment'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines()]

# s2 cells (1508)
with open('../data/1508_s2_cell_brian_annot.txt', 'r') as fh:
    s2 = [x.strip() for x in fh.readlines()]

# rnaseq runselector (13462)
with open('../data/run_selector_RNA_accession_list.txt', 'r') as fh:
    rsel = [x.strip() for x in fh.readlines()]

# Import run_id and experiment_id and dump into a dataframe
samples = remap.aggregate([
    {'$unwind': '$runs'},
    {'$match': {
        'runs.srr': {'$exists': 1},
        '$or': [
                {'runs.pre_aln_flags': {'$exists': 0}},
                {'runs.pre_aln_flags': {'$eq': []}},
            ],
        }
    },
    {'$project': {'_id': 0, 'experiment': '$_id', 'sample': '$runs.srr'}},
    {'$sort': {'sample': 1}},
    {'$limit': 5000},
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns
################################################################################
patterns = {'fastq': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz'}

# Build target files
targets = helpers.fill_patterns(patterns, sample_table)

rule targets:
    input: utils.flatten(targets)

################################################################################
# FASTQ dump and check for SE or PE
################################################################################
def check_fastq(fn):
    """Checks if a FASTQ file actually has data in it."""
    try:
        assert os.stat(fn).st_size > 10000
        return True
    except (FileNotFoundError, AssertionError) as err:
        return False
    except:
        raise

def md5sum(fn):
    """Calculates the md5sum of the raw fastq file."""
    md5 = hashlib.md5()
    with open(fn, 'rb') as f:
        for chunk in iter(lambda: f.read(128 * md5.block_size), b''):
            md5.update(chunk)
    return md5.hexdigest()


def fastq_stats(fn):
    """Calculates the number of reads (libsize) and the average read length."""
    libsize, lens = 0, 0

    with open(fn, 'r') as fh:
        for i, row in enumerate(fh):
            if (i % 4 == 1):
                libsize += 1
                lens += len(row.strip())

    avgLen = lens / libsize
    return libsize, avgLen


def singleEnd(sample, odir, R1Libsize, R1avgLen, R1md5):
    """Adds a single-end read to the database.

    Compresses (.gz) and add an entry to the database.
    """
    # Compress files and copy over
    shell(
            "gzip --best $TMPDIR/{sample}_1.fastq "
            "&& cp $TMPDIR/{sample}*.gz {odir}/ "
            "&& rm $TMPDIR/{sample}*.gz"
        )

    # Save results to Database
    remap.find_one_and_update({'runs.srr': sample},
            {
                '$addToSet': {'runs.$.pre_aln_flags': 'SE'},
                '$set': {
                    'runs.$.libsize': {'R1': R1Libsize},
                    'runs.$.avgReadLen': {'R1': R1avgLen},
                    'runs.$.md5': {'R1': R1md5},
                }
            })


def pairEnd(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5):
    """Adds a pair-end read to the database.

    Compresses (.gz) and add an entry to the database.
    """
    shell(
            "gzip --best $TMPDIR/{sample}_1.fastq "
            "&& gzip --best $TMPDIR/{sample}_2.fastq "
            "&& cp $TMPDIR/{sample}*.gz {odir}/ "
            "&& rm $TMPDIR/{sample}*.gz"
        )

    # Save results to Database
    remap.find_one_and_update({'runs.srr': sample},
            {
                '$addToSet': {'runs.$.pre_aln_flags': 'PE'},
                '$set': {
                    'runs.$.libsize': {'R1': R1Libsize, 'R2': R2Libsize},
                    'runs.$.avgReadLen': {'R1': R1avgLen, 'R2': R2avgLen},
                    'runs.$.md5': {'R1': R1md5, 'R2': R2md5},
                }
            })


def actuallySingle(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5, link=False):
    """Adds a single-end read to the database, despite it looking like PE.

    Sometimes I have found that the second read is empty or is a barcode or
    something else. To simplify processing, we figure out which read pair looks
    the best and then just set it to be R1.

    If R1 is the best, then just don't copy over R2, if R2 is the best (i.e.,
    link=True) then copy R2 over and symlink it to R1.

    Compresses (.gz) and add an entry to the database.
    """
    if link:
        shell(
                "gzip --best $TMPDIR/{sample}_2.fastq "
                "&& cp $TMPDIR/{sample}*.gz {odir}/ "
                "&& cd {odir} "
                "&& ln -s {sample}_2.fastq.gz {sample}_1.fastq.gz "
                "&& rm $TMPDIR/{sample}*.gz "
                "&& rm $TMPDIR/{sample}*.fastq "
            )
        keep = 'keep_R2'
    else:
        shell(
                "gzip --best $TMPDIR/{sample}_1.fastq "
                "&& cp $TMPDIR/{sample}_1.fastq.gz {odir}/ "
                "&& rm $TMPDIR/{sample}_1.fastq.gz "
                "&& rm $TMPDIR/{sample}*.fastq "
            )
        keep = 'keep_R1'

    # Save results to Database
    remap.find_one_and_update({'runs.srr': sample},
            {
                '$addToSet': {'runs.$.pre_aln_flags': {'$each': ['SE', keep]}},
                '$set': {
                    'runs.$.libsize': {'R1': R1Libsize, 'R2': R2Libsize},
                    'runs.$.avgReadLen': {'R1': R1avgLen, 'R2': R2avgLen},
                    'runs.$.md5': {'R1': R1md5, 'R2': R2md5},
                }
            })


def downloadBad(sample, output, R1Libsize, R1avgLen, R1md5, R2Libsize=None, R2avgLen=None, R2md5=None):
    """Something went wrong.

    There are just some entries that don't download right. Either they are
    empty or corrupted. We may be able to download them using another
    mechanism later, so just mark as problematic is move on with life.

    Note I touch a file so that snakemake does not complain, but set the entry
    as `download_bad` in the database, so this should not be used in the
    downstream steps.
    """
    remap.find_one_and_update({'runs.srr': sample},
            {
                '$addToSet': {'runs.$.pre_aln_flags': 'download_bad'},
                '$set': {
                    'runs.$.libsize': {'R1': R1Libsize, 'R2': R2Libsize},
                    'runs.$.avgReadLen': {'R1': R1avgLen, 'R2': R2avgLen},
                    'runs.$.md5': {'R1': R1md5, 'R2': R2md5},
                }
            })
    shell("touch {output}".format(output=output))


"""Downloads fastq and checks if there is one or two sets of reads."""
rule fastq_dump:
    output:
        fastq=patterns['fastq'],
    run:
        sample = wildcards.sample
        odir = os.path.dirname(output.fastq)
        R1 = sample + '_1.fastq'
        R2 = sample + '_2.fastq'

        # Dump FASTQ
        shell("fastq-dump -O $TMPDIR -M 0 --split-files {sample}")

        # Pair-end
        if check_fastq(os.path.join(TMPDIR, R2)):
            # Get md5sum
            R1md5 = md5sum(os.path.join(TMPDIR, R1))
            R2md5 = md5sum(os.path.join(TMPDIR, R2))

            # Calculate libsize and average read length
            R1Libsize, R1avgLen = fastq_stats(os.path.join(TMPDIR, R1))
            R2Libsize, R2avgLen = fastq_stats(os.path.join(TMPDIR, R2))

            # Copy and Add to database
            if (R1Libsize > 1000) & (R2Libsize > 1000) & (R1avgLen > 10) & (R2avgLen > 10):
                if R1Libsize == R2Libsize:
                    # Both reads look good.
                    pairEnd(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5)
                else:
                    # There are an uneven number of reads between R1 and R2.
                    # Instead of messing with this, just consider SE and use R1.
                    actuallySingle(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5)
            elif (R1Libsize > 1000) & (R1avgLen > 10):
                # Only R1 looks ok, consider single-end
                actuallySingle(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5)
            elif (R2Libsize > 1000) & (R2avgLen > 10):
                # Only R2 looks ok, consider single-end, symlink R2 to R1 for proper workflow.
                actuallySingle(sample, odir, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5, link=True)
            else:
                downloadBad(sample, output.fastq, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5)

        # Single-end
        elif check_fastq(os.path.join(TMPDIR, R1)):
            # Get md5sum
            R1md5 = md5sum(os.path.join(TMPDIR, R1))

            # Calculate libsize and average read length
            R1Libsize, R1avgLen = fastq_stats(os.path.join(TMPDIR, R1))

            # Copy and Add to database
            singleEnd(sample, odir, R1Libsize, R1avgLen, R1md5)
        else:

            try:
                R1md5 = md5sum(os.path.join(TMPDIR, R1))
            except:
                R1md5 = None

            try:
                R1Libsize, R1avgLen = fastq_stats(os.path.join(TMPDIR, R1))
            except:
                R1Libsize, R1avgLen = None, None

            try:
                R2md5 = md5sum(os.path.join(TMPDIR, R2))
            except:
                R2md5 = None

            try:
                R2Libsize, R2avgLen = fastq_stats(os.path.join(TMPDIR, R2))
            except:
                R2Libsize, R2avgLen = None, None

            downloadBad(sample, output.fastq, R1Libsize, R1avgLen, R1md5, R2Libsize, R2avgLen, R2md5)
