#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from pymongo import MongoClient

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib/python')
import ncbi_remap

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.alignment'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines()]

# s2 SRRs
with open('../data/1104_s2_cell_in_mieg.txt', 'r') as fh:
    s2 = [x.strip() for x in fh.readlines()]

with open('../data/1508_s2_cell_brian_annot.txt', 'r') as fh:
    s2_2 = [x.strip() for x in fh.readlines()]

samples = remap.aggregate([
    {'$unwind': '$runs'},
    {
        '$match': {
            '$and': [
                {'runs.srr': {'$exists': 1}},
                {'runs.srr': {'$in': s2 + s2_2 + golden}},
                {'runs.pre_aln_flags': 'complete'},
                {'runs.aln_flags': {'$ne': 'complete'}},
                {'runs.aln_flags': {'$ne': 'alignment_bad'}},
            ]
        }
    },
    {'$group': {'_id': {'sample': '$runs.srr', 'experiment': '$_id'}}},
    {'$project': {'_id': 0, 'experiment': '$_id.experiment', 'sample': '$_id.sample'}},
    {'$sort': {'sample': 1}},
    {'$limit': 400},
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns and targets
################################################################################
# Patterns
fastqs = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.fastq.gz'
    }

patterns = {
    'atropos': {
        'r1': '../output/alignment/raw/{experiment}/{sample}/{sample}_1.trim.clean.fastq.gz',
        'r2': '../output/alignment/raw/{experiment}/{sample}/{sample}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': '../output/known_splice_sites_r6-11.txt',
        'bam': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam',
        'db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.done',
    },
    'bai': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bai',
    'feature_counts': {
        'counts': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts',
        'summary': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts.summary',
    },
    'samtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats',
    'samtools_stats_db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats.done',
    'samtools_idxstats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.idxstats',
    'bamtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bamtools.stats',
}

# Build target files
all_complete = {'files': '../output/alignment/raw/{experiment}/{sample}/{sample}.done'}
targets = helpers.fill_patterns(all_complete, sample_table)

localrules: hisat2_db

rule targets:
    input: utils.flatten(targets)


onsuccess:
    print('All Finished')
    mongo_client.close()


onerror:
    print('Something went wrong, you need to re-run')
    mongo_client.close()


rule check_results:
    input:
        utils.flatten(patterns['feature_counts']) +
        [
            patterns['samtools_stats_db'],
            patterns['samtools_idxstats'],
            patterns['bamtools_stats'],
        ]
    output: all_complete['files']
    run:
        # Update database
        remap.find_one_and_update(
            {'runs.srr': wildcards.sample},
            {'$addToSet': {'runs.$.aln_flags': 'complete'}}
        )

        # Touch dummy file for snakemake
        Path(output[0]).touch()

################################################################################
# Fuctions
################################################################################
# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'first_strand' in flags:
        return 'first_strand'
    elif 'second_strand' in flags:
        return 'second_strand'
    else:
        return 'unstranded'


def clean_types(dic):
    newDic = {}
    for k, v in dic.items():
        if isinstance(v, np.int64):
            newDic[k] = int(v)
        elif isinstance(v, np.float64):
            newDic[k] = float(v)
        elif v is np.nan:
            pass
        elif v is None:
            pass
        else:
            newDic[k] = v
    return newDic


################################################################################
# FASTQ Pre-process
################################################################################
def _atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return {'R1': expand(fastqs['r1'], **wildcards)[0], 'R2': expand(fastqs['r2'], **wildcards)[0]}
    else:
        return {'R1': expand(fastqs['r1'], **wildcards)[0]}


def _params_extra_atropos(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return '-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25'
    else:
        return '-a file:../data/adapters.fa -q 20 --minimum-length 25'


def _params_r2_atropos(wildcards):
    """Determine if the sample is PE or SE and return a temp R2 if PE."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r2'], **wildcards)[0] + '.tmp.gz'
    else:
        return None


rule atropos:
    input: unpack(_atropos)
    output:
        R1=temp(patterns['atropos']['r1'])
    params:
        extra=_params_extra_atropos,
        R2=_params_r2_atropos
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper: wrapper_for('atropos')


rule atropos_phony:
    input: rules.atropos.output
    output: temp(patterns['atropos']['r2'])
    shell: """
    mv {output[0]}.tmp.gz {output[0]}
    """


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input: gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


def _hisat2(wildcards):
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r1'], **wildcards)[0], expand(patterns['atropos']['r2'], **wildcards)[0]
    else:
        return expand(patterns['atropos']['r1'], **wildcards)[0]


def _params_hisat2_fastq(wildcards):
    base = '--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites'])
    strand = get_strand(wildcards.sample)
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if strand == 'first_strand':
        if 'PE' in flags:
            return base + '--rna-strandness FR'
        else:
            return base + '--rna-strandness F'
    elif strand == 'second_strand':
        if 'PE' in flags:
            return base + '--rna-strandness RF'
        else:
            return base + '--rna-strandness R'

    return base


rule hisat2:
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=_hisat2,
    output: bam=patterns['hisat2']['bam']
    threads: 8
    params:
        hisat2_extra=_params_hisat2_fastq,
        samtools_view_extra="--threads 6 -q 20",
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper: wrapper_for('hisat2/align')


rule hisat2_db:
    input:
        fn=patterns['hisat2']['bam']
    output:
        done=patterns['hisat2']['db']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.sample
        df = parse_hisat2(srr, input.fn + '.log')
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.hisat2': clean_types(dd)
                }
            }
        )

        if df.ix[0, 'per_alignment'] < .20:
            remap.update_one(
                {'runs.srr': srr},
                {
                    '$addToSet': {
                        'runs.$.aln_flags': 'alignment_bad'
                    }
                }
            )

        Path(output.done).touch()


rule bai:
    input: bam='{prefix}.bam'
    output: bai='{prefix}.bam.bai'
    conda: '../config/extra_env.yaml'
    shell: """
    samtools index {input.bam}
    """


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        base = '-p -P -C -J '
    else:
        base = '-J '

    strand = get_strand(wildcards.sample)
    if strand == 'first_strand':
        return base + '-s 1'
    elif strand == 'second_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule featurecounts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam']
    output:
        counts=patterns['feature_counts']['counts'],
        summary=patterns['feature_counts']['summary']
    params: extra=_params_featurecounts
    threads: 4
    log: patterns['feature_counts']['counts'] + '.log'
    wrapper: wrapper_for('featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


rule samtools_stats_db:
    input:
        fn=patterns['samtools_stats']
    output:
        done=patterns['samtools_stats_db']
    run:
        from ncbi_remap.parser import parse_samtools_stats
        srr = wildcards.sample
        df = parse_samtools_stats(srr, input.fn)
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.samtools_stats': clean_types(dd)
                }
            }
        )

        Path(output.done).touch()
