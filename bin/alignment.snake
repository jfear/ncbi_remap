#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from pymongo import MongoClient

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib/python')
import ncbi_remap

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.alignment'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines()]

# s2 SRRs
with open('../data/1508_s2_cell_brian_annot.txt', 'r') as fh:
    s2 = [x.strip() for x in fh.readlines()]

samples = list(remap.aggregate([
    # Pull out an initial set of SRXs
    {
        '$match': {
            'aln_flags': {'$ne': 'complete'},
            'runs.srr': {'$exists': 1},
        }
    },
    {'$sort': {'_id': 1}},
    {'$limit': 5000},

    # Flag SRRs that have problems
    {'$unwind': '$runs'},
    {
        '$project': {
            '_id': 0,
            'runs.aln_flags': 1,
            'srx': '$_id',
            'srr': '$runs.srr',
            'pre_aln_flags': '$runs.pre_aln_flags',
            'aln_flags': '$runs.aln_flags',
            'flag_pre_aln_bad': {
                '$cond': [
                    {'$or': [
                        {'$in': ['download_bad', '$runs.pre_aln_flags']},
                        {'$in': ['alignment_bad', '$runs.pre_aln_flags']},
                        {'$in': ['quality_scores_bad', '$runs.pre_aln_flags']},
                        {'$in': ['abi_solid', '$runs.pre_aln_flags']},
                    ]},
                    1, 0]
            },
            'flag_pre_aln_complete': {
                '$cond': [{'$in': ['complete', '$runs.pre_aln_flags']}, 1, 0]
            },
            'flag_aln_bad': {
                '$cond': [{'$in': ['alignment_bad', {'$ifNull': ['$runs.aln_flags', []]}]}, 1, 0]
            },
        }
    },
    # Remove problem SRRs
    {
        '$match': {
            'flag_pre_aln_bad': {'$eq': 0},
            'flag_aln_bad': {'$eq': 0},
        }
    },

    # Collapse back to SRX level
    {
        '$group': {
            '_id': '$srx',
            'runs': {'$push': '$srr'},
            'total': {'$sum': 1},
            'ready': {'$sum': '$flag_pre_aln_complete'},
        },
    },
    {'$match': {'total': {'$gte': 1}}},

    # Make sure that all remaining SRRs for an SRX are ready
    {
        '$project': {
            '_id': 1,
            'runs': 1,
            'total': 1,
            'ready': 1,
            'diff': {'$subtract': ['$total', '$ready']},
        }
    },
    {'$match': {'diff': {'$eq': 0}}},

    # Format for sample table
    {'$unwind': '$runs'},
    {'$project': {'_id': 0, 'experiment': '$_id', 'sample': '$runs'}},
    {'$sort': {'sample': 1}},
]))

sample_table = pd.DataFrame(samples)

################################################################################
# Set up file naming patterns and targets
################################################################################
# Patterns
fastqs = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.fastq.gz'
    }

patterns = {
    # SRR Level Files
    'atropos': {
        'r1': '../output/alignment/raw/{experiment}/{sample}/{sample}_1.trim.clean.fastq.gz',
        'r2': '../output/alignment/raw/{experiment}/{sample}/{sample}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': '../output/known_splice_sites_r6-11.txt',
        'bam': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam',
        'db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.done',
    },
    'bai': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bai',
    'feature_counts': {
        'counts': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts',
        'summary': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts.summary',
    },
    'samtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats',
    'samtools_stats_db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats.done',
    'samtools_idxstats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.idxstats',
    'bamtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bamtools.stats',

    # SRX Level Files
    'bamCoverage': '../output/bigwigs/raw/{experiment}/{sample}/{sample}.fq.{strand}.bw',
    'expFeatureCounts': '../output/alignment/raw/{experiment}/{experiment}.counts',
    'expBamCoverage': '../output/bigwigs/raw/{experiment}/{experiment}.{strand}.bw',

    # MISC Files
    'chromSizes_fb': '../output/dmel_r6-11.flybase.chromsizes',
}

# Build target files
all_complete = {
    'srx': '../output/alignment/raw/{experiment}/{experiment}.done',
    'srr': '../output/alignment/raw/{experiment}/{sample}/{sample}.done',
    'bigwig': patterns['expBamCoverage']
}
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})
targets = helpers.fill_patterns(all_complete, sample_table)

localrules: hisat2_db

rule targets:
    input: utils.flatten(targets)


onsuccess:
    print('All Finished')
    mongo_client.close()


onerror:
    print('Something went wrong, you need to re-run')
    mongo_client.close()


rule check_srr:
    input:
        utils.flatten(patterns['feature_counts']) +
        [
            patterns['hisat2']['db'],
            patterns['samtools_stats_db'],
            patterns['samtools_idxstats'],
            patterns['bamtools_stats'],
        ]
    output: all_complete['srr']
    run:
        # Update database
        remap.find_one_and_update(
            {'runs.srr': wildcards.sample},
            {'$addToSet': {'runs.$.aln_flags': 'complete'}}
        )

        # Touch dummy file for snakemake
        Path(output[0]).touch()


def _check_srx(wildcards):
    sTable = sample_table[(sample_table['experiment'] == wildcards.experiment)].copy()
    _patterns = {
        'srrs': all_complete['srr'],
        'expBamCoverage': patterns['expBamCoverage'],
        'expFeatureCounts': patterns['expFeatureCounts'],
    }
    return utils.flatten(helpers.fill_patterns(_patterns, sTable))


rule check_srx:
    input: _check_srx
    output: all_complete['srx']
    run:
        # Update database
        remap.find_one_and_update(
            {'_id': wildcards.experiment},
            {'$addToSet': {'aln_flags': 'complete'}}
        )

        # Touch dummy file for snakemake
        Path(output[0]).touch()

################################################################################
# Fuctions
################################################################################
# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'first_strand' in flags:
        return 'first_strand'
    elif 'second_strand' in flags:
        return 'second_strand'
    else:
        return 'unstranded'


def clean_types(dic):
    newDic = {}
    for k, v in dic.items():
        if isinstance(v, np.int64):
            newDic[k] = int(v)
        elif isinstance(v, np.float64):
            newDic[k] = float(v)
        elif v is np.nan:
            pass
        elif v is None:
            pass
        else:
            newDic[k] = v
    return newDic


################################################################################
# FASTQ Pre-process
################################################################################
def _atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return {'R1': expand(fastqs['r1'], **wildcards)[0], 'R2': expand(fastqs['r2'], **wildcards)[0]}
    else:
        return {'R1': expand(fastqs['r1'], **wildcards)[0]}


def _params_extra_atropos(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return '-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20 --minimum-length 25'
    else:
        return '-a file:../data/adapters.fa -q 20 --minimum-length 25'


def _params_r2_atropos(wildcards):
    """Determine if the sample is PE or SE and return a temp R2 if PE."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r2'], **wildcards)[0] + '.tmp.gz'
    else:
        return None


rule atropos:
    input: unpack(_atropos)
    output:
        R1=temp(patterns['atropos']['r1'])
    params:
        extra=_params_extra_atropos,
        R2=_params_r2_atropos
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper: wrapper_for('atropos')


rule atropos_phony:
    input: rules.atropos.output
    output: temp(patterns['atropos']['r2'])
    shell: """
    mv {output[0]}.tmp.gz {output[0]}
    """


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input: gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


def _hisat2(wildcards):
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r1'], **wildcards)[0], expand(patterns['atropos']['r2'], **wildcards)[0]
    else:
        return expand(patterns['atropos']['r1'], **wildcards)[0]


def _params_hisat2_fastq(wildcards):
    base = '--dta --max-intronlen 300000 --known-splicesite-infile {splice} '.format(splice=patterns['hisat2']['splice_sites'])
    strand = get_strand(wildcards.sample)
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if strand == 'first_strand':
        if 'PE' in flags:
            return base + '--rna-strandness FR'
        else:
            return base + '--rna-strandness F'
    elif strand == 'second_strand':
        if 'PE' in flags:
            return base + '--rna-strandness RF'
        else:
            return base + '--rna-strandness R'

    return base


rule hisat2:
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=_hisat2,
    output: bam=temp(patterns['hisat2']['bam'])
    threads: 8
    params:
        hisat2_extra=_params_hisat2_fastq,
        samtools_view_extra="--threads 6 -q 20",
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper: wrapper_for('hisat2/align')


rule hisat2_db:
    input:
        fn=patterns['hisat2']['bam']
    output:
        done=patterns['hisat2']['db']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.sample
        df = parse_hisat2(srr, input.fn + '.log')
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.hisat2': clean_types(dd)
                }
            }
        )

        if df.ix[0, 'per_alignment'] < .20:
            remap.update_one(
                {'runs.srr': srr},
                {
                    '$addToSet': {
                        'runs.$.aln_flags': 'alignment_bad'
                    }
                }
            )

        Path(output.done).touch()


rule bai:
    input: bam='{prefix}.bam'
    output: bai=temp('{prefix}.bam.bai')
    conda: '../config/extra_env.yaml'
    shell: """
    samtools index {input.bam}
    """


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        base = '-p -P -C -J '
    else:
        base = '-J '

    strand = get_strand(wildcards.sample)
    if strand == 'first_strand':
        return base + '-s 1'
    elif strand == 'second_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule featurecounts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam']
    output:
        counts=patterns['feature_counts']['counts'],
        summary=patterns['feature_counts']['summary']
    params: extra=_params_featurecounts
    threads: 4
    log: patterns['feature_counts']['counts'] + '.log'
    wrapper: wrapper_for('featurecounts')


def _expFeatureCounts(wildcards):
    sTable = sample_table[(sample_table['experiment'] == wildcards.experiment)].copy()
    return utils.flatten(helpers.fill_patterns(dict(fc=patterns['feature_counts']['counts']), sTable))


rule expFeatureCounts:
    input: cnts=_expFeatureCounts
    output: patterns['expFeatureCounts']
    run:
        from ncbi_remap.parser import parse_featureCounts_counts
        dfs = []
        for cnt in input.cnts:
            name = os.path.basename(os.path.dirname(cnt))
            dfs.append(parse_featureCounts_counts(name, cnt).unstack(level=0))

        if len(dfs) > 1:
            df = pd.concat(dfs, axis=1)
        else:
            df = dfs[0]

        df.to_csv(output[0], sep='\t')



################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


rule samtools_stats_db:
    input:
        fn=patterns['samtools_stats']
    output:
        done=patterns['samtools_stats_db']
    run:
        from ncbi_remap.parser import parse_samtools_stats
        srr = wildcards.sample
        df = parse_samtools_stats(srr, input.fn)
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.samtools_stats': clean_types(dd)
                }
            }
        )

        Path(output.done).touch()


################################################################################
# Make Individual BedGraphs
################################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = '--outFileFormat bedgraph --binSize 1 '
    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai']
    output: temp('../output/bigwigs/raw/{experiment}/{sample}/{sample}.fq.{strand,first|second}.bedgraph')
    params:
        extra=_param_bamCoverage
    threads: 8
    wrapper: wrapper_for('deeptools/bamCoverage')


def _convertBedGraphToBigWig(wildcards):
    if 'flybase' in wildcards.prefix:
        return targets['chromSizes_fb']
    else:
        return config['references']['dmel']['chromSizes']


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=_convertBedGraphToBigWig
    output:
        bigwig='{prefix}.bw'
    conda: '../config/extra_env.yaml'
    shell:
        'export LC_COLLATE=C; '
        'tmpBg=`mktemp --suffix=.bedgraph` '
        '&& sort -k1,1 -k2,2n {input.bedgraph} > $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '


################################################################################
# Merge Experiment BigWigs
################################################################################
def _input_expMerge(wildcards):
    sTable = sample_table[(sample_table['experiment'] == wildcards.experiment) & (sample_table['strand'] == wildcards.strand)].copy()
    return utils.flatten(helpers.fill_patterns(dict(bam=patterns['bamCoverage']), sTable))


rule expMerge:
    input: _input_expMerge
    output: bedgraph=temp('../output/bigwigs/raw/{experiment}/{experiment}.{strand,first|second}.bedgraph')
    conda: '../config/extra_env.yaml'
    shell:"""
    inputs=({input})
    if [ ${{#inputs[@]}} -gt 1 ]; then
        bigWigMerge {input} {output.bedgraph}
    else
        bigWigToBedGraph {input[0]} {output.bedgraph}
    fi
    """
