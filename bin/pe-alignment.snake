#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys
from pathlib import Path

import pandas as pd
from pymongo import MongoClient

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib/python')
import ncbi_remap

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.pe-alignment'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines() if x.strip() != 'SRR325179']

samples = remap.aggregate([
    {'$unwind': '$runs'},
    {
        '$match': {
            '$and': [
                {'runs.srr': {'$exists': 1}},
                {'runs.srr': {'$in': golden}},
                {'runs.pre_aln_flags': 'PE'},
                {'runs.pre_aln_flags': 'complete'}
            ]
        }
    },
    {'$group': {'_id': {'sample': '$runs.srr', 'experiment': '$_id'}}},
    {'$project': {'_id': 0, 'experiment': '$_id.experiment', 'sample': '$_id.sample'}},
    {'$sort': {'sample': 1}},
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns and targets
################################################################################
# Patterns
fastqs = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.fastq.gz'
    }

fastqs_clean = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.clean.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.clean.fastq.gz'
    }

patterns = {
    'atropos': {
        'r1': '../output/alignment/raw/{experiment}/{sample}/{sample}_1.trim.clean.fastq.gz',
        'r2': '../output/alignment/raw/{experiment}/{sample}/{sample}_2.trim.clean.fastq.gz',
    },
    'fastq_atropos_count': {
        'r1': '../output/alignment/raw/{experiment}/{sample}/{sample}_1.trim.clean.fastq.gz.libsize',
        'r2': '../output/alignment/raw/{experiment}/{sample}/{sample}_2.trim.clean.fastq.gz.libsize',
    },
    'atropos_agg': {
        'summary': '../output/alignment/atropos_se_summary.tsv',
        'counts': '../output/alignment/atropos_se_counts.tsv',
    },
    'hisat2': {
        'splice_sites': '../output/known_splice_sites_r6-11.txt',
        'bam': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam',
        'db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.done',
    },
    'bai': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bai',
    'feature_counts': {
        'counts': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts',
        'summary': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.counts.summary',
    },
    'samtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats',
    'samtools_stats_db': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats.done',
    'samtools_idxstats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.idxstats',
    'bamtools_stats': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bamtools.stats',
}

# Build target files
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})
targets = helpers.fill_patterns(patterns, sample_table)

localrules: hisat2_db

rule targets:
    input:
        utils.flatten(targets['fastq_atropos_count']) +
        utils.flatten(targets['feature_counts']) +
        utils.flatten(targets['hisat2']) +
        utils.flatten(targets['atropos_agg']) +
        [targets['samtools_stats'],
         targets['samtools_stats_db'],
         targets['samtools_idxstats'],
         targets['bamtools_stats']]


################################################################################
# Fuctions
################################################################################
# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'first_strand' in flags:
        return 'first_strand'
    elif 'second_strand' in flags:
        return 'second_strand'
    else:
        return 'unstranded'


################################################################################
# FASTQ Pre-process
################################################################################
rule atropos:
    input:
        R1=fastqs['r1'],
        R2=fastqs['r2'],
    output:
        R1=temp(patterns['atropos']['r1']),
        R2=temp(patterns['atropos']['r2'])
    params:
        extra='-a file:../data/adapters.fa -A file:../data/adapters.fa -q 20'
    log: patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper: wrapper_for('atropos')


rule fastq_count:
    input: '{prefix}.fastq.gz'
    output: '{prefix}.fastq.gz.libsize'
    shell: 'gunzip -c {input} | echo $((`wc -l`/4)) > {output}'

def _atropos_agg(wildcards):
    return [x + '.log' for x in targets['atropos']['r1']]

rule atropos_agg:
    input: _atropos_agg
    output:
        summary=patterns['atropos_agg']['summary'],
        counts=patterns['atropos_agg']['counts'],
    run:
        from ncbi_remap.parser import parse_atropos
        summary = []
        cnts = []
        for fn in input:
            srr = os.path.basename(os.path.dirname(fn))
            dfSummary, dfCounts = parse_atropos(srr, fn)
            summary.append(dfSummary)
            cnts.append(dfCounts)
        pd.concat(summary).to_csv(output.summary, sep='\t')
        pd.concat(cnts).to_csv(output.counts, sep='\t')


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input: gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


def _params_hisat2_fastq(wildcards):
    base = '--dta --max-intronlen 300000 --known-splicesite-infile {input.splice_sites} '
    strand = get_strand(wildcards.sample)
    if strand == 'first_strand':
        return base + '--rna-strandness FR'
    elif strand == 'second_strand':
        return base + '--rna-strandness RF'
    else:
        return base


rule hisat2_fastq:
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=[rules.atropos.output.R1, rules.atropos.output.R2],
    output: bam=patterns['hisat2']['bam']
    threads: 8
    params:
        hisat2_extra=_params_hisat2_fastq,
        samtools_view_extra="--threads 6 -q 20",
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper: wrapper_for('hisat2/align')


rule hisat2_db:
    input:
        fn=patterns['hisat2']['bam'] + '.log'
    output:
        done=patterns['hisat2']['db']
    run:
        from ncbi_remap.parser import parse_hisat2_pe
        srr = wildcards.sample
        df = parse_hisat2_pe(srr, input.fn)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.hisat2': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()


rule bai:
    input: bam='{prefix}.bam'
    output: bai='{prefix}.bam.bai'
    conda: '../config/extra_env.yaml'
    shell: """
    samtools index {input.bam}
    """


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine strandedness and pass correct settings."""
    strand = get_strand(wildcards.sample)
    base = '-p -P -C -J '

    if strand == 'first_strand':
        return base + '-s 1'
    elif strand == 'second_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule featurecounts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam']
    output:
        counts=patterns['feature_counts']['counts'],
        summary=patterns['feature_counts']['summary']
    params: extra=_params_featurecounts
    threads: 4
    log: patterns['feature_counts']['counts'] + '.log'
    wrapper: wrapper_for('featurecounts')


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'

rule samtools_stats_db:
    input:
        fn=patterns['samtools_stats']
    output:
        done=patterns['samtools_stats_db']
    run:
        from ncbi_remap.parser import parse_samtools_stats
        srr = wildcards.sample
        df = parse_samtools_stats(srr, input.fn)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.aln_workflow.samtools_stats': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()

