#/usr/bin/env python
# vim: set ft=python.snakemake
""" The prealignment workflow to identify basic characteristics about SRA runs.

Zhenxia originally developed this workflow. The goal here is to use the data to
determine if the sample is really RNA-seq and strandedness.

"""
import os
import signal
import re
from textwrap import dedent
import shutil as sh
from gzip import open as gopen

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils

from sramongo.mongo import start_mongo
from mongoengine import connect

sys.path.insert(0, '../lib/python')
from ncbi_remap import mongo_schema as ms

try:
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
    assert os.path.exists(TMPDIR)
except:
    try:
        TMPDIR = os.getenv('TMPDIR')
        assert os.path.exists(TMPDIR)
    except:
        from tempfile import mkdtemp
        TMPDIR = mkdtemp(dir='.snakemake/')
        print('Temporary storage is at: {}'.format(TMPDIR))

shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

workdir: '.'

################################################################################
# Start/Stop MongoDB
################################################################################
# Store mongodb hostname
MONGODB_HOSTNAME = '.mongodb_host'
MONGODB_PID = '.mongodb_pid'

if not os.path.exists(MONGODB_HOSTNAME):
    # Record host
    with open(MONGODB_HOSTNAME, 'w') as fh:
        fh.write(os.getenv('SLURMD_NODENAME'))

    # Start mongo and save PID
    pid = start_mongo(dbDir='../output/db', logDir='../output/logs')
    with open(MONGODB_PID, 'w') as fh:
        fh.write(str(pid.pid))

onsuccess:
    # Stop mongodb
    with open(MONGODB_PID, 'r') as fh:
        pid = int(fh.read())
    os.kill(pid, signal.SIGTERM)
    os.remove(MONGODB_PID)

    # Remove hostname
    os.remove(MONGODB_HOSTNAME)
    print('Script complete.')

onerror:
    # Stop mongodb
    with open(MONGODB_PID, 'r') as fh:
        pid = int(fh.read())
    os.kill(pid, signal.SIGTERM)
    os.remove(MONGODB_PID)

    # Remove hostname
    os.remove(MONGODB_HOSTNAME)
    print('There was an error.')

# connect to mongo engine
with open(MONGODB_HOSTNAME, 'r') as fh:
    mongo_client = connect(db='sra', host=fh.read())

################################################################################
# Set up file naming patterns
################################################################################
patterns = {
    'gtf': '../references/dm6/r6-11/gtf/dm6_r6-11.gtf',
    'hisat2_index': ['../references/dm6/r6-11/hisat2/dm6_r6-11_chr2L.1.ht2',
                     '../references/dm6/r6-11/hisat2/dm6_r6-11_chr2L.2.ht2'],
    'splice_sites':     '../output/known_splice_sites_r6-11.txt',
    'bed12': '../output/dm6_r6-11.bed12',
    'bam': '../output/prealignment/raw/{experiment}/{sample}/{sample}.bam',
    'bai': '../output/prealignment/raw/{experiment}/{sample}/{sample}.bam.bai',
    'fastqc': {
        'html': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fastqc.html',
        'zip': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fastqc.zip',
        },
    'rseqc': {
        'bam_stat': '../output/prealignment/raw/{experiment}/{sample}/{sample}.bam_stat.txt',
        'infer_experiment': '../output/prealignment/raw/{experiment}/{sample}/{sample}.infer_experiment.txt',
        'geneBodyCoverage': {
            'txt': '../output/prealignment/raw/{experiment}/{sample}/{sample}.geneBody_coverage.txt',
            'r': '../output/prealignment/raw/{experiment}/{sample}/{sample}.geneBody_coverage.r',
            'img': '../output/prealignment/raw/{experiment}/{sample}/{sample}.geneBody_coverage.pdf',
        },
        'tin': {
            'table': '../output/prealignment/raw/{experiment}/{sample}/{sample}.tin.tsv',
            'summary': '../output/prealignment/raw/{experiment}/{sample}/{sample}.tin.txt',
        },
    },
}

FASTQ_pattern = '../output/prealignment/raw/{experiment}/{sample}/{sample}_{num}.fastq.gz'
# FASTQ_TRIM_pattern = '../output/prealignment/raw/{experiment}/{sample}/{sample}_{num}.trim.fastq.gz'

# Import run_id and experiment_id and dump into a dataframe
sample_table = pd.DataFrame(list(ms.Run._get_collection().find({}, {'_id': 1, 'experiment_id': 1})))
sample_table.columns = ['sample', 'experiment']
sample_table.sort_values(by='sample', inplace=True)
sample_table['num'] = 1

#targets = helpers.fill_patterns(patterns, sample_table.iloc[0:10000,:])
targets = helpers.fill_patterns(patterns, sample_table.iloc[0:5000,:])

rule targets:
    input: utils.flatten(targets)

def wrapper_for(path):
    URI = '../lcdb-wrapper-tests'
    return 'file:' + os.path.join(URI, 'wrappers', path)

################################################################################
# FASTQ Pre-process
################################################################################
def check_fastq(fn):
    """Checks if a gzip file actually has data in it."""
    try:
        assert os.stat(fn).st_size > 10000
        return True
    except (FileNotFoundError, AssertionError) as err:
        return False
    except:
        raise

"""Downloads fastq and checks if there is one or two sets of reads."""
rule fastq_dump:
    output:
        fastq = dynamic(FASTQ_pattern),
    run:
        srr = ms.Run.objects(pk=wildcards.sample).first()
        shell("fastq-dump -O $TMPDIR --split-files --gzip {wildcards.sample}")

        # Check read 1
        fn1 = wildcards.sample + '_1.fastq.gz'
        if check_fastq(os.path.join(TMPDIR, fn1)):
            sh.move(os.path.join(TMPDIR, fn1), FASTQ_pattern.format(sample=wildcards.sample, experiment=wildcards.experiment, num=1))

        # Check read 2
        fn2 = wildcards.sample + '_2.fastq.gz'
        if check_fastq(os.path.join(TMPDIR, fn2)):
            sh.move(os.path.join(TMPDIR, fn2), FASTQ_pattern.format(sample=wildcards.sample, experiment=wildcards.experiment, num=2))
            srr.modify(add_to_set__pipeline_flags='PE')
        else:
            srr.modify(add_to_set__pipeline_flags='SE')

rule fastqc:
    input: dynamic(FASTQ_pattern)
    output:
        html=patterns['fastqc']['html'],
        zip=patterns['fastqc']['zip']
    params: extra="--format fastq"
    wrapper:
        wrapper_for('fastqc')

#TODO: fix wrapper to accept a list and use length to determine how it handles output.
# rule cutadapt:
#     input: dynamic(FASTQ_pattern)
#     output: dynamic(FASTQ_TRIM_pattern)
#     params: extra="-a file:../data/adapters.fa"
#     wrapper:
#         wrapper_for('cutadapt')

################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input:
        gtf = patterns['gtf']
    output:
        patterns['splice_sites']
    shell:
        "hisat2_extract_splice_sites.py {input.gtf} > {output}"

rule hisat2:
    input:
        index=patterns['hisat2_index'],
        splice_sites=patterns['splice_sites'],
        fastq=dynamic(FASTQ_pattern)
    output:
        bam = patterns['bam']
    threads: 8
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {input.splice_sites}',
        samtools_view_extra = '-q 20',
        samtools_sort_extra = '--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log:
        patterns['bam'] + '.log'
    wrapper:
        wrapper_for('hisat2/align')

rule bai:
    input:
        bam = patterns['bam']
    output:
        bai = patterns['bai']
    shell:
        "samtools index {input.bam}"

################################################################################
# RSEQC
################################################################################
rule gtf2bed12:
    input:
        gtf = patterns['gtf']
    output:
        bed12 = patterns['bed12']
    run:
        import gffutils
        if not os.path.exists(input.gtf + '.db'):
            db = gffutils.create_db(data=input.gtf, dbfn=input.gtf + '.db', merge_strategy='merge',
                    id_spec={'transcript': ['transcript_id', 'transcript_symbol'],
                             'gene': ['gene_id', 'gene_symbol']},
                    gtf_transcript_key='transcript_id', gtf_gene_key='gene_id')
        else:
            db = gffutils.FeatureDB(input.gtf + '.db')

        with open(output.bed12, 'w') as fo:
            for t in db.features_of_type('transcript'):
                fo.write(db.bed12(t, name_field='transcript_id') + '\n')

rule infer_experiment:
    input:
        bam = patterns['bam'],
        bed = patterns['bed12']
    output:
        txt = patterns['rseqc']['infer_experiment']
    wrapper:
        wrapper_for('rseqc/infer_experiment')

rule geneBody_coverage:
    input:
        bam = patterns['bam'],
        bai = patterns['bai'],
        bed = patterns['bed12']
    output:
        txt = patterns['rseqc']['geneBodyCoverage']['txt'],
        r = patterns['rseqc']['geneBodyCoverage']['r'],
        img = patterns['rseqc']['geneBodyCoverage']['img']
    log:
        patterns['rseqc']['geneBodyCoverage']['txt'] + '.log'
    wrapper:
        wrapper_for('rseqc/geneBody_coverage')

rule tin:
    input:
        bam = patterns['bam'],
        bai = patterns['bai'],
        bed = patterns['bed12']
    output:
        table = patterns['rseqc']['tin']['table'],
        summary = patterns['rseqc']['tin']['summary']
    wrapper:
        wrapper_for('rseqc/tin')

rule bam_stat:
    input:
        bam = patterns['bam']
    output:
        txt = patterns['rseqc']['bam_stat']
    wrapper:
        wrapper_for('rseqc/bam_stat')
