#/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys

import numpy as np
import pandas as pd
from pymongo import MongoClient
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib/python')
import ncbi_remap

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.prealignment'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'same_strand' in flags:
        return 'same_strand'
    elif 'opposite_strand' in flags:
        return 'opposite_strand'
    else:
        return 'unstranded'


def clean_types(dic):
    if isinstance(dic, dict):
        newDic = {}
        for k, v in dic.items():
            if isinstance(v, np.int64):
                newDic[k] = int(v)
            elif isinstance(v, np.float64):
                newDic[k] = float(v)
            elif v is np.nan:
                pass
            elif v is None:
                pass
            else:
                newDic[k] = v
        return newDic
    elif isinstance(dic, list):
        newLis = []
        for row in dic:
            newLis.append(clean_types(row))
        return newLis


################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines()]

# s2 SRRs
with open('../data/1104_s2_cell_in_mieg.txt', 'r') as fh:
    s2 = [x.strip() for x in fh.readlines()]

with open('../data/1508_s2_cell_brian_annot.txt', 'r') as fh:
    s2_2 = [x.strip() for x in fh.readlines()]

samples = remap.aggregate([
    {'$unwind': '$runs'},
    {
        '$match': {
            '$and': [
                {'runs.srr': {'$exists': 1}},
                {'runs.libsize.R1': {'$exists': 1}},
                {'runs.pre_aln_flags': {'$ne': 'complete'}},
                {'runs.pre_aln_flags': {'$ne': 'download_bad'}},
                {'runs.pre_aln_flags': {'$ne': 'abi_solid'}},
                {'runs.pre_aln_flags': {'$ne': 'alignment_bad'}},
                {'runs.pre_aln_flags': {'$ne': 'quality_scores_bad'}},
            ],
        }
    },
    {'$group': {'_id': {'sample': '$runs.srr', 'experiment': '$_id'}}},
    {'$project': {'_id': 0, 'experiment': '$_id.experiment', 'sample': '$_id.sample'}},
    {'$sort': {'sample': 1}},
    {'$limit': 2000}
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns and targets
################################################################################
# Build target files
all_complete = {'files': '../output/prealignment/raw/{experiment}/{sample}/{sample}.done'}
targets = helpers.fill_patterns(all_complete, sample_table)

# Patterns
fastqs = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.fastq.gz'
    }

patterns = {
    'fastq_screen': {
        'txt': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fastq_screen.txt',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fastq_screen.done',
    },
    'fastqc': {
        'html': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fastqc.html',
        'zip': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fastqc.zip',
    },
    'atropos': {
        'r1': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.trim.clean.fastq.gz',
        'r2': '../output/prealignment/raw/{experiment}/{sample}/{sample}_2.trim.clean.fastq.gz',
    },
    'hisat2': {
        'splice_sites': '../output/known_splice_sites_r6-11.txt',
        'bam': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.done',
    },
    'bai': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.bai',
    'feature_counts': {
        'counts': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.feature_counts.counts',
        'jcounts': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.feature_counts.counts.jcounts',
        'summary': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.feature_counts.counts.summary',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.feature_counts.done',
    },
    'picard': {
        'collectrnaseqmetrics': {
            'metrics': {
                'unstranded': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.NONE.picard.collectrnaseqmetrics',
                'first': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.FIRST_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
                'second': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.SECOND_READ_TRANSCRIPTION_STRAND.picard.collectrnaseqmetrics',
            },
            'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.picard.collectrnaseqmetrics.done',
        },
        'markduplicates': {
            'bam': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.picard.markduplicates.bam',
            'metrics': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.picard.markduplicates.metrics',
            'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.picard.markduplicates.done',
        },
    },
    'samtools_stats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.samtools.stats',
    'samtools_stats_db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.samtools.stats.done',
    'samtools_idxstats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.samtools.idxstats',
    'samtools_idxstats_db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.samtools.idxstats.done',
    'bamtools_stats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.hisat2.bam.bamtools.stats',
}


localrules:
    check_results, bai, fastq_screen_db, hisat2_db,
    samtools_stats_db, samtools_idxstats_db, featurecounts_db, collectrnaseqmetrics_db,
    markduplicates_db


rule targets:
    input: utils.flatten(targets)

onsuccess:
    print('All Finished')
    mongo_client.close()

onerror:
    print('Something went wrong, you need to re-run')
    mongo_client.close()


rule check_results:
    input:
#         utils.flatten(patterns['fastqc']) +
        [
            patterns['fastq_screen']['db'],
            patterns['hisat2']['db'],
            patterns['samtools_stats_db'],
            patterns['samtools_idxstats_db'],
            patterns['feature_counts']['db'],
            patterns['picard']['collectrnaseqmetrics']['db'],
            patterns['picard']['markduplicates']['db'],
            patterns['samtools_idxstats'],
            patterns['bamtools_stats'],
        ]
    output: all_complete['files']
    run:
        # Update database
        remap.find_one_and_update(
            {'runs.srr': wildcards.sample},
            {'$addToSet': {'runs.$.pre_aln_flags': 'complete'}}
        )

        # Touch dummy file for snakemake
        Path(output[0]).touch()


################################################################################
# FASTQ QC
################################################################################
rule fastqc:
    input: fastqs['r1']
    output:
        html=patterns['fastqc']['html'],
        zip=patterns['fastqc']['zip']
    params: extra="--format fastq"
    log: patterns['fastqc']['html'] + '.log'
    wrapper: wrapper_for('fastqc')


rule fastq_screen:
    input:
        fastq=fastqs['r1'],
        dm6=config['references']['dmel']['bowtie2'],
        hg19=config['references']['human']['bowtie2'],
        wolbachia=config['references']['wolbachia']['bowtie2'],
        ecoli=config['references']['ecoli']['bowtie2'],
        yeast=config['references']['yeast']['bowtie2'],
        rRNA=config['references']['rRNA']['bowtie2'],
        phix=config['references']['phix']['bowtie2'],
        ercc=config['references']['ercc']['bowtie2'],
        adapters=config['references']['adapters']['bowtie2']
    output: txt=patterns['fastq_screen']['txt']
    log: patterns['fastq_screen']['txt'] + '.log'
    wrapper: wrapper_for('fastq_screen')


rule fastq_screen_db:
    input:
        fn=patterns['fastq_screen']['txt']
    output:
        done=patterns['fastq_screen']['db']
    run:
        from ncbi_remap.parser import parse_fastq_screen
        srr = wildcards.sample
        df = parse_fastq_screen(srr, input.fn)
        df2 = {k[1]: v for k, v in df.to_dict('index').items()}
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.fastq_screen': clean_types(df2)
                }
            }
        )

        Path(output.done).touch()


################################################################################
# FASTQ Pre-process
################################################################################
def _atropos(wildcards):
    """Determine if the sample is PE or SE"""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return {'R1': expand(fastqs['r1'], **wildcards)[0], 'R2': expand(fastqs['r2'], **wildcards)[0]}
    else:
        return {'R1': expand(fastqs['r1'], **wildcards)[0]}


def _params_extra_atropos(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return '-U 0 --minimum-length 25'
    else:
        return '--minimum-length 25'


def _params_r2_atropos(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r2'], **wildcards)[0] + '.tmp.gz'
    else:
        return None


rule atropos:
    input: unpack(_atropos)
    output:
        R1=temp(patterns['atropos']['r1'])
    params:
        extra=_params_extra_atropos,
        R2=_params_r2_atropos
    log:
        patterns['atropos']['r1'] + '.log'
    threads: 8
    wrapper: wrapper_for('atropos')


rule atropos_phony:
    input: rules.atropos.output
    output: temp(patterns['atropos']['r2'])
    shell: """
    mv {output[0]}.tmp.gz {output[0]}
    """


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input: gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda: "../config/extra_env.yaml"
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


def _hisat2(wildcards):
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        return expand(patterns['atropos']['r1'], **wildcards)[0], expand(patterns['atropos']['r2'], **wildcards)[0]
    else:
        return expand(patterns['atropos']['r1'], **wildcards)[0]


rule hisat2:
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=_hisat2
    output: bam=temp(patterns['hisat2']['bam'])
    threads: 8
    params:
        hisat2_extra='--max-intronlen 300000 --known-splicesite-infile {splice}'.format(splice=patterns['hisat2']['splice_sites']),
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper: wrapper_for('hisat2/align')


rule hisat2_db:
    input:
        fn=patterns['hisat2']['bam']
    output:
        done=patterns['hisat2']['db']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.sample
        df = parse_hisat2(srr, input.fn + '.log')
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.hisat2': clean_types(dd)
                }
            }
        )

        if df.ix[0, 'per_alignment'] < .50:
            remap.update_one(
                {'runs.srr': srr},
                {
                    '$addToSet': {
                        'runs.$.pre_aln_flags': 'alignment_bad'
                    }
                }
            )

        Path(output.done).touch()


rule bai:
    input: bam='{prefix}.bam'
    output: bai=temp('{prefix}.bam.bai')
    conda: "../config/extra_env.yaml"
    shell: "samtools index {input.bam}"


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine strandedness and pass correct settings."""
    flags = remap.find_one({'runs.srr': wildcards.sample}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']
    if 'PE' in flags:
        base = '-p -P -C -J '
    else:
        base = '-J '

    strand = get_strand(wildcards.sample)
    if strand == 'first_strand':
        return base + '-s 1'
    elif strand == 'second_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule feature_counts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam'],
    output:
        counts=patterns['feature_counts']['counts'],
        jcounts=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    params: extra=_params_featurecounts
    threads: 4
    log: patterns['feature_counts']['counts'] + '.log'
    wrapper: wrapper_for('featurecounts')


rule feature_counts_db:
    input:
        jcount=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    output:
        done=patterns['feature_counts']['db']
    run:
        from ncbi_remap.parser import parse_featureCounts_jcounts, parse_featureCounts_summary
        srr = wildcards.sample
        dfJ = parse_featureCounts_jcounts(srr, input.jcount)
        if dfJ.shape[0] > 0:
            num_junction_reads = dfJ.loc[~dfJ.PrimaryGene.isnull(), 'count'].sum()
        else:
            num_junction_reads = 0

        dfS = parse_featureCounts_summary(srr, input.summary)
        dd = dfS.to_dict('index')[srr]
        dd['Assigned_Junction'] = num_junction_reads

        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.featurecounts': clean_types(dd)
                }
            }
        )

        Path(output.done).touch()


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


rule samtools_stats_db:
    input:
        fn=patterns['samtools_stats']
    output:
        done=patterns['samtools_stats_db']
    run:
        from ncbi_remap.parser import parse_samtools_stats
        srr = wildcards.sample
        df = parse_samtools_stats(srr, input.fn)
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.samtools_stats': clean_types(dd)
                }
            }
        )

        Path(output.done).touch()


rule samtools_idxstats_db:
    input:
        fn=patterns['samtools_idxstats']
    output:
        done=patterns['samtools_idxstats_db']
    run:
        from ncbi_remap.parser import parse_samtools_idxstats
        srr = wildcards.sample
        df = parse_samtools_idxstats(srr, input.fn)
        df.columns = ['chrom', 'length', 'num_mapped_reads', 'num_unmapped_reads']
        records = df.to_dict('records')
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.samtools_idxstats': clean_types(records)
                }
            }
        )

        Path(output.done).touch()

################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params:
        extra='STRAND=NONE',
        java_args='-Xmx30g'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params:
        extra='STRAND=FIRST_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params:
        extra='STRAND=SECOND_READ_TRANSCRIPTION_STRAND',
        java_args='-Xmx30g'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_db:
    input:
        unstranded=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first=patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second=patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        done=patterns['picard']['collectrnaseqmetrics']['db']
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srr = wildcards.sample
        dfU = parse_picardCollect_summary(srr, input.unstranded)
        dfF = parse_picardCollect_summary(srr, input.first)
        dfS = parse_picardCollect_summary(srr, input.second)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.picard_collectrnaseqmetrics': {
                        'unstranded': clean_types(dfU.to_dict('index')[srr]),
                        'first': clean_types(dfF.to_dict('index')[srr]),
                        'second': clean_types(dfS.to_dict('index')[srr]),
                    },
                }
            }
        )

        Path(output.done).touch()


rule markduplicates:
    input: bam=patterns['hisat2']['bam']
    output:
        bam=temp(patterns['picard']['markduplicates']['bam']),
        metrics=patterns['picard']['markduplicates']['metrics']
    params:
        java_args='-Xmx30g'
    log: patterns['picard']['markduplicates']['metrics'] + '.log'
    wrapper: wrapper_for('picard/markduplicates')


rule markduplicates_db:
    input:
        fn=patterns['picard']['markduplicates']['metrics'],
    output:
        done=patterns['picard']['markduplicates']['db']
    run:
        from ncbi_remap.parser import parse_picard_markduplicate_metrics
        srr = wildcards.sample
        df = parse_picard_markduplicate_metrics(srr, input.fn)
        dd = df.to_dict('index')[srr]
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.picard_markduplicates': clean_types(dd)
                }
            }
        )

        Path(output.done).touch()
