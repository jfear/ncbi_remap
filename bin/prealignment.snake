#/usr/bin/env python
# vim: set ft=python.snakemake
""" The prealignment workflow to identify basic characteristics about SRA runs.

Zhenxia originally developed this workflow. The goal here is to use the data to
determine if the sample is really RNA-seq and strandedness.

"""
import os
import re
from textwrap import dedent
from shutil import move as shmove
from gzip import open as gopen

import numpy as np
import pandas as pd

from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils

from sramongo.mongo import start_mongo
from mongoengine import connect

sys.path.insert(0, '../lib/python')
from ncbi_remap import mongo_schema as ms

try:
    TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
    assert os.path.exists(TMPDIR)
except:
    try:
        TMPDIR = os.getenv('TMPDIR')
        assert os.path.exists(TMPDIR)
    except:
        from tempfile import mkdtemp
        TMPDIR = mkdtemp(dir='.snakemake/')
        print('Temporary storage is at: {}'.format(TMPDIR))

shell.prefix("set -euo pipefail; export TMPDIR={}; source activate ncbi_remap;".format(TMPDIR))

workdir: '.'


# onstart:
#     # Start mongo
#     global pid
#     pid = start_mongo(dbDir='../output/db', logDir='../output/logs')
#
# onsuccess:
#     # Shutdown mongo
#     pid.kill()
#     print('Script complete.')
#
# onerror:
#     # Shutdown mongo
#     pid.kill()
#     print('There was an error.')

# connect to mongo engine
mongo_client = connect('sra')


patterns = {
    'gtf': '../references/dm6/r6-11/gtf/dm6_r6-11.gtf',
    'hisat2_index': ['../references/dm6/r6-11/hisat2/dm6_r6-11_chr2L.1.ht2',
                     '../references/dm6/r6-11/hisat2/dm6_r6-11_chr2L.2.ht2'],
    'splice_sites':     '../output/known_splice_sites_r6-11.txt',
    'bed12': '../output/dm6_r6-11.bed12',
    'bam': '../output/prealignment/samples/{sample}/{sample}.bam',
    'bai': '../output/prealignment/samples/{sample}/{sample}.bam.bai',
    'fastq_dump': {
        'fastq_1': '../output/prealignment/samples/{sample}/{sample}_1.fastq.gz',
        },
    'fastqc': {
        'html': '../output/prealignment/samples/{sample}/{sample}.fastqc.html',
        'zip': '../output/prealignment/samples/{sample}/{sample}.fastqc.zip',
        },
    'rseqc': {
        'bam_stat': '../output/prealignment/samples/{sample}/{sample}.bam_stat.txt',
        'bam_stat_agg': '../output/prealignment/rseqc_bam_stat.txt',
        'infer_experiment': '../output/prealignment/samples/{sample}/{sample}.infer_experiment.txt',
        'infer_experiment_agg': '../output/prealignment/rseqc_infer_experiment.txt',
        'geneBodyCoverage': {
            'txt': '../output/prealignment/samples/{sample}/{sample}.geneBody_coverage.txt',
            'r': '../output/prealignment/samples/{sample}/{sample}.geneBody_coverage.r',
            'img': '../output/prealignment/samples/{sample}/{sample}.geneBody_coverage.pdf',
        },
        'geneBodyCoverage_agg': '../output/prealignment/rseqc_geneBody_coverage.txt',
        'tin': {
            'table': '../output/prealignment/samples/{sample}/{sample}.tin.tsv',
            'summary': '../output/prealignment/samples/{sample}/{sample}.tin.txt',
        },
        'tin_agg': '../output/prealignment/rseqc_tin.txt',
    },
}


samples = ['SRR3001915', 'SRR5100246']

fill = dict(sample=samples)
targets = helpers.fill_patterns(patterns, fill)

rule targets:
    input: utils.flatten(targets)

def wrapper_for(path):
    URI = '../lcdb-wrapper-tests'
    return 'file://' + os.path.join(URI, 'wrappers', path)

# FASTQ Pre-process ##############################################################################

def check_fastq(fn):
    try:
        with gopen(fn, 'rt') as fh:
            assert len(fh.read()) > 10
        return True
    except (FileNotFoundError, AssertionError) as err:
        return False
    except:
        raise

"""Downloads fastq and checks if there is one or two sets of reads."""
rule fastq_dump:
    output:
        fastq = patterns['fastq_dump']['fastq_1'],
    run:
        srr = ms.Run.objects(pk=wildcards.sample).first()
        shell("fastq-dump -O $TMPDIR --split-files --gzip {wildcards.sample}")

        # Check read 1
        fn1 = wildcards.sample + '_1.fastq.gz'
        if check_fastq(os.path.join(TMPDIR, fn1)):
            shmove(os.path.join(TMPDIR, fn1), output.fastq)

        # Check read 2
        fn2 = wildcards.sample + '_2.fastq.gz'
        if check_fastq(os.path.join(TMPDIR, fn2)):
            shmove(os.path.join(TMPDIR, fn2), output.fastq.replace('_1.fastq', '_2.fastq'))
            srr.modify(add_to_set__pipeline_flags='PE')
        else:
            srr.modify(add_to_set__pipeline_flags='SE')

def _fastqc(wildcards):
    _sample = wildcards.sample
    srr = ms.Run.objects(pk=_sample).first()
    fn1 = patterns['fastq_dump']['fastq_1'].format(sample=_sample)
    if 'SE' in srr.pipeline_flags:
        return fn1
    elif 'PE' in srr.pipeline_flags:
        return fn1, fn1.replace('_1.fastq', '_2.fastq')

rule fastqc:
    input: _fastqc
    output:
        html=patterns['fastqc']['html'],
        zip=patterns['fastqc']['zip']
    params: extra="--format fastq"
    wrapper:
        wrapper_for('fastqc')


# Alignment ##############################################################################
rule hisat2_splice_site:
    input:
        gtf = patterns['gtf']
    output:
        patterns['splice_sites']
    shell:
        "hisat2_extract_splice_sites.py {input.gtf} > {output}"

def _hisat2(wildcards):
    _sample = wildcards.sample
    srr = ms.Run.objects(pk=_sample).first()
    fn1 = patterns['fastq_dump']['fastq_1'].format(sample=_sample)
    if 'SE' in srr.pipeline_flags:
        return fn1
    elif 'PE' in srr.pipeline_flags:
        return fn1, fn1.replace('_1.fastq', '_2.fastq')

rule hisat2:
    input:
        index=patterns['hisat2_index'],
        splice_sites = patterns['splice_sites'],
        fastq = _hisat2
    output:
        bam = patterns['bam']
    threads: 8
    params:
        hisat2_extra = '--max-intronlen 300000 --known-splicesite-infile {input.splice_sites}',
        samtools_view_extra = '-q 20',
        samtools_sort_extra = '--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log:
        patterns['bam'] + '.log'
    wrapper:
        wrapper_for('hisat2/align')

rule bai:
    input:
        bam = patterns['bam']
    output:
        bai = patterns['bai']
    shell:
        "samtools index {input.bam}"

rule gtf2bed12:
    input:
        gtf = patterns['gtf']
    output:
        bed12 = patterns['bed12']
    run:
        import gffutils
        if not os.path.exists(input.gtf + '.db'):
            db = gffutils.create_db(data=input.gtf, dbfn=input.gtf + '.db', merge_strategy='merge',
                    id_spec={'transcript': ['transcript_id', 'transcript_symbol'],
                             'gene': ['gene_id', 'gene_symbol']},
                    gtf_transcript_key='transcript_id', gtf_gene_key='gene_id')
        else:
            db = gffutils.FeatureDB(input.gtf + '.db')

        with open(output.bed12, 'w') as fo:
            for t in db.features_of_type('transcript'):
                fo.write(db.bed12(t, name_field='transcript_id') + '\n')

# RSEQC ##############################################################################
rule infer_experiment:
    input:
        bam = patterns['bam'],
        bed = patterns['bed12']
    output:
        txt = patterns['rseqc']['infer_experiment']
    wrapper:
        wrapper_for('rseqc/infer_experiment')

rule infer_experiment_agg:
    input:
        txt = targets['rseqc']['infer_experiment']
    output:
        txt = patterns['rseqc']['infer_experiment_agg']
    run:
        header = ['sample_id', 'Type', 'Undetermined', '++,--', '+-,+-', 'stranded']

        se_regex = dedent("""\
                This is SingleEnd Data
                Fraction of reads failed to determine: (\d\.\d+)
                Fraction of reads explained by \"\+\+\,\-\-\"\: (\d\.\d+)
                Fraction of reads explained by \"\+\-\,\-\+\"\: (\d\.\d+).*""")

        pe_regex = dedent("""\
                This is PairEnd Data
                Fraction of reads failed to determine: (\d\.\d+)
                Fraction of reads explained by \"1\+\+\,1\-\-,2\+\-\,2\-\+\"\: (\d\.\d+)
                Fraction of reads explained by \"1\+\-\,1\-\+,2\+\+,2\-\-\"\: (\d\.\d+).*""")

        with open(output.txt, 'w') as fo:
            fo.write('\t'.join(header) + '\n')

            for f in input.txt:
                sample = os.path.basename(f).replace('.infer_experiment.txt', '')

                with open(f, 'r') as fh:
                    data = fh.read().strip()
                    try:
                        # SingleEnd
                        m = re.match(se_regex, data, flags=re.DOTALL).groups()
                        _type = 'SingleEnd'
                    except:
                        try:
                            # PairEnd
                            m = re.match(pe_regex, data, flags=re.DOTALL).groups()
                            _type = 'PairEnd'
                        except:
                            print(f)
                            print(regex)
                            print(data)
                            raise

                    if np.abs(float(m[1]) - float(m[2])) >= .20:
                        if _type == 'SingleEnd':
                            if float(m[1]) > float(m[2]):
                                stranded = 'F'
                            else:
                                stranded = 'R'
                        else:
                            if float(m[1]) > float(m[2]):
                                stranded = 'FR'
                            else:
                                stranded = 'RF'
                    else:
                        stranded = 'NonStranded'

                    fo.write('\t'.join([sample, _type, *m, stranded]) + '\n')

rule geneBody_coverage:
    input:
        bam = patterns['bam'],
        bai = patterns['bai'],
        bed = patterns['bed12']
    output:
        txt = patterns['rseqc']['geneBodyCoverage']['txt'],
        r = patterns['rseqc']['geneBodyCoverage']['r'],
        img = patterns['rseqc']['geneBodyCoverage']['img']
    log:
        patterns['rseqc']['geneBodyCoverage']['txt'] + '.log'
    wrapper:
        wrapper_for('rseqc/geneBody_coverage')

rule geneBody_coverage_agg:
    input:
        txt = targets['rseqc']['geneBodyCoverage']['txt']
    output:
        txt = patterns['rseqc']['geneBodyCoverage_agg']
    run:
        with open(output.txt , 'w') as fo:
            for i, f in enumerate(input.txt):
                with open(f, 'r') as fh:
                    if i == 0:
                        fo.write(fh.read().replace('.prealn', '').replace('Percentile', 'sample_id'))
                    else:
                        try:
                            fo.write(fh.readlines()[1].replace('.prealn', ''))
                        except IndexError:
                            print(f)

rule tin:
    input:
        bam = patterns['bam'],
        bai = patterns['bai'],
        bed = patterns['bed12']
    output:
        table = patterns['rseqc']['tin']['table'],
        summary = patterns['rseqc']['tin']['summary']
    wrapper:
        wrapper_for('rseqc/tin')

rule tin_agg:
    input:
        txt = targets['rseqc']['tin']['summary']
    output:
        txt = patterns['rseqc']['tin_agg']
    run:
        with open(output.txt , 'w') as fo:
            for i, f in enumerate(input.txt):
                with open(f, 'r') as fh:
                    if i == 0:
                        fo.write(fh.read().replace('.prealn.bam', '').replace('Bam_file', 'sample_id'))
                    else:
                        fo.write(fh.readlines()[1].replace('.prealn.bam', ''))

rule bam_stat:
    input:
        bam = patterns['bam']
    output:
        txt = patterns['rseqc']['bam_stat']
    wrapper:
        wrapper_for('rseqc/bam_stat')

rule bam_stat_agg:
    input:
        txt = targets['rseqc']['bam_stat']
    output:
        txt = patterns['rseqc']['bam_stat_agg']
    run:
        header = ['sample_id', 'Total_records', 'QC_failed', 'Optical_PCR_duplicate',
                  'Non_primary_hits', 'Unmapped_reads', 'non-unique',
                  'unique', 'Read-1', 'Read-2', 'Reads_map_plus', 'Reads_map_minus', 'Non-splice_reads',
                  'Splice_reads', 'Reads_mapped_in_proper_pairs',
                  'Proper-paired_reads_map_to_different_chrom']

        regex = dedent("""\
            .*Total.* (\d+)

            QC.* (\d+)
            Optical.* (\d+)
            Non primary hits.* (\d+)
            Unmapped reads.* (\d+)
            mapq.* (\d+)

            mapq.* (\d+)
            Read-1.* (\d+)
            Read-2.* (\d+)
            Reads map to \'\+\'.* (\d+)
            Reads map to \'-\'.* (\d+)
            Non-splice reads.* (\d+)
            Splice reads.* (\d+)
            Reads mapped.* (\d+)
            Proper-paired.*:(\d+)""")

        with open(output.txt, 'w') as fo:
            fo.write('\t'.join(header) + '\n')
            for f in input.txt:
                sample = os.path.basename(f).replace('.bam_stat.txt', '')
                with open(f, 'r') as fh:
                    m = re.match(regex, fh.read().strip(), flags=re.DOTALL).groups()
                    fo.write('\t'.join([sample, *m]) + '\n')
