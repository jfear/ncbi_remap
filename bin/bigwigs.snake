#!/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys

import pandas as pd
from pymongo import MongoClient

from snakemake.io import regex

from lcdblib.snakemake import helpers
from lcdblib.utils import utils
from lcdblib.pandas.utils import cartesian_product

sys.path.insert(0, '../lib/python')
import ncbi_remap

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.bigwigs'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

################################################################################
# Build Sample Table
################################################################################
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

samples = remap.aggregate([
    {'$unwind': '$runs'},
    {
        '$match': {
            '$and': [
                {'runs.srr': {'$exists': 1}},
                {'runs.srr': {'$in': golden}},
                {'runs.pre_aln_flags': 'SE'},
                {'runs.pre_aln_flags': 'complete'}
            ]
        }
    },
    {'$group': {'_id': {'sample': '$runs.srr', 'experiment': '$_id'}}},
    {'$project': {'_id': 0, 'experiment': '$_id.experiment', 'sample': '$_id.sample'}},
    {'$sort': {'sample': 1}},
    {'$limit': 1}
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns and targets
################################################################################

# Patterns
patterns = {
    'bam': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam',
    'bai': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.bam.bai',
    'bamCoverage': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.{strand}.bw',
    'bamCoverage_fb': '../output/alignment/raw/{experiment}/{sample}/{sample}.fq.flybase.{strand}.bw',
    'expBamCoverage': '../output/alignment/raw/{experiment}/{experiment}.{strand}.bw',
    'expBamCoverage_fb': '../output/alignment/raw/{experiment}/{experiment}.flybase.{strand}.bw',
    'allBamCoverage': '../output/alignment/golden_312.{strand}.bw',
    'allBamCoverage_fb': '../output/alignment/golden_312.flybase.{strand}.bw',
    'inverse_bed': '../output/inverse_exons_20bp.{strand}.bed',
    'unannotated': '../output/alignment/golden_312.{strand}.unannotated.bw',
    'hub': '/data/fearjm/datashare/ncbi_remap/hubs/flybase_example_hub.txt',
}

# Build target files
sample_table = cartesian_product(sample_table, {'strand': ['first', 'second']})
targets = helpers.fill_patterns(patterns, sample_table)

rule targets:
    input:
       utils.flatten(targets['bamCoverage']) +
       utils.flatten(targets['hub'])


################################################################################
# Fuctions
################################################################################
# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'first_strand' in flags:
        return 'first_strand'
    elif 'second_strand' in flags:
        return 'second_strand'
    else:
        return 'unstranded'


################################################################################
# Make Individual BedGraphs
################################################################################
def _param_bamCoverage(wildcards):
    """Get strand information from wildcards."""
    base = '--outFileFormat bedgraph --binSize 1 '
    if wildcards.strand == 'first':
        return base + '--filterRNAstrand forward'
    elif wildcards.strand == 'second':
        return base + '--filterRNAstrand reverse'


rule bamCoverage:
    input:
        bam='{prefix}.bam',
        bai='{prefix}.bam.bai',
    output: '{prefix}.{strand,first|second}.bedgraph'
    params:
        extra=_param_bamCoverage
    threads: 8
    wrapper: wrapper_for('deeptools/bamCoverage')


rule convertBedGraphToBigWig:
    input:
        bedgraph='{prefix}.bedgraph',
        chromSizes=config['references']['dmel']['chromSizes']
    output:
        bigwig='{prefix}.bw'
    shell:
        'source activate ncbi_remap '
        '&& tmpBg=`mktemp --suffix=.bedgraph` '
        '&& sort -k1,1 -k2,2n {input.bedgraph} > $tmpBg '
        '&& bedGraphToBigWig $tmpBg {input.chromSizes} {output.bigwig} '
        '&& rm $tmpBg '


################################################################################
# Merge Experiment BigWigs
################################################################################
def _input_expMerge(wildcards):
    sTable = sample_table[sample_table['experiment'] == wildcards.experiment].copy()
    return utils.flatten(helpers.fill_patterns(dict(bam=patterns['bamCoverage']), sTable))


rule expMerge:
    input: _input_expMerge
    output: bedgraph=temp(patterns['expBamCoverage'].replace('.bw', '.bedgraph'))
    shell: """
    source activate ncbi_remap
    bigWigMerge {input} {output.bedgraph}
    """


def _allMerge(wildcards):
    inputs = []
    for fn in targets['bamCoverage']:
        if wildcards.strand in fn:
            inputs.append(fn)
    return inputs


rule allMerge:
    input: _allMerge
    output: bedgraph='../output/alignment/golden_312.{strand,first|second}.bedgraph'
    shell: """
    source activate ncbi_remap
    bigWigMerge {input} {output.bedgraph}
    """


################################################################################
# Convert to Flybase
################################################################################
rule convertFlybase:
    input: bedgraph='{prefix}.{strand}.bedgraph'
    output: bedgraph=temp('{prefix}.flybase.{strand,first|second}.bedgraph')
    shell:"""
    source activate ncbi_remap
    chrom_convert --from UCSC --to FlyBase --fileType BED -i {input.bedgraph} -o {output.bedgraph}
    """


################################################################################
# Create Diff BedGraphs
################################################################################
rule create_inverse_bed:
    input:
        gtf=config['references']['dmel']['gtf'],
        chromSizes=config['references']['dmel']['chromSizes']
    output:
        plus=patterns['inverse_bed'].format(strand='first'),
        minus=patterns['inverse_bed'].format(strand='second')
    run:
        import pybedtools

        # Import GTF
        gtf = pybedtools.BedTool(input.gtf).remove_invalid().saveas()

        # Subset functions for pybedtools
        def featuretype_filter(feature, featuretype):
            if feature[2] == featuretype:
                return True
            return False

        def subset_featuretypes(g, featuretype):
            result = g.filter(featuretype_filter, featuretype).saveas()
            return pybedtools.BedTool(result.fn)

        def strand_filter(feature, strand):
            if feature[3] == strand:
                return True
            return False

        def subset_strand(g, strand):
            result = g.filter(strand_filter, strand).saveas()
            return pybedtools.BedTool(result.fn)

        # pull out all exons
        exons = subset_featuretypes(gtf, 'exon').bed6()

        # Merge overlapping regions strand specifically
        merged = exons.sort().merge(s=True, bed=True)

        # Extend features by 20bp
        extended = merged.slop( g=input.chromSizes, b=20)

        # split by strand
        exons_plus = pybedtools.BedTool([x[:3] for x in subset_strand(extended, '+')])
        exons_minus = pybedtools.BedTool([x[:3] for x in subset_strand(extended, '-')])

        pybedtools.BedTool([x[:3] + ['.', '.', '+'] for x in exons_plus.sort().complement(g=input.chromSizes)]).saveas(output.plus)
        pybedtools.BedTool([x[:3] + ['.', '.', '-'] for x in exons_minus.sort().complement(g=input.chromSizes)]).saveas(output.minus)


rule unannotated:
    input:
        bedgraph='{prefix}.{strand}.bedgraph',
        bed=patterns['inverse_bed']
    output:
        bedgraph=temp('{prefix}.{strand}.unannotated.bedgraph')
    run:
        import pybedtools

        if wildcards.strand == 'first':
            strand = '+'
        else:
            strand = '-'

        bg = pybedtools.BedTool([x[:4] + ['.', strand] for x in pybedtools.BedTool(input.bedgraph)])
        bed = pybedtools.BedTool(input.bed)
        pybedtools.BedTool([x[:4] for x in bg.intersect(bed, s=True)]).saveas(output.bedgraph)


################################################################################
# Create Track Hub
################################################################################
def _trackhub_individual(wildcards):
    pat = dict(bc=patterns['bamCoverage'])
    first_st = sample_table[sample_table['strand'] == 'first']
    first = helpers.fill_patterns(pat, first_st)['bc']

    return list(first)


rule trackhub:
    input:
        individual=_trackhub_individual
    output: targets['hub']
    run:
        from trackhub import Track, AggregateTrack, default_hub
        from trackhub.upload import upload_hub

        hub, genomes_file, genome, trackdb = default_hub(
            hub_name="hisat2",
            genome="dm6",
            short_label="Flybase example Hisat2",
            long_label="Flybase example Hisat2",
            email="justin.fear@nih.gov")

        # hub's location on remote host, for use with rsync
        hub.remote_fn = output[0]

        # publicly accessible hub URL
        hub.url = output[0].replace('/data/fearjm/datashare', 'http://helix.nih.gov/~fearjm')

        rx = re.compile(regex(patterns['bamCoverage']))
        for first in input.individual:
            second = first.replace('first', 'second')

            m = re.match(rx, first).groupdict()
            label = '{experiment}_{sample}'.format(**m)

            aggregate = AggregateTrack(
                name=label,
                tracktype='bigWig 0 2000',
                short_label=label,
                long_label=label,
                aggregate='transparentOverlay',
                showSubtrackColorOnUi='on',
            )

            # Parameters are checked for valid values, see
            l1 = label + '_first'
            track1 = Track(
                name=l1,
                short_label=l1,
                long_label=l1,
                autoScale='off',
                local_fn=first,
                tracktype='bigWig',
                color="255,0,0"
                )

            l2 = label + '_second'
            track2 = Track(
                name=l2,
                short_label=l2,
                long_label=l2,
                autoScale='off',
                local_fn=second,
                tracktype='bigWig',
                color="0,0,255"
                )

            aggregate.add_subtrack(track1)
            aggregate.add_subtrack(track2)
            trackdb.add_tracks(aggregate)

        # Render the hub to text files
        hub.render()

        # Upload the hub files and all the bigwig files using rsync.
        upload_hub(None, None, hub=hub)

