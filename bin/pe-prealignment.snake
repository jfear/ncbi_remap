#/usr/bin/env python
# vim: set ft=python.snakemake
import os
import sys

import pandas as pd
from pymongo import MongoClient
from pathlib import Path

from lcdblib.snakemake import helpers
from lcdblib.utils import utils

sys.path.insert(0, '../lib/python')
import ncbi_remap
from ncbi_remap.parser import parse_inferExperiment

# Setup tempdir to work with lscratch
TMPDIR = os.path.join('/lscratch', os.getenv('SLURM_JOBID'))
shell.prefix("set -euo pipefail; export TMPDIR={};".format(TMPDIR))

# Set working dir
workdir: '../.pe-prealignment'

# import config
configfile: '../config/prealignment_config.yaml'

# Connect to mongodb
with open('../output/.mongodb_host', 'r') as fh:
    mongo_client = MongoClient(host=fh.read().strip(), port=27022)
    db = mongo_client['sra2']
    remap = db['remap']

# Find snakemake wrappers:
def wrapper_for(path):
    URI = '../lcdb-wf/wrappers/wrappers'
    return 'file:' + os.path.join(URI, path)


def get_strand(srr):
    """Check strandedsess."""
    flags = remap.find_one({'runs.srr': srr}, {'runs.$.pre_aln_flags': 1})['runs'][0]['pre_aln_flags']

    if 'same_strand' in flags:
        return 'same_strand'
    elif 'opposite_strand' in flags:
        return 'opposite_strand'
    else:
        return 'unstranded'


################################################################################
# Build Sample Table
################################################################################
# Golden 312 test runs (312)
with open('../data/312_sample_golden_set_2016-06-14.txt', 'r') as fh:
    golden = [x.strip() for x in fh.readlines()]

# All SRRs that the Miegs analyzed (13495)
with open('../data/13495_runs_analyzed_by_mieg.txt', 'r') as fh:
    mieg = [x.strip() for x in fh.readlines()]

# modEncode SRRs (644)
with open('../data/modEncode_srrs.txt', 'r') as fh:
    modEncode = [x.strip() for x in fh.readlines()]

# s2 SRRs
with open('../data/1104_s2_cell_in_mieg.txt', 'r') as fh:
    s2 = [x.strip() for x in fh.readlines()]

samples = remap.aggregate([
    {'$unwind': '$runs'},
    {
        '$match': {
            '$and': [
                {'runs.srr': {'$exists': 1}},
                {'runs.srr': {'$in': s2}},
                {'runs.pre_aln_flags': {'$ne': 'complete'}},
                {'runs.pre_aln_flags': {'$ne': 'download_bad'}},
                {'runs.pre_aln_flags': 'PE'}
            ]
        }
    },
    {'$group': {'_id': {'sample': '$runs.srr', 'experiment': '$_id'}}},
    {'$project': {'_id': 0, 'experiment': '$_id.experiment', 'sample': '$_id.sample'}},
    {'$sort': {'sample': 1}},
])

sample_table = pd.DataFrame(list(samples))

################################################################################
# Set up file naming patterns and targets
################################################################################
# Build target files
all_complete = {'files': '../output/prealignment/raw/{experiment}/{sample}/{sample}.done'}
targets = helpers.fill_patterns(all_complete, sample_table)

# Patterns
fastqs = {
        'r1': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_1.fastq.gz',
        'r2': '../output/pre-prealignment/raw/{experiment}/{sample}/{sample}_2.fastq.gz'
    }

patterns = {
    'fastq_screen': {
        'txt': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.clean_screen.txt',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.clean_screen.done',
    },
    'fastqc': {
        'html': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1_fastqc.html',
        'zip': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1_fastqc.zip',
    },
    'bed12': '../output/dm6_r6-11.bed12',
    'hisat2': {
        'splice_sites': '../output/known_splice_sites_r6-11.txt',
        'bam': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.done',
    },
    'bai': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.bai',
    'rseqc': {
        'bam_stat': '../output/prealignment/raw/{experiment}/{sample}/{sample}.bam_stat.txt',
        'bam_stat_db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.bam_stat.txt.done',
        'infer_experiment': '../output/prealignment/raw/{experiment}/{sample}/{sample}.infer_experiment.txt',
        'infer_experiment_db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.infer_experiment.txt.done',
    },
    'dupradar': {
        'density_scatter': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.density_scatter.png',
        'expression_histogram': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.expression_histogram.png',
        'expression_boxplot': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.expression_boxplot.png',
        'expression_barplot': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.expression_barplot.png',
        'multimapping_histogram': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.multimapping_histogram.png',
        'dataframe': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.tsv',
        'model': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.model.txt',
        'curve': '../output/prealignment/raw/{experiment}/{sample}/{sample}.dupradar.curve.txt',
    },
    'feature_counts': {
        'counts': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fq.bam.counts',
        'jcounts': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fq.bam.counts.jcounts',
        'summary': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fq.bam.counts.summary',
        'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}_1.fq.bam.counts.done',
    },
    'picard': {
        'collectrnaseqmetrics': {
            'metrics': {
                'unstranded': '../output/prealignment/raw/{experiment}/{sample}/{sample}_NONE.fq.bam.picard.collectrnaseqmetrics',
                'first': '../output/prealignment/raw/{experiment}/{sample}/{sample}_FIRST_READ_TRANSCRIPTION_STRAND.fq.bam.picard.collectrnaseqmetrics',
                'second': '../output/prealignment/raw/{experiment}/{sample}/{sample}_SECOND_READ_TRANSCRIPTION_STRAND.fq.bam.picard.collectrnaseqmetrics',
            },
        },
        'markduplicates': {
            'bam': '../output/prealignment/raw/{experiment}/{sample}/{sample}.picard.dups.fq.bam',
            'metrics': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.picard.markduplicatesmetrics',
            'db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.picard.markduplicatesmetrics.done',
        },
    },
    'samtools_stats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats',
    'samtools_stats_db': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.stats.done',
    'samtools_idxstats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.samtools.idxstats',
    'bamtools_stats': '../output/prealignment/raw/{experiment}/{sample}/{sample}.fq.bam.bamtools.stats',
}


localrules:
    check_results, bai, fastq_screen_db, hisat2_db,
    infer_experiment_db, bam_stat_db, samtools_stats_db


rule targets:
    input: utils.flatten(targets)


rule check_results:
    input:
        utils.flatten(patterns['fastqc']) +
        utils.flatten(patterns['rseqc']) +
        utils.flatten(patterns['dupradar']) +
        utils.flatten(patterns['feature_counts']) +
        utils.flatten(patterns['picard']) +
        utils.flatten(patterns['fastq_screen']) +
        [
            patterns['samtools_stats_db'],
            patterns['samtools_idxstats'],
            patterns['bamtools_stats'],
        ]
    output: all_complete['files']
    params: infer=patterns['rseqc']['infer_experiment']
    run:
        # Update database
        remap.find_one_and_update(
            {'runs.srr': wildcards.sample},
            {'$addToSet': {'runs.$.pre_aln_flags': {'$each': ['complete', strand_type]}}}
        )

        # Touch dummy file for snakemake
        Path(output[0]).touch()


################################################################################
# FASTQ QC
################################################################################
rule fastqc:
    input: fastqs['r1']
    output:
        html=patterns['fastqc']['html'],
        zip=patterns['fastqc']['zip']
    params: extra="--format fastq"
    log: patterns['fastqc']['html'] + '.log'
    wrapper: wrapper_for('fastqc')


rule fastq_screen:
    input:
        fastq=fastqs['r1'],
        dm6=config['references']['dmel']['bowtie2'],
        hg19=config['references']['human']['bowtie2'],
        wolbachia=config['references']['wolbachia']['bowtie2'],
        ecoli=config['references']['ecoli']['bowtie2'],
        yeast=config['references']['yeast']['bowtie2'],
        rRNA=config['references']['rRNA']['bowtie2'],
        phix=config['references']['phix']['bowtie2'],
        ercc=config['references']['ercc']['bowtie2'],
        adapters=config['references']['adapters']['bowtie2']
    output: txt=patterns['fastq_screen']['txt']
    log: patterns['fastq_screen']['txt'] + '.log'
    wrapper: wrapper_for('fastq_screen')


rule fastq_screen_db:
    input:
        fn=patterns['fastq_screen']['txt']
    output:
        done=patterns['fastq_screen']['db']
    run:
        from ncbi_remap.parser import parse_fastq_screen
        srr = wildcards.sample
        df = parse_fastq_screen(srr, input.fn)
        df2 = {k[0]: {'_'.join(k[1:]): v} for k, v in df.to_dict('index')[srr].items()}
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.fastq_screen': df2
                }
            }
        )

        Path(output.done).touch()


################################################################################
# Alignment
################################################################################
rule hisat2_splice_site:
    input: gtf=config['references']['dmel']['gtf']
    output: patterns['hisat2']['splice_sites']
    conda: "../config/extra_env.yaml"
    shell: "hisat2_extract_splice_sites.py {input.gtf} > {output}"


rule hisat2_fastq:
    input:
        index=config['references']['dmel']['hisat2'],
        splice_sites=patterns['hisat2']['splice_sites'],
        fastq=[fastqs['r1'], fastqs['r2']],
    output: bam=temp(patterns['hisat2']['bam'])
    threads: 8
    params:
        hisat2_extra='--max-intronlen 300000 --known-splicesite-infile {input.splice_sites}',
        samtools_sort_extra='--threads 6 -l 9 -m 3G -T $TMPDIR/samtools_sort'
    log: patterns['hisat2']['bam'] + '.log'
    wrapper: wrapper_for('hisat2/align')


rule hisat2_db:
    input:
        fn=patterns['hisat2']['bam'] + '.log'
    output:
        done=patterns['hisat2']['db']
    run:
        from ncbi_remap.parser import parse_hisat2
        srr = wildcards.sample
        df = parse_hisat2(srr, input.fn)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.hisat2': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()


rule bai:
    input: bam='{prefix}.bam'
    output: bai=temp('{prefix}.bam.bai')
    conda: "../config/extra_env.yaml"
    shell: "samtools index {input.bam}"


################################################################################
# RSEQC
################################################################################
rule gtf2bed12:
    input: db=config['references']['dmel']['db']
    output: bed12=patterns['bed12']
    run:
        import gffutils
        db = gffutils.FeatureDB(input.db)
        with open(output.bed12, 'w') as fo:
            for t in db.features_of_type('transcript'):
                fo.write(db.bed12(t, name_field='transcript_id') + '\n')


rule infer_experiment:
    input:
        bam=patterns['hisat2']['bam'],
        bed=patterns['bed12']
    output: txt=patterns['rseqc']['infer_experiment']
    log: patterns['rseqc']['infer_experiment'] + '.log'
    wrapper: wrapper_for('rseqc/infer_experiment')


rule infer_experiment_db:
    input:
        fn=patterns['rseqc']['infer_experiment']
    output:
        done=patterns['rseqc']['infer_experiment_db']
    run:
        from ncbi_remap.parser import parse_inferExperiment
        srr = wildcards.sample
        df = parse_inferExperiment(srr, input.fn)

        if df.ix[0, 'same_strand'] > 0.75:
            strand_type = 'same_strand'
        elif df.ix[0, 'opposite_strand'] > 0.75:
            strand_type = 'opposite_strand'
        else:
            strand_type = 'unstranded'

        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.infer_expeirment': df.to_dict('index')[srr]
                },
                '$addToSet': {
                    'runs.$.pre_aln_flags': strand_type]
                }
            }
        )

        Path(output.done).touch()


rule bam_stat:
    input: bam=patterns['hisat2']['bam'],
    output: txt=patterns['rseqc']['bam_stat']
    log: patterns['rseqc']['bam_stat'] + '.log'
    wrapper: wrapper_for('rseqc/bam_stat')


rule bam_stat_db:
    input:
        fn=patterns['rseqc']['bam_stat']
    output:
        done=patterns['rseqc']['bam_stat_db']
    run:
        from ncbi_remap.parser import parse_bamStat
        srr = wildcards.sample
        df = parse_bamStat(srr, input.fn)

        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.bam_stat': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()


################################################################################
# DupRadar
################################################################################
rule dupRadar:
    input:
       bam=patterns['hisat2']['bam'],
       annotation=config['references']['dmel']['gtf'],
    output:
        density_scatter=patterns['dupradar']['density_scatter'],
        expression_histogram=patterns['dupradar']['expression_histogram'],
        expression_boxplot=patterns['dupradar']['expression_boxplot'],
        expression_barplot=patterns['dupradar']['expression_barplot'],
        multimapping_histogram=patterns['dupradar']['multimapping_histogram'],
        dataframe=patterns['dupradar']['dataframe'],
        model=patterns['dupradar']['model'],
        curve=patterns['dupradar']['curve'],
    wrapper:
        wrapper_for('dupradar')


################################################################################
# Feature Counts
################################################################################
def _params_featurecounts(wildcards):
    """Determine strandedness and pass correct settings."""
    strand = get_strand(wildcards.sample)
    base = '-J '

    if strand == 'same_strand':
        return base + '-s 1'
    elif strand == 'opposite_strand':
        return base + '-s 2'
    else:
        return base + '-s 0'


rule featurecounts:
    input:
        annotation=config['references']['dmel']['gtf'],
        bam=patterns['hisat2']['bam'],
        db=patterns['rseqc']['infer_experiment_db']
    output:
        counts=patterns['feature_counts']['counts'],
        jcounts=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    params: extra=_params_featurecounts
    threads: 4
    log: patterns['feature_counts']['counts'] + '.log'
    wrapper: wrapper_for('featurecounts')


rule featurecounts_db:
    input:
        jcount=patterns['feature_counts']['jcounts'],
        summary=patterns['feature_counts']['summary']
    output:
        done=patterns['feature_counts']['db']
    run:
        from ncbi_remap.parser import parse_featureCounts_jcounts, parse_featureCounts_summary
        srr = wildcards.sample
        dfJ = parse_featureCounts_jcounts(srr, input.jcount)
        num_junction_reads = df.loc[~df.PrimaryGene.isnull(), 'count'].sum()

        dfS = parse_featureCounts_jcounts(srr, input.summary)
        summary = dfS.to_dict('index')[srr]
        summary['Assigned_Junction'] = num_junction_reads

        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.featurecounts': summary
                }
            }
        )

        Path(output.done).touch()


################################################################################
# Stats
################################################################################
rule run_stats:
    input:
        bam=patterns['hisat2']['bam'],
        bai=patterns['bai'],
    output:
        samtools_stats=patterns['samtools_stats'],
        samtools_idxstats=patterns['samtools_idxstats'],
        bamtools_stats=patterns['bamtools_stats']
    conda: '../config/extra_env.yaml'
    shell:
        'BAM=$(mktemp --suffix=".bam") '
        '&& cp {input.bam} $BAM '
        '&& cp {input.bam}.bai $BAM.bai '
        '&& samtools stats $BAM > {output.samtools_stats} '
        '&& samtools idxstats $BAM > {output.samtools_idxstats} '
        '&& bamtools stats -in $BAM > {output.bamtools_stats} '
        '&& rm $BAM'


rule samtools_stats_db:
    input:
        fn=patterns['samtools_stats']
    output:
        done=patterns['samtools_stats_db']
    run:
        from ncbi_remap.parser import parse_samtools_stats
        srr = wildcards.sample
        df = parse_samtools_stats(srr, input.fn)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.samtools_stats': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()


################################################################################
# PICARD RNA Seq Metrics
################################################################################
rule collectrnaseqmetrics_unstrand:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded']
    params: extra='STRAND=NONE'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_first:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['first']
    params: extra='STRAND=FIRST_READ_TRANSCRIPTION_STRAND'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['first'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_second:
    input:
        bam=patterns['hisat2']['bam'],
        refflat=config['references']['dmel']['refflat']
    output: metrics=patterns['picard']['collectrnaseqmetrics']['metrics']['second']
    params: extra='STRAND=SECOND_READ_TRANSCRIPTION_STRAND'
    log: patterns['picard']['collectrnaseqmetrics']['metrics']['second'] + '.log'
    wrapper: wrapper_for('picard/collectrnaseqmetrics')


rule collectrnaseqmetrics_db:
    input:
        unstranded=patterns['picard']['collectrnaseqmetrics']['metrics']['unstranded'],
        first=patterns['picard']['collectrnaseqmetrics']['metrics']['first'],
        second=patterns['picard']['collectrnaseqmetrics']['metrics']['second'],
    output:
        done=patterns['picard']['collectrnaseqmetrics']['db']
    run:
        from ncbi_remap.parser import parse_picardCollect_summary
        srr = wildcards.sample
        dfU = parse_picardCollect_summary(srr, input.unstranded)
        dfF = parse_picardCollect_summary(srr, input.first)
        dfS = parse_picardCollect_summary(srr, input.second)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.picard_collectrnaseqmetrics': {
                        'unstranded': dfU.to_dict('index')[srr],
                        'first': dfF.to_dict('index')[srr],
                        'second': dfS.to_dict('index')[srr],
                    },
                }
            }
        )

        Path(output.done).touch()


rule markduplicates:
    input: bam=patterns['hisat2']['bam']
    output:
        bam=temp(patterns['picard']['markduplicates']['bam']),
        metrics=patterns['picard']['markduplicates']['metrics']
    params:
        java_args='-Xmx15g'
    log: patterns['picard']['markduplicates']['metrics'] + '.log'
    wrapper: wrapper_for('picard/markduplicates')


rule markduplicates_db:
    input:
        fn=patterns['picard']['markduplicates']['metrics'],
    output:
        done=patterns['picard']['markduplicates']['db']
    run:
        from ncbi_remap.parser import parse_picard_markduplicate_metrics
        srr = wildcards.sample
        df = parse_picard_markduplicate_metrics(srr, input.fn)
        remap.update_one(
            {'runs.srr': srr},
            {
                '$set': {
                    'runs.$.pre_aln_workflow.picard_markduplicates': df.to_dict('index')[srr]
                }
            }
        )

        Path(output.done).touch()
